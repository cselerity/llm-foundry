# æ¨¡å‹æ¶æ„è¯¦è§£

æœ¬æ–‡æ¡£æ·±å…¥è§£æ LLM Foundry çš„æ¨¡å‹æ¶æ„,å¸®åŠ©æ‚¨ç†è§£ç°ä»£ Transformer è¯­è¨€æ¨¡å‹çš„å·¥ä½œåŸç†ã€‚

## ğŸ“ æ•´ä½“æ¶æ„

LLM Foundry å®ç°äº†ä¸€ä¸ª **Decoder-Only Transformer** æ¶æ„,ç±»ä¼¼äº GPT å’Œ LLaMA ç³»åˆ—æ¨¡å‹ã€‚

### æ¶æ„å›¾

```
è¾“å…¥æ–‡æœ¬ "Hello World"
    â†“
[Tokenizer] â†’ [101, 2345, 3456]
    â†“
[Token Embedding] â†’ (batch, seq_len, dim)
    â†“
[Transformer Block 1]
    â”œâ”€ RMSNorm
    â”œâ”€ Causal Self-Attention (with RoPE)
    â””â”€ RMSNorm + MLP (SwiGLU)
    â†“
[Transformer Block 2]
    ...
    â†“
[Transformer Block N]
    â†“
[RMSNorm]
    â†“
[Output Projection] â†’ (batch, seq_len, vocab_size)
    â†“
[Softmax] â†’ æ¦‚ç‡åˆ†å¸ƒ
    â†“
ç”Ÿæˆä¸‹ä¸€ä¸ª token
```

---

## ğŸ§± æ ¸å¿ƒç»„ä»¶

### 1. Token Embedding

å°† token ID æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´ã€‚

```python
self.token_embedding = nn.Embedding(vocab_size, dim)
# è¾“å…¥: (batch, seq_len) - token IDs
# è¾“å‡º: (batch, seq_len, dim) - embeddings
```

**ç‰¹ç‚¹:**
- å¯è®­ç»ƒçš„åµŒå…¥çŸ©é˜µ
- ç»´åº¦: `vocab_size Ã— dim`
- åˆå§‹åŒ–: æ­£æ€åˆ†å¸ƒ (mean=0, std=0.02)

---

### 2. RMSNorm (Root Mean Square Normalization)

å‡æ–¹æ ¹å½’ä¸€åŒ–,æ¯” LayerNorm æ›´é«˜æ•ˆã€‚

```python
class RMSNorm(nn.Module):
    def forward(self, x):
        # è®¡ç®— RMS
        var = torch.mean(x ** 2, dim=-1, keepdim=True)
        # å½’ä¸€åŒ–
        x_norm = x * torch.rsqrt(var + eps)
        # ç¼©æ”¾
        return self.weight * x_norm
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ RMSNorm?**

| ç‰¹æ€§ | LayerNorm | RMSNorm |
|------|-----------|---------|
| è®¡ç®—å¤æ‚åº¦ | é«˜ (mean + std) | ä½ (åªéœ€ RMS) |
| å‚æ•° | 2Ã—dim | 1Ã—dim |
| æ€§èƒ½ | åŸºçº¿ | æå‡ 5-10% |
| ç¨³å®šæ€§ | å¥½ | å¥½ |

**æ•°å­¦å…¬å¼:**

```
LayerNorm: y = (x - mean(x)) / std(x) * Î³ + Î²
RMSNorm:   y = x / RMS(x) * Î³
å…¶ä¸­ RMS(x) = sqrt(mean(xÂ²) + Îµ)
```

---

### 3. RoPE (Rotary Position Embedding)

æ—‹è½¬ä½ç½®ç¼–ç ,é€šè¿‡æ—‹è½¬å˜æ¢æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚

```python
def apply_rotary_emb(xq, xk, freqs_cis):
    # å°†å®æ•°å¼ é‡è§†ä¸ºå¤æ•°
    xq_ = torch.view_as_complex(xq.reshape(..., -1, 2))
    xk_ = torch.view_as_complex(xk.reshape(..., -1, 2))

    # æ—‹è½¬
    xq_out = torch.view_as_real(xq_ * freqs_cis)
    xk_out = torch.view_as_real(xk_ * freqs_cis)

    return xq_out, xk_out
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ RoPE?**

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|
| ç»å¯¹ä½ç½®ç¼–ç  | ç®€å• | é•¿åº¦å¤–æ¨èƒ½åŠ›å·® |
| ç›¸å¯¹ä½ç½®ç¼–ç  | çµæ´» | è®¡ç®—å¤æ‚ |
| **RoPE** | **å¤–æ¨èƒ½åŠ›å¼º** | **å®ç°ç®€å•** |

**æ ¸å¿ƒæ€æƒ³:**

é€šè¿‡æ—‹è½¬çŸ©é˜µå¯¹ query å’Œ key è¿›è¡Œå˜æ¢,ä½¿å¾—æ³¨æ„åŠ›åˆ†æ•°éšå¼åœ°åŒ…å«ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚

```python
# é¢„è®¡ç®—æ—‹è½¬é¢‘ç‡
Î¸ = 10000^(-2i/d) for i in [0, d/2)
freqs = [Î¸â‚€, Î¸â‚, ..., Î¸_{d/2-1}]

# å¯¹äºä½ç½® m
m Ã— freqs = [mÃ—Î¸â‚€, mÃ—Î¸â‚, ..., mÃ—Î¸_{d/2-1}]

# è½¬æ¢ä¸ºå¤æ•°
e^(iÃ—mÃ—Î¸â±¼) = cos(mÃ—Î¸â±¼) + iÃ—sin(mÃ—Î¸â±¼)
```

**ä¼˜åŠ¿:**
- âœ… ç›¸å¯¹ä½ç½®ç¼–ç 
- âœ… é•¿åºåˆ—å¤–æ¨èƒ½åŠ›å¼º
- âœ… ä¸å¢åŠ å‚æ•°
- âœ… è®¡ç®—é«˜æ•ˆ

---

### 4. Grouped Query Attention (GQA)

åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›,åœ¨ MHA å’Œ MQA ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, cfg):
        self.n_heads = 8      # Query heads
        self.n_kv_heads = 4   # Key/Value heads (GQA)

        self.wq = nn.Linear(dim, n_heads * head_dim)
        self.wk = nn.Linear(dim, n_kv_heads * head_dim)
        self.wv = nn.Linear(dim, n_kv_heads * head_dim)

    def forward(self, x, freqs_cis):
        # æŠ•å½±
        q = self.wq(x)  # (B, S, n_heads * head_dim)
        k = self.wk(x)  # (B, S, n_kv_heads * head_dim)
        v = self.wv(x)  # (B, S, n_kv_heads * head_dim)

        # é‡å¤ KV heads ä»¥åŒ¹é… Q heads
        k = repeat_kv(k, n_heads // n_kv_heads)
        v = repeat_kv(v, n_heads // n_kv_heads)

        # æ³¨æ„åŠ›è®¡ç®—...
```

**æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”:**

| ç±»å‹ | Query Heads | KV Heads | å‚æ•° | é€Ÿåº¦ | è´¨é‡ |
|------|-------------|----------|------|------|------|
| MHA | 8 | 8 | æœ€å¤š | æ…¢ | æœ€å¥½ |
| **GQA** | **8** | **4** | **ä¸­ç­‰** | **ä¸­ç­‰** | **å¥½** |
| MQA | 8 | 1 | æœ€å°‘ | å¿« | å¯æ¥å— |

**ç¤ºä¾‹é…ç½®:**

```python
# æ ‡å‡†é…ç½® (GQA)
n_heads = 8
n_kv_heads = 4  # æ¯ 2 ä¸ª Q å…±äº« 1 ä¸ª KV

# MHA (Multi-Head Attention)
n_heads = 8
n_kv_heads = 8  # æ¯ä¸ª Q æœ‰ç‹¬ç«‹çš„ KV

# MQA (Multi-Query Attention)
n_heads = 8
n_kv_heads = 1  # æ‰€æœ‰ Q å…±äº« 1 ä¸ª KV
```

**ä¼˜åŠ¿:**
- âœ… å‡å°‘ KV Cache å¤§å° (æ¨ç†æ—¶é‡è¦)
- âœ… é™ä½å‚æ•°é‡å’Œè®¡ç®—é‡
- âœ… è´¨é‡æŸå¤±å¾ˆå°
- âœ… é€‚åˆå¤§è§„æ¨¡æ¨¡å‹

---

### 5. Scaled Dot-Product Attention

ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›,ä½¿ç”¨ PyTorch ä¼˜åŒ–å®ç°ã€‚

```python
# ä½¿ç”¨ PyTorch 2.0+ çš„é«˜æ•ˆå®ç°
output = F.scaled_dot_product_attention(
    query, key, value,
    attn_mask=None,
    dropout_p=dropout if training else 0.0,
    is_causal=True  # å› æœæ©ç 
)
```

**æ‰‹åŠ¨å®ç° (æ•™å­¦ç”¨):**

```python
# 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
scores = query @ key.transpose(-2, -1)  # (B, H, S, S)
scores = scores / sqrt(head_dim)         # ç¼©æ”¾

# 2. å› æœæ©ç  (ä¸‹ä¸‰è§’)
mask = torch.triu(torch.ones(S, S) * float('-inf'), diagonal=1)
scores = scores + mask  # å±è”½æœªæ¥ä½ç½®

# 3. Softmax
probs = F.softmax(scores, dim=-1)
probs = dropout(probs)

# 4. åŠ æƒæ±‚å’Œ
output = probs @ value  # (B, H, S, D_h)
```

**å› æœæ©ç ç¤ºä¾‹:**

```
è¾“å…¥åºåˆ—: ["The", "cat", "sat"]

æ³¨æ„åŠ›çŸ©é˜µ (å…è®¸çœ‹åˆ°çš„ä½ç½®):
       The  cat  sat
The  [  1    0    0  ]
cat  [  1    1    0  ]
sat  [  1    1    1  ]

â†’ "cat" åªèƒ½çœ‹åˆ° "The" å’Œ "cat"
â†’ "sat" å¯ä»¥çœ‹åˆ°æ‰€æœ‰å‰é¢çš„è¯
```

---

### 6. MLP with SwiGLU

å‰é¦ˆç½‘ç»œ,ä½¿ç”¨ SwiGLU æ¿€æ´»å‡½æ•°ã€‚

```python
class MLP(nn.Module):
    def __init__(self, cfg):
        hidden_dim = 4 * cfg.dim
        hidden_dim = int(2 * hidden_dim / 3)  # SwiGLU æƒ¯ä¾‹

        self.w1 = nn.Linear(dim, hidden_dim)  # Gate
        self.w2 = nn.Linear(hidden_dim, dim)  # Down
        self.w3 = nn.Linear(dim, hidden_dim)  # Up

    def forward(self, x):
        # SwiGLU: (Swish(xW1) âŠ™ xW3) W2
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```

**æ¿€æ´»å‡½æ•°å¯¹æ¯”:**

| æ¿€æ´»å‡½æ•° | å…¬å¼ | æ€§èƒ½ |
|---------|------|------|
| ReLU | max(0, x) | åŸºçº¿ |
| GELU | x Ã— Î¦(x) | +1-2% |
| Swish | x Ã— Ïƒ(x) | +1-2% |
| **SwiGLU** | **Swish(xWâ‚) âŠ™ xWâ‚ƒ** | **+2-3%** |

**ä¸ºä»€ä¹ˆæ˜¯ 2/3?**

æ ‡å‡† FFN: `4d â†’ 4d`
SwiGLU éœ€è¦ä¸¤ä¸ªé—¨: `4d â†’ 2Ã—(8d/3) â‰ˆ 5.33d`

ä¸ºäº†ä¿æŒå‚æ•°é‡ç›¸è¿‘:
`4d â†’ 2Ã—(8d/3) â†’ 4d` â‰ˆ `4d â†’ 4d â†’ 4d`

---

### 7. Transformer Block

å®Œæ•´çš„ Transformer å±‚,ä½¿ç”¨ Pre-Normalizationã€‚

```python
class Block(nn.Module):
    def forward(self, x, freqs_cis):
        # Pre-normalization
        h = x + self.attention(self.attention_norm(x), freqs_cis)
        out = h + self.feed_forward(self.ffn_norm(h))
        return out
```

**Pre-Norm vs Post-Norm:**

```
Post-Norm (åŸå§‹ Transformer):
x â†’ Attention â†’ Add â†’ Norm â†’ FFN â†’ Add â†’ Norm

Pre-Norm (ç°ä»£ LLM):
x â†’ Norm â†’ Attention â†’ Add â†’ Norm â†’ FFN â†’ Add
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ Pre-Norm?**
- âœ… è®­ç»ƒæ›´ç¨³å®š
- âœ… ä¸éœ€è¦å­¦ä¹ ç‡é¢„çƒ­
- âœ… å¯ä»¥è®­ç»ƒæ›´æ·±çš„æ¨¡å‹
- âœ… æ¢¯åº¦æµåŠ¨æ›´å¥½

---

## ğŸ”¢ å‚æ•°é‡è®¡ç®—

### Small é…ç½®ç¤ºä¾‹

```python
ModelConfig(
    dim=256,
    n_layers=4,
    n_heads=8,
    n_kv_heads=4,
    vocab_size=8192,
    max_seq_len=256
)
```

**å‚æ•°åˆ†è§£:**

| ç»„ä»¶ | å‚æ•°é‡ | å…¬å¼ |
|------|--------|------|
| Token Embedding | 2.1M | vocab_size Ã— dim |
| **æ¯ä¸ª Block** | | |
| - RMSNorm (Ã—2) | 512 | 2 Ã— dim |
| - Attention Q | 65K | dim Ã— (n_heads Ã— head_dim) |
| - Attention K | 33K | dim Ã— (n_kv_heads Ã— head_dim) |
| - Attention V | 33K | dim Ã— (n_kv_heads Ã— head_dim) |
| - Attention Out | 65K | (n_heads Ã— head_dim) Ã— dim |
| - MLP W1 | 87K | dim Ã— hidden_dim |
| - MLP W2 | 87K | hidden_dim Ã— dim |
| - MLP W3 | 87K | dim Ã— hidden_dim |
| **Block å°è®¡** | **458K** | |
| **4 ä¸ª Block** | **1.83M** | 458K Ã— 4 |
| Output Layer | 2.1M | dim Ã— vocab_size |
| **æ€»è®¡** | **~2.08M** | |

---

## ğŸ¯ è®¾è®¡å†³ç­–

### 1. ä¸ºä»€ä¹ˆé€‰æ‹© Decoder-Only?

| æ¶æ„ | ç”¨é€” | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|------|
| Encoder-Only (BERT) | ç†è§£ä»»åŠ¡ | åŒå‘ä¸Šä¸‹æ–‡ | ä¸èƒ½ç”Ÿæˆ |
| Encoder-Decoder (T5) | ç¿»è¯‘ | çµæ´» | å¤æ‚ |
| **Decoder-Only (GPT)** | **ç”Ÿæˆ** | **ç®€å•ã€é€šç”¨** | **å•å‘** |

Decoder-Only æ¶æ„:
- âœ… ç»Ÿä¸€çš„è®­ç»ƒç›®æ ‡(ä¸‹ä¸€è¯é¢„æµ‹)
- âœ… å¯ä»¥å¤„ç†æ‰€æœ‰ NLP ä»»åŠ¡
- âœ… æ¶æ„ç®€å•,æ˜“äºæ‰©å±•
- âœ… æœ€é€‚åˆå¤§è§„æ¨¡é¢„è®­ç»ƒ

### 2. ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›ç°ä»£æŠ€æœ¯?

| æŠ€æœ¯ | æ›¿ä»£æ–¹æ¡ˆ | é€‰æ‹©åŸå›  |
|------|---------|---------|
| RMSNorm | LayerNorm | æ›´å¿«,å‚æ•°æ›´å°‘ |
| RoPE | ç»å¯¹ä½ç½®ç¼–ç  | å¤–æ¨èƒ½åŠ›å¼º |
| GQA | MHA | KV Cache æ›´å° |
| SwiGLU | ReLU/GELU | æ€§èƒ½æ›´å¥½ |
| Pre-Norm | Post-Norm | è®­ç»ƒæ›´ç¨³å®š |

è¿™äº›æŠ€æœ¯åœ¨ LLaMAã€Mistral ç­‰æœ€æ–°æ¨¡å‹ä¸­è¢«å¹¿æ³›é‡‡ç”¨ã€‚

---

## ğŸ“Š ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”

### æ¶æ„å¯¹æ¯”

| ç‰¹æ€§ | GPT-2 | LLaMA | **LLM Foundry** |
|------|-------|-------|-----------------|
| Normalization | LayerNorm | RMSNorm | âœ… RMSNorm |
| Position Encoding | Learned | RoPE | âœ… RoPE |
| Attention | MHA | GQA | âœ… GQA |
| Activation | GELU | SwiGLU | âœ… SwiGLU |
| Norm Position | Post | Pre | âœ… Pre |

LLM Foundry é‡‡ç”¨äº†æœ€æ–°çš„æœ€ä½³å®è·µ!

---

## ğŸ” ä»£ç å¯¼èˆª

**æ¨¡å‹å®ç°ä½ç½®:**

- å®Œæ•´æ¨¡å‹: [src/llm_foundry/models/transformer.py](../../src/llm_foundry/models/transformer.py:1)
- ç»„ä»¶: [src/llm_foundry/models/components.py](../../src/llm_foundry/models/components.py:1)
  - RMSNorm: ç¬¬ 17 è¡Œ
  - RoPE: ç¬¬ 41 è¡Œ
  - Attention: ç¬¬ 95 è¡Œ
  - MLP: ç¬¬ 181 è¡Œ
  - Block: ç¬¬ 213 è¡Œ

---

## ğŸ’¡ ä¸‹ä¸€æ­¥

- ğŸ“– å­¦ä¹ å¦‚ä½•è®­ç»ƒæ¨¡å‹ â†’ [è®­ç»ƒæŒ‡å—](training.md)
- ğŸ”§ è‡ªå®šä¹‰æ¨¡å‹é…ç½® â†’ [é…ç½®ç³»ç»Ÿ](configuration.md)
- ğŸš€ ä¼˜åŒ–æ¨ç†æ€§èƒ½ â†’ [æ¨ç†ä¼˜åŒ–](production/optimization.md)

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) - RoPE
- [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467) - RMSNorm
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) - GQA, SwiGLU
