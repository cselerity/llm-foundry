# RTX 5060 è®­ç»ƒæŒ‡å—

æœ¬æŒ‡å—ä¸“é—¨ä¸ºä½¿ç”¨ RTX 5060 (8GB æ˜¾å­˜) æˆ–åŒç­‰æ€§èƒ½ GPU è¿›è¡Œæœ¬åœ°å­¦ä¹ å’Œè®­ç»ƒçš„ç”¨æˆ·å‡†å¤‡ã€‚

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
- [é«˜çº§æŠ€å·§](#é«˜çº§æŠ€å·§)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ£€æŸ¥ç¯å¢ƒ

```bash
# æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"

# æ£€æŸ¥æ˜¾å­˜
nvidia-smi
```

### 2. è¿è¡Œè®­ç»ƒ

```bash
cd tutorials
python train_rtx5060.py
```

### 3. é¢„æœŸç»“æœ

- **è®­ç»ƒæ—¶é—´**: 30-40 åˆ†é’Ÿ (10k steps)
- **æ˜¾å­˜å ç”¨**: 3-4 GB
- **è®­ç»ƒé€Ÿåº¦**: 2500-3500 tokens/sec
- **æœ€ç»ˆæŸå¤±**: train ~2.0, val ~2.2

---

## âš™ï¸ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½® (70-75M å‚æ•°)

```python
from config import get_rtx5060_config

model_cfg = get_rtx5060_config()
```

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `dim` | 768 | éšè—å±‚ç»´åº¦ (BERT-base åŒæ¬¾) |
| `n_layers` | 12 | Transformer å±‚æ•° |
| `n_heads` | 12 | æ³¨æ„åŠ›å¤´æ•° (æ¯å¤´ 64 ç»´) |
| `n_kv_heads` | 6 | KV å¤´æ•° (GQA ä¼˜åŒ–,èŠ‚çœ 50% æ˜¾å­˜) |
| `vocab_size` | 32768 | è¯æ±‡è¡¨å¤§å° (32k tokens) |
| `max_seq_len` | 1024 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `dropout` | 0.1 | Dropout ç‡ |

### è®­ç»ƒé…ç½®

```python
from config import get_rtx5060_train_config

train_cfg = get_rtx5060_train_config()
```

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `batch_size` | 24 | æ‰¹æ¬¡å¤§å° (ä¼˜åŒ–çš„æ˜¾å­˜å ç”¨) |
| `learning_rate` | 3e-4 | å­¦ä¹ ç‡ (Adam æ¨èå€¼) |
| `max_iters` | 10000 | è®­ç»ƒæ­¥æ•° |
| `eval_interval` | 500 | è¯„ä¼°é—´éš” |
| `eval_iters` | 50 | è¯„ä¼°æ‰¹æ¬¡æ•° |

---

## ğŸ“Š æ€§èƒ½é¢„æœŸ

### ä¸åŒ GPU çš„æ€§èƒ½å¯¹æ¯”

| GPU å‹å· | æ˜¾å­˜ | Batch Size | è®­ç»ƒé€Ÿåº¦ | 10k Steps æ—¶é—´ |
|----------|------|------------|----------|----------------|
| RTX 3060 | 12GB | 32-48 | 3000-4000 tok/s | 25-35 åˆ†é’Ÿ |
| **RTX 5060** | **8GB** | **24-32** | **2500-3500 tok/s** | **30-40 åˆ†é’Ÿ** |
| RTX 4060 | 8GB | 24-32 | 2500-3500 tok/s | 30-40 åˆ†é’Ÿ |
| RTX 4060 Ti | 16GB | 48-64 | 4000-5000 tok/s | 20-30 åˆ†é’Ÿ |
| RTX 4090 | 24GB | 96-128 | 8000-10000 tok/s | 10-15 åˆ†é’Ÿ |

### æ˜¾å­˜å ç”¨è¯¦ç»†åˆ†è§£

```
æ€»æ˜¾å­˜å ç”¨: 3-4 GB (è®­ç»ƒæ—¶)
â”œâ”€â”€ æ¨¡å‹å‚æ•°:   ~0.28 GB (70M Ã— 4 bytes)
â”œâ”€â”€ ä¼˜åŒ–å™¨çŠ¶æ€: ~0.56 GB (AdamW, 2 çŠ¶æ€)
â”œâ”€â”€ æ¿€æ´»å€¼:     ~1.5-2 GB (å–å†³äº batch_size)
â””â”€â”€ KV cache:   ~0.3 GB (GQA ä¼˜åŒ–å)
```

---

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–

### 1. è°ƒæ•´ Batch Size

**å¦‚æœæ˜¾å­˜ä¸è¶³:**

```python
# åœ¨ config.py ä¸­ä¿®æ”¹
TrainConfig(
    batch_size=16,  # å‡å°åˆ° 16
    # ... å…¶ä»–å‚æ•°
)
```

**å¦‚æœæ˜¾å­˜å……è¶³:**

```python
# å°è¯•æ›´å¤§çš„ batch size
TrainConfig(
    batch_size=32,  # å¢åŠ åˆ° 32
    # ... å…¶ä»–å‚æ•°
)
```

### 2. è°ƒæ•´åºåˆ—é•¿åº¦

**å‡å°‘åºåˆ—é•¿åº¦å¯ä»¥æ˜¾è‘—èŠ‚çœæ˜¾å­˜:**

```python
ModelConfig(
    max_seq_len=512,  # ä» 1024 å‡åˆ° 512
    # ... å…¶ä»–å‚æ•°
)
```

### 3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

**å¦‚æœæƒ³è¦æ›´å¤§çš„æœ‰æ•ˆ batch size ä½†æ˜¾å­˜ä¸è¶³:**

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ 
accumulation_steps = 2  # ç´¯ç§¯ 2 æ­¥å†æ›´æ–°

for iter in range(max_iters):
    for micro_step in range(accumulation_steps):
        xb, yb = loader.get_batch('train')
        logits, loss = model(xb, yb)
        loss = loss / accumulation_steps  # ç¼©æ”¾æŸå¤±
        loss.backward()

    optimizer.step()
    optimizer.zero_grad()
```

### 4. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

**å¯ä»¥èŠ‚çœçº¦ 40% æ˜¾å­˜:**

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for iter in range(max_iters):
    xb, yb = loader.get_batch('train')

    with autocast():  # è‡ªåŠ¨æ··åˆç²¾åº¦
        logits, loss = model(xb, yb)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
```

### 5. å¯ç”¨ PyTorch ç¼–è¯‘ (PyTorch 2.0+)

**å¯ä»¥æå‡ 20-30% é€Ÿåº¦:**

```python
import torch

model = MiniLLM(cfg).to('cuda')
model = torch.compile(model)  # ç¼–è¯‘æ¨¡å‹
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: CUDA out of memory

**ç—‡çŠ¶:**
```
RuntimeError: CUDA out of memory. Tried to allocate X GB
```

**è§£å†³æ–¹æ¡ˆ:**

1. **å‡å° batch_size:**
   ```python
   TrainConfig(batch_size=16)  # æˆ–æ›´å°
   ```

2. **å‡å°åºåˆ—é•¿åº¦:**
   ```python
   ModelConfig(max_seq_len=512)
   ```

3. **æ¸…ç† GPU ç¼“å­˜:**
   ```python
   torch.cuda.empty_cache()
   ```

4. **å…³é—­å…¶ä»–å ç”¨ GPU çš„ç¨‹åº:**
   ```bash
   nvidia-smi  # æŸ¥çœ‹ GPU å ç”¨
   kill <pid>  # å…³é—­å ç”¨è¿›ç¨‹
   ```

### é—®é¢˜ 2: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆ:**

1. **æ²¡æœ‰ä½¿ç”¨ GPU:**
   ```bash
   # æ£€æŸ¥
   python -c "import torch; print(torch.cuda.is_available())"

   # å¦‚æœè¿”å› False,é‡æ–°å®‰è£… CUDA ç‰ˆæœ¬çš„ PyTorch
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
   ```

2. **GPU è¢«å…¶ä»–ç¨‹åºå ç”¨:**
   ```bash
   nvidia-smi  # æ£€æŸ¥ GPU åˆ©ç”¨ç‡åº”è¯¥æ¥è¿‘ 100%
   ```

3. **æ•°æ®åŠ è½½æˆä¸ºç“¶é¢ˆ:**
   ```python
   # å¢åŠ æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹ (å¦‚æœä½¿ç”¨ DataLoader)
   DataLoader(..., num_workers=4)
   ```

4. **PyTorch ç‰ˆæœ¬è¿‡æ—§:**
   ```bash
   pip install --upgrade torch
   ```

### é—®é¢˜ 3: æŸå¤±ä¸º NaN

**å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆ:**

1. **å­¦ä¹ ç‡è¿‡é«˜:**
   ```python
   TrainConfig(learning_rate=1e-4)  # é™ä½å­¦ä¹ ç‡
   ```

2. **æ¢¯åº¦çˆ†ç‚¸:**
   ```python
   # æ·»åŠ æ¢¯åº¦è£å‰ª
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
   ```

3. **æ•°æ®é—®é¢˜:**
   ```python
   # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰å¼‚å¸¸å€¼
   print(f"Data range: {loader.tokens.min()} - {loader.tokens.max()}")
   ```

### é—®é¢˜ 4: è¿‡æ‹Ÿåˆ

**ç—‡çŠ¶:**
```
train loss: 1.5, val loss: 2.8  (å·®è·è¿‡å¤§)
```

**è§£å†³æ–¹æ¡ˆ:**

1. **å¢åŠ  Dropout:**
   ```python
   ModelConfig(dropout=0.2)  # ä» 0.1 å¢åŠ åˆ° 0.2
   ```

2. **å‡å°‘è®­ç»ƒæ­¥æ•°:**
   ```python
   TrainConfig(max_iters=5000)  # å‡å°‘è¿­ä»£æ¬¡æ•°
   ```

3. **ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®:**
   - æ·»åŠ æ›´å¤šæ–‡æœ¬åˆ°è®­ç»ƒé›†
   - ä½¿ç”¨æ•°æ®å¢å¼º

4. **å‡å°æ¨¡å‹è§„æ¨¡:**
   ```python
   # ä½¿ç”¨ medium é…ç½®
   from config import get_medium_config
   model_cfg = get_medium_config()
   ```

---

## ğŸ¯ é«˜çº§æŠ€å·§

### 1. å­¦ä¹ ç‡é¢„çƒ­ (Learning Rate Warmup)

**é€æ¸å¢åŠ å­¦ä¹ ç‡,è®­ç»ƒæ›´ç¨³å®š:**

```python
def get_lr(iter, warmup_iters=1000, lr_decay_iters=10000, min_lr=1e-5):
    # Warmup
    if iter < warmup_iters:
        return learning_rate * iter / warmup_iters
    # Decay
    if iter > lr_decay_iters:
        return min_lr
    # Cosine decay
    decay_ratio = (iter - warmup_iters) / (lr_decay_iters - warmup_iters)
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (learning_rate - min_lr)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for iter in range(max_iters):
    lr = get_lr(iter)
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    # ... è®­ç»ƒæ­¥éª¤
```

### 2. æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤

**å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹,é¿å…è®­ç»ƒä¸­æ–­:**

```python
# ä¿å­˜æ£€æŸ¥ç‚¹
if iter % 1000 == 0:
    checkpoint = {
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'iter': iter,
        'config': model_cfg,
    }
    torch.save(checkpoint, f'checkpoint_{iter}.pt')

# æ¢å¤è®­ç»ƒ
checkpoint = torch.load('checkpoint_5000.pt')
model.load_state_dict(checkpoint['model'])
optimizer.load_state_dict(checkpoint['optimizer'])
start_iter = checkpoint['iter']
```

### 3. æ—©åœ (Early Stopping)

**éªŒè¯æŸå¤±ä¸å†ä¸‹é™æ—¶è‡ªåŠ¨åœæ­¢:**

```python
best_val_loss = float('inf')
patience = 5
patience_counter = 0

for iter in range(max_iters):
    if iter % eval_interval == 0:
        losses = estimate_loss(...)

        if losses['val'] < best_val_loss:
            best_val_loss = losses['val']
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), 'best_model.pt')
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("Early stopping!")
                break
```

### 4. ä½¿ç”¨ TensorBoard ç›‘æ§

**å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹:**

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/rtx5060_experiment')

for iter in range(max_iters):
    # ... è®­ç»ƒæ­¥éª¤

    if iter % eval_interval == 0:
        losses = estimate_loss(...)
        writer.add_scalar('Loss/train', losses['train'], iter)
        writer.add_scalar('Loss/val', losses['val'], iter)
        writer.add_scalar('LearningRate', lr, iter)

writer.close()
```

ç„¶åå¯åŠ¨ TensorBoard:
```bash
tensorboard --logdir=runs
```

### 5. æ‰¹é‡ç”Ÿæˆå’Œè¯„ä¼°

**è¯„ä¼°æ¨¡å‹ç”Ÿæˆè´¨é‡:**

```python
def generate_samples(model, tokenizer, prompts, max_tokens=100):
    """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬æ ·æœ¬"""
    model.eval()
    samples = []

    for prompt in prompts:
        ids = tokenizer.encode(prompt)
        x = torch.tensor(ids).unsqueeze(0).to(device)

        with torch.no_grad():
            y = generate(model, x, max_tokens, temperature=0.8, top_k=50)

        text = tokenizer.decode(y[0].tolist())
        samples.append(text)

    return samples

# ä½¿ç”¨
prompts = ["çº¢æ¥¼æ¢¦", "äººå·¥æ™ºèƒ½", "ä»å‰æœ‰åº§å±±"]
samples = generate_samples(model, tokenizer, prompts)
for prompt, sample in zip(prompts, samples):
    print(f"Prompt: {prompt}")
    print(f"Generated: {sample}\n")
```

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•

### è¿è¡ŒåŸºå‡†æµ‹è¯•

```bash
# æµ‹è¯•ä¸åŒ batch size çš„æ€§èƒ½
for bs in 16 24 32; do
    echo "Testing batch_size=$bs"
    python -c "
from config import ModelConfig, TrainConfig
config = TrainConfig(batch_size=$bs)
# ... è¿è¡Œè®­ç»ƒ
    "
done
```

### é¢„æœŸç»“æœ

| Batch Size | æ˜¾å­˜å ç”¨ | é€Ÿåº¦ (tokens/s) | å¤‡æ³¨ |
|------------|----------|-----------------|------|
| 16 | 2.5-3 GB | 2200-2800 | å®‰å…¨é€‰æ‹© |
| **24** | **3-4 GB** | **2500-3500** | **æ¨è (å¹³è¡¡)** |
| 32 | 4-5 GB | 2800-4000 | éœ€è¦è¶³å¤Ÿæ˜¾å­˜ |

---

## ğŸ“ å­¦ä¹ å»ºè®®

### å¾ªåºæ¸è¿›çš„å­¦ä¹ è·¯å¾„

1. **ç¬¬ä¸€æ¬¡è®­ç»ƒ** (10-30 åˆ†é’Ÿ):
   - ä½¿ç”¨é»˜è®¤ RTX 5060 é…ç½®
   - è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹å’Œæ—¥å¿—è¾“å‡º
   - ç†è§£å„é¡¹æŒ‡æ ‡çš„å«ä¹‰

2. **å®éªŒå‚æ•°** (1-2 å°æ—¶):
   - è°ƒæ•´ batch_size: 16, 24, 32
   - è°ƒæ•´ learning_rate: 1e-4, 3e-4, 5e-4
   - è§‚å¯Ÿå¯¹è®­ç»ƒé€Ÿåº¦å’ŒæŸå¤±çš„å½±å“

3. **ä¼˜åŒ–æ˜¾å­˜** (30 åˆ†é’Ÿ):
   - å°è¯•å‡å° max_seq_len: 512, 768, 1024
   - æµ‹è¯•æ··åˆç²¾åº¦è®­ç»ƒ
   - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

4. **æå‡è´¨é‡** (2-3 å°æ—¶):
   - å¢åŠ è®­ç»ƒæ­¥æ•°: 20k, 50k
   - ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†
   - å®éªŒä¸åŒçš„é‡‡æ ·ç­–ç•¥

5. **ç”Ÿäº§ä¼˜åŒ–** (è¿›é˜¶):
   - å®ç°å­¦ä¹ ç‡è°ƒåº¦
   - æ·»åŠ æ—©åœæœºåˆ¶
   - ä½¿ç”¨ TensorBoard ç›‘æ§

---

## ğŸ“š å‚è€ƒèµ„æ–™

### ç›¸å…³æ–‡ä»¶

- [config.py](config.py) - é…ç½®å®šä¹‰
- [train_rtx5060.py](train_rtx5060.py) - RTX 5060 è®­ç»ƒè„šæœ¬
- [model.py](model.py) - æ¨¡å‹å®ç°
- [LEARNING_PATH.md](../LEARNING_PATH.md) - å­¦ä¹ è·¯å¾„

### æœ‰ç”¨çš„å‘½ä»¤

```bash
# ç›‘æ§ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹ PyTorch ç‰ˆæœ¬
python -c "import torch; print(torch.__version__)"

# æµ‹è¯• CUDA æ€§èƒ½
python -c "import torch; x = torch.rand(1000, 1000).cuda(); print(x @ x)"

# æŸ¥çœ‹æ˜¾å­˜å ç”¨è¯¦æƒ…
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: æˆ‘çš„ GPU ä¸æ˜¯ RTX 5060,ä¹Ÿèƒ½ç”¨è¿™ä¸ªé…ç½®å—?

**A:** å¯ä»¥! è¿™ä¸ªé…ç½®é€‚ç”¨äºå¤§å¤šæ•° 8GB æ˜¾å­˜çš„ GPU,åŒ…æ‹¬:
- RTX 3060 (12GB) - å¯ä»¥å¢å¤§ batch_size
- RTX 4060 (8GB) - å®Œå…¨å…¼å®¹
- RTX 2060 Super (8GB) - å¯èƒ½éœ€è¦ç¨å¾®è°ƒæ•´
- GTX 1080 Ti (11GB) - å¯ä»¥å¢å¤§ batch_size

### Q2: è®­ç»ƒéœ€è¦å¤šä¹…æ‰èƒ½çœ‹åˆ°å¥½çš„ç»“æœ?

**A:**
- **åˆæ­¥ç»“æœ**: 1000-2000 steps (3-5 åˆ†é’Ÿ)
- **å¯ç”¨è´¨é‡**: 5000-10000 steps (15-30 åˆ†é’Ÿ)
- **è‰¯å¥½è´¨é‡**: 20000-50000 steps (1-2 å°æ—¶)

### Q3: å¦‚ä½•çŸ¥é“è®­ç»ƒæ˜¯å¦æ­£å¸¸?

**A:** è§‚å¯Ÿä»¥ä¸‹æŒ‡æ ‡:
- **è®­ç»ƒæŸå¤±ä¸‹é™**: åº”è¯¥ä» ~10 é™åˆ° ~2
- **éªŒè¯æŸå¤±è·Ÿéš**: ä¸åº”è¯¥è¿œé«˜äºè®­ç»ƒæŸå¤±
- **GPU åˆ©ç”¨ç‡**: nvidia-smi åº”è¯¥æ˜¾ç¤º 90-100%
- **ç”Ÿæˆè´¨é‡**: å®šæœŸæµ‹è¯•ç”Ÿæˆæ–‡æœ¬

### Q4: å¯ä»¥åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ç”µè„‘å—?

**A:** å¯ä»¥,ä½†å»ºè®®:
- ä¸è¦åŒæ—¶è¿è¡Œå…¶ä»– GPU å¯†é›†ä»»åŠ¡
- æµè§ˆå™¨å’Œè½»é‡åº”ç”¨æ²¡é—®é¢˜
- å¯ä»¥åœ¨åå°è¿è¡Œè®­ç»ƒ,ä½¿ç”¨ screen æˆ– tmux

### Q5: å¦‚ä½•æé«˜ç”Ÿæˆè´¨é‡?

**A:** å°è¯•:
1. å¢åŠ è®­ç»ƒæ­¥æ•° (max_iters)
2. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ (ä½†éœ€è¦æ›´å¤šæ˜¾å­˜)
3. å¢åŠ è®­ç»ƒæ•°æ®
4. è°ƒæ•´é‡‡æ ·å‚æ•° (temperature, top_k, top_p)
5. ä½¿ç”¨æ›´é•¿çš„æç¤ºè¯ (prompt)

---

## ğŸ’¡ è´¡çŒ®

å‘ç°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®? æ¬¢è¿:
- æäº¤ Issue
- åˆ›å»º Pull Request
- åˆ†äº«ä½ çš„è®­ç»ƒç»éªŒ

---

**ç¥è®­ç»ƒæ„‰å¿«! ğŸš€**
