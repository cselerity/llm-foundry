# å¿«é€Ÿå…¥é—¨

æ¬¢è¿ä½¿ç”¨ LLM Foundry! æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨åœ¨ 5 åˆ†é’Ÿå†…å¼€å§‹è®­ç»ƒå’Œä½¿ç”¨æ‚¨çš„ç¬¬ä¸€ä¸ªè¯­è¨€æ¨¡å‹ã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

- Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- è‡³å°‘ 4GB RAM
- (å¯é€‰) NVIDIA GPU ç”¨äºåŠ é€Ÿè®­ç»ƒ

## ğŸš€ å®‰è£…

### æ–¹æ³• 1: ä»æºç å®‰è£… (æ¨è)

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-org/llm-foundry.git
cd llm-foundry

# å®‰è£…ä¾èµ–
pip install -e .

# (å¯é€‰) å®‰è£…å¼€å‘å·¥å…·
pip install -r requirements-dev.txt
```

### æ–¹æ³• 2: ä½¿ç”¨ pip å®‰è£…

```bash
pip install llm-foundry
```

## ğŸ¯ ä¸¤ç§ä½¿ç”¨æ¨¡å¼

LLM Foundry æä¾›ä¸¤ç§ä½¿ç”¨æ¨¡å¼,é€‚åº”ä¸åŒçš„éœ€æ±‚:

### ç®€å•æ¨¡å¼ - å¿«é€Ÿä½“éªŒ ğŸ“

é€‚åˆ: å­¦ä¹ ã€æ•™å­¦ã€å¿«é€Ÿå®éªŒ

```bash
cd simple
python train.py      # è®­ç»ƒæ¨¡å‹
python generate.py   # ç”Ÿæˆæ–‡æœ¬
```

### åŒ…æ¨¡å¼ - ç”Ÿäº§ä½¿ç”¨ ğŸ­

é€‚åˆ: ç ”ç©¶ã€ç”Ÿäº§ã€å®šåˆ¶åŒ–å¼€å‘

```python
from llm_foundry import ModelConfig, MiniLLM, DataLoader
```

æœ¬æŒ‡å—ä¸»è¦ä»‹ç»**ç®€å•æ¨¡å¼**,åŒ…æ¨¡å¼è¯·å‚è€ƒ [API å‚è€ƒ](api-reference.md)ã€‚

---

## ğŸ“ ç®€å•æ¨¡å¼å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: è®­ç»ƒæ‚¨çš„ç¬¬ä¸€ä¸ªæ¨¡å‹

```bash
cd simple
python train.py
```

**å‘ç”Ÿäº†ä»€ä¹ˆ?**

1. è‡ªåŠ¨ä¸‹è½½çº¢æ¥¼æ¢¦æ•°æ®é›† (~100KB)
2. è®­ç»ƒ SentencePiece åˆ†è¯å™¨ (è¯è¡¨å¤§å°: 8192)
3. è®­ç»ƒ Mini LLM æ¨¡å‹ (é»˜è®¤ 100 æ­¥)
4. ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹åˆ° `minillm.pt`

**é¢„æœŸè¾“å‡º:**

```
ä½¿ç”¨è®¾å¤‡: cuda
æ­£åœ¨ä¸‹è½½æ•°æ®...
æ­£åœ¨è®­ç»ƒ Tokenizer (vocab_size=8192)...
Tokenizer è®­ç»ƒå®Œæˆã€‚
æ•°æ®åŠ è½½å®Œæˆã€‚æ€» token æ•°: 145234
æ¨¡å‹å‚æ•°é‡: 2.08M
å¼€å§‹è®­ç»ƒ...
step 0: train loss 9.1234, val loss 9.2345
step 50: train loss 5.6789, val loss 5.7890
è®­ç»ƒå®Œæˆ,è€—æ—¶ 32.45s
æ¨¡å‹å·²ä¿å­˜è‡³ minillm.pt
```

### æ­¥éª¤ 2: ç”Ÿæˆæ–‡æœ¬

```bash
python generate.py
```

**é¢„æœŸè¾“å‡º:**

```
ä½¿ç”¨è®¾å¤‡: cuda
å·²åŠ è½½ Checkpoint 'minillm.pt'

æç¤ºè¯: æ»¡çº¸è’å”è¨€ï¼Œ
æ­£åœ¨ç”Ÿæˆ...

--- ç”Ÿæˆçš„æ–‡æœ¬ ---
æ»¡çº¸è’å”è¨€ï¼Œä¸€æŠŠè¾›é…¸æ³ªã€‚éƒ½äº‘ä½œè€…ç—´ï¼Œè°è§£å…¶ä¸­å‘³ï¼Ÿ...
```

### æ­¥éª¤ 3: è‡ªå®šä¹‰é…ç½®

ç¼–è¾‘ `simple/config.py`:

```python
@dataclass
class ModelConfig:
    dim: int = 512          # å¢å¤§æ¨¡å‹ç»´åº¦
    n_layers: int = 8       # å¢åŠ å±‚æ•°
    n_heads: int = 8
    vocab_size: int = 8192
    max_seq_len: int = 512  # å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦

@dataclass
class TrainConfig:
    batch_size: int = 16
    learning_rate: float = 3e-4
    max_iters: int = 5000   # è®­ç»ƒæ›´å¤šæ­¥æ•°
```

**é‡æ–°è®­ç»ƒ:**

```bash
python train.py
```

---

## ğŸ­ åŒ…æ¨¡å¼å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: åŸºæœ¬ä½¿ç”¨

```python
from llm_foundry import ModelConfig, MiniLLM, Tokenizer, DataLoader
from llm_foundry.utils import get_device

# 1. é…ç½®
cfg = ModelConfig(
    dim=512,
    n_layers=8,
    n_heads=8,
    vocab_size=8192,
    max_seq_len=512
)

# 2. åˆ›å»ºæ¨¡å‹
device = get_device()
model = MiniLLM(cfg).to(device)

print(f"æ¨¡å‹å‚æ•°é‡: {model.get_num_params()/1e6:.2f}M")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
```

### æ­¥éª¤ 2: åŠ è½½æ•°æ®

```python
from llm_foundry import DataLoader

# åŠ è½½æ•°æ®
loader = DataLoader(
    file_path='data/your_text.txt',  # æ‚¨çš„æ•°æ®
    batch_size=32,
    block_size=cfg.max_seq_len,
    device=device
)

# è·å–ä¸€ä¸ªæ‰¹æ¬¡
x, y = loader.get_batch('train')
print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
print(f"ç›®æ ‡å½¢çŠ¶: {y.shape}")
```

### æ­¥éª¤ 3: è®­ç»ƒæ¨¡å‹

```python
import torch

# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

# è®­ç»ƒå¾ªç¯
model.train()
for step in range(1000):
    # è·å–æ‰¹æ¬¡
    x, y = loader.get_batch('train')

    # å‰å‘ä¼ æ’­
    logits, loss = model(x, y)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % 50 == 0:
        print(f"Step {step}: loss {loss.item():.4f}")

# ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), 'model.pt')
```

### æ­¥éª¤ 4: ç”Ÿæˆæ–‡æœ¬

```python
from llm_foundry.inference import generate

# åŠ è½½æ¨¡å‹
model.load_state_dict(torch.load('model.pt'))
model.eval()

# å‡†å¤‡æç¤ºè¯
tokenizer = loader.tokenizer
prompt = "æ»¡çº¸è’å”è¨€ï¼Œ"
input_ids = torch.tensor(
    tokenizer.encode(prompt),
    dtype=torch.long,
    device=device
)[None, ...]

# ç”Ÿæˆ
with torch.no_grad():
    output_ids = generate(
        model,
        input_ids,
        max_new_tokens=100,
        temperature=0.8,
        top_k=50
    )

# è§£ç 
generated_text = tokenizer.decode(output_ids[0].tolist())
print(generated_text)
```

---

## ğŸ“Š éªŒè¯å®‰è£…

è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯å®‰è£…:

```python
import llm_foundry
print(f"LLM Foundry ç‰ˆæœ¬: {llm_foundry.__version__}")

# æµ‹è¯•å¯¼å…¥
from llm_foundry import ModelConfig, MiniLLM, Tokenizer
print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ!")
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥

ç°åœ¨æ‚¨å·²ç»æˆåŠŸè¿è¡Œäº†ç¬¬ä¸€ä¸ªæ¨¡å‹,æ¥ä¸‹æ¥å¯ä»¥:

### æ·±å…¥å­¦ä¹ 
- ğŸ“– é˜…è¯» [æ¶æ„è¯¦è§£](architecture.md) ç†è§£æ¨¡å‹åŸç†
- ğŸ“ æŸ¥çœ‹ [è®­ç»ƒæŒ‡å—](training.md) å­¦ä¹ è®­ç»ƒæŠ€å·§
- ğŸ” æ¢ç´¢ [æ¨ç†æŒ‡å—](inference.md) äº†è§£ç”Ÿæˆç­–ç•¥

### å®è·µé¡¹ç›®
- ğŸ“ ä½¿ç”¨è‡ªå·±çš„æ•°æ®è®­ç»ƒæ¨¡å‹ â†’ [æ•°æ®å‡†å¤‡](data-preparation.md)
- âš™ï¸ è‡ªå®šä¹‰æ¨¡å‹é…ç½® â†’ [é…ç½®ç³»ç»Ÿ](configuration.md)
- ğŸš€ éƒ¨ç½²æ¨¡å‹æœåŠ¡ â†’ [æ¨¡å‹æœåŠ¡](production/model-serving.md)

### é«˜çº§ä¸»é¢˜
- ğŸ”¥ [åˆ†å¸ƒå¼è®­ç»ƒ](production/distributed-training.md) - å¤š GPU è®­ç»ƒ
- âš¡ [æ··åˆç²¾åº¦](production/mixed-precision.md) - åŠ é€Ÿè®­ç»ƒ
- ğŸ“ˆ [æ¨ç†ä¼˜åŒ–](production/optimization.md) - æå‡æ¨ç†é€Ÿåº¦

---

## â“ å¸¸è§é—®é¢˜

### Q: è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠ?

**A:** å°è¯•ä»¥ä¸‹æ–¹æ³•:
- ä½¿ç”¨ GPU: ç¡®ä¿å®‰è£…äº† CUDA ç‰ˆæœ¬çš„ PyTorch
- å‡å°æ¨¡å‹: é™ä½ `dim` æˆ– `n_layers`
- å‡å°æ‰¹æ¬¡: é™ä½ `batch_size`
- ä½¿ç”¨æ··åˆç²¾åº¦: å‚è€ƒ [æ··åˆç²¾åº¦è®­ç»ƒ](production/mixed-precision.md)

### Q: æ˜¾å­˜ä¸è¶³ (OOM) æ€ä¹ˆåŠ?

**A:**
- å‡å° `batch_size`
- å‡å° `max_seq_len`
- å‡å°æ¨¡å‹å¤§å° (`dim`, `n_layers`)
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

### Q: ç”Ÿæˆçš„æ–‡æœ¬è´¨é‡ä¸å¥½?

**A:**
- å¢åŠ è®­ç»ƒæ­¥æ•° (`max_iters`)
- ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
- ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®
- è°ƒæ•´é‡‡æ ·å‚æ•° (`temperature`, `top_k`, `top_p`)

### Q: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®?

**A:** å‚è€ƒ [æ•°æ®å‡†å¤‡æŒ‡å—](data-preparation.md),ç®€è¦æ­¥éª¤:
1. å‡†å¤‡çº¯æ–‡æœ¬æ–‡ä»¶
2. ä½¿ç”¨ `DataLoader(file_path='your_data.txt')`
3. è®­ç»ƒåˆ†è¯å™¨å’Œæ¨¡å‹

### Q: ç®€å•æ¨¡å¼å’ŒåŒ…æ¨¡å¼æœ‰ä»€ä¹ˆåŒºåˆ«?

**A:**
- **ç®€å•æ¨¡å¼**: å•æ–‡ä»¶è„šæœ¬,é€‚åˆå¿«é€Ÿå®éªŒå’Œå­¦ä¹ 
- **åŒ…æ¨¡å¼**: æ¨¡å—åŒ–æ¶æ„,é€‚åˆç”Ÿäº§å’Œå®šåˆ¶å¼€å‘
- åŠŸèƒ½ç›¸åŒ,åªæ˜¯ç»„ç»‡æ–¹å¼ä¸åŒ

---

## ğŸ†˜ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜?

- ğŸ“š æŸ¥çœ‹ [å®Œæ•´æ–‡æ¡£](../README.md)
- ğŸ’¬ æäº¤ [GitHub Issue](https://github.com/your-org/llm-foundry/issues)
- ğŸ—¨ï¸ å‚ä¸ [GitHub Discussions](https://github.com/your-org/llm-foundry/discussions)

---

## ğŸ‰ æ­å–œ!

æ‚¨å·²ç»æˆåŠŸå®Œæˆå¿«é€Ÿå…¥é—¨!ç°åœ¨æ‚¨å¯ä»¥:
- âœ… è®­ç»ƒè‡ªå·±çš„è¯­è¨€æ¨¡å‹
- âœ… ç”Ÿæˆæ–‡æœ¬
- âœ… ç†è§£åŸºæœ¬å·¥ä½œæµç¨‹

ç»§ç»­æ¢ç´¢ LLM Foundry çš„æ›´å¤šåŠŸèƒ½å§! ğŸš€
