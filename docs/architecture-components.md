# æ ¸å¿ƒç»„ä»¶æ·±å…¥è§£æ

> **ç°ä»£ Transformer æ¶æ„çš„æ ¸å¿ƒæ„å»ºå—**

æœ¬æ–‡æ¡£æ·±å…¥è§£æ LLM Foundry ä¸­ä½¿ç”¨çš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬ RMSNormã€RoPEã€GQAã€SwiGLU ç­‰ç°ä»£ Transformer æŠ€æœ¯ã€‚

---

## ğŸ“– ç›®å½•

1. [Token Embedding](#1-token-embedding)
2. [RMSNorm](#2-rmsnorm-root-mean-square-normalization)
3. [RoPE](#3-rope-rotary-position-embedding)
4. [Grouped Query Attention (GQA)](#4-grouped-query-attention-gqa)
5. [Scaled Dot-Product Attention](#5-scaled-dot-product-attention)
6. [MLP with SwiGLU](#6-mlp-with-swiglu)
7. [Transformer Block](#7-transformer-block)
8. [ä»£ç å¯¼èˆª](#8-ä»£ç å¯¼èˆª)

---

## 1. Token Embedding

å°† token ID æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´ã€‚

```python
self.token_embedding = nn.Embedding(vocab_size, dim)
# è¾“å…¥: (batch, seq_len) - token IDs
# è¾“å‡º: (batch, seq_len, dim) - embeddings
```

**ç‰¹ç‚¹:**
- å¯è®­ç»ƒçš„åµŒå…¥çŸ©é˜µ
- ç»´åº¦: `vocab_size Ã— dim`
- åˆå§‹åŒ–: æ­£æ€åˆ†å¸ƒ (mean=0, std=0.02)

**ä»£ç ä½ç½®**: [src/llm_foundry/models/transformer.py](../../../src/llm_foundry/models/transformer.py)

---

## 2. RMSNorm (Root Mean Square Normalization)

å‡æ–¹æ ¹å½’ä¸€åŒ–ï¼Œæ¯” LayerNorm æ›´é«˜æ•ˆã€‚

### å®ç°

```python
class RMSNorm(nn.Module):
    def forward(self, x):
        # è®¡ç®— RMS
        var = torch.mean(x ** 2, dim=-1, keepdim=True)
        # å½’ä¸€åŒ–
        x_norm = x * torch.rsqrt(var + eps)
        # ç¼©æ”¾
        return self.weight * x_norm
```

### ä¸ºä»€ä¹ˆä½¿ç”¨ RMSNorm?

| ç‰¹æ€§ | LayerNorm | RMSNorm |
|------|-----------|---------|
| è®¡ç®—å¤æ‚åº¦ | é«˜ (mean + std) | ä½ (åªéœ€ RMS) |
| å‚æ•° | 2Ã—dim | 1Ã—dim |
| æ€§èƒ½ | åŸºçº¿ | æå‡ 5-10% |
| ç¨³å®šæ€§ | å¥½ | å¥½ |

### æ•°å­¦å…¬å¼

```
LayerNorm: y = (x - mean(x)) / std(x) * Î³ + Î²
RMSNorm:   y = x / RMS(x) * Î³
å…¶ä¸­ RMS(x) = sqrt(mean(xÂ²) + Îµ)
```

### ä¼˜åŠ¿
- âœ… è®¡ç®—æ›´å¿«ï¼ˆä¸éœ€è¦è®¡ç®—å‡å€¼ï¼‰
- âœ… å‚æ•°æ›´å°‘ï¼ˆåªæœ‰ç¼©æ”¾å‚æ•°ï¼Œæ— åç½®ï¼‰
- âœ… è®­ç»ƒç¨³å®šæ€§å¥½
- âœ… åœ¨ LLaMAã€Mistral ç­‰æ¨¡å‹ä¸­éªŒè¯æœ‰æ•ˆ

**ä»£ç ä½ç½®**: [src/llm_foundry/models/components.py:18](../../../src/llm_foundry/models/components.py#L18)

---

## 3. RoPE (Rotary Position Embedding)

æ—‹è½¬ä½ç½®ç¼–ç ï¼Œé€šè¿‡æ—‹è½¬å˜æ¢æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚

### å®ç°

```python
def apply_rotary_emb(xq, xk, freqs_cis):
    # å°†å®æ•°å¼ é‡è§†ä¸ºå¤æ•°
    xq_ = torch.view_as_complex(xq.reshape(..., -1, 2))
    xk_ = torch.view_as_complex(xk.reshape(..., -1, 2))

    # æ—‹è½¬
    xq_out = torch.view_as_real(xq_ * freqs_cis)
    xk_out = torch.view_as_real(xk_ * freqs_cis)

    return xq_out, xk_out
```

### ä¸ºä»€ä¹ˆä½¿ç”¨ RoPE?

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|
| ç»å¯¹ä½ç½®ç¼–ç  | ç®€å• | é•¿åº¦å¤–æ¨èƒ½åŠ›å·® |
| ç›¸å¯¹ä½ç½®ç¼–ç  | çµæ´» | è®¡ç®—å¤æ‚ |
| **RoPE** | **å¤–æ¨èƒ½åŠ›å¼º** | **å®ç°ç®€å•** |

### æ ¸å¿ƒæ€æƒ³

é€šè¿‡æ—‹è½¬çŸ©é˜µå¯¹ query å’Œ key è¿›è¡Œå˜æ¢ï¼Œä½¿å¾—æ³¨æ„åŠ›åˆ†æ•°éšå¼åœ°åŒ…å«ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚

```python
# é¢„è®¡ç®—æ—‹è½¬é¢‘ç‡
Î¸ = 10000^(-2i/d) for i in [0, d/2)
freqs = [Î¸â‚€, Î¸â‚, ..., Î¸_{d/2-1}]

# å¯¹äºä½ç½® m
m Ã— freqs = [mÃ—Î¸â‚€, mÃ—Î¸â‚, ..., mÃ—Î¸_{d/2-1}]

# è½¬æ¢ä¸ºå¤æ•°
e^(iÃ—mÃ—Î¸â±¼) = cos(mÃ—Î¸â±¼) + iÃ—sin(mÃ—Î¸â±¼)
```

### ä¼˜åŠ¿
- âœ… ç›¸å¯¹ä½ç½®ç¼–ç 
- âœ… é•¿åºåˆ—å¤–æ¨èƒ½åŠ›å¼º
- âœ… ä¸å¢åŠ å‚æ•°
- âœ… è®¡ç®—é«˜æ•ˆ

### å·¥ä½œåŸç†ç¤ºä¾‹

```
ä½ç½® 0 çš„ token: æ—‹è½¬ 0Â°
ä½ç½® 1 çš„ token: æ—‹è½¬ Î¸
ä½ç½® 2 çš„ token: æ—‹è½¬ 2Î¸
...

ä¸¤ä¸ª token ä¹‹é—´çš„ç›¸å¯¹ä½ç½®é€šè¿‡æ—‹è½¬è§’åº¦å·®ä½“ç°:
pos_i å’Œ pos_j çš„ç›¸å¯¹ä½ç½® = (j-i)Ã—Î¸
```

**ä»£ç ä½ç½®**: [src/llm_foundry/models/components.py:66](../../../src/llm_foundry/models/components.py#L66)

---

## 4. Grouped Query Attention (GQA)

åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼Œåœ¨ MHA å’Œ MQA ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

### å®ç°

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, cfg):
        self.n_heads = 8      # Query heads
        self.n_kv_heads = 4   # Key/Value heads (GQA)

        self.wq = nn.Linear(dim, n_heads * head_dim)
        self.wk = nn.Linear(dim, n_kv_heads * head_dim)
        self.wv = nn.Linear(dim, n_kv_heads * head_dim)

    def forward(self, x, freqs_cis):
        # æŠ•å½±
        q = self.wq(x)  # (B, S, n_heads * head_dim)
        k = self.wk(x)  # (B, S, n_kv_heads * head_dim)
        v = self.wv(x)  # (B, S, n_kv_heads * head_dim)

        # é‡å¤ KV heads ä»¥åŒ¹é… Q heads
        k = repeat_kv(k, n_heads // n_kv_heads)
        v = repeat_kv(v, n_heads // n_kv_heads)

        # æ³¨æ„åŠ›è®¡ç®—...
```

### æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”

| ç±»å‹ | Query Heads | KV Heads | å‚æ•° | é€Ÿåº¦ | è´¨é‡ |
|------|-------------|----------|------|------|------|
| MHA | 8 | 8 | æœ€å¤š | æ…¢ | æœ€å¥½ |
| **GQA** | **8** | **4** | **ä¸­ç­‰** | **ä¸­ç­‰** | **å¥½** |
| MQA | 8 | 1 | æœ€å°‘ | å¿« | å¯æ¥å— |

### é…ç½®ç¤ºä¾‹

```python
# æ ‡å‡†é…ç½® (GQA)
n_heads = 8
n_kv_heads = 4  # æ¯ 2 ä¸ª Q å…±äº« 1 ä¸ª KV

# MHA (Multi-Head Attention)
n_heads = 8
n_kv_heads = 8  # æ¯ä¸ª Q æœ‰ç‹¬ç«‹çš„ KV

# MQA (Multi-Query Attention)
n_heads = 8
n_kv_heads = 1  # æ‰€æœ‰ Q å…±äº« 1 ä¸ª KV
```

### ä¼˜åŠ¿
- âœ… å‡å°‘ KV Cache å¤§å°ï¼ˆæ¨ç†æ—¶é‡è¦ï¼‰
- âœ… é™ä½å‚æ•°é‡å’Œè®¡ç®—é‡
- âœ… è´¨é‡æŸå¤±å¾ˆå°
- âœ… é€‚åˆå¤§è§„æ¨¡æ¨¡å‹

### ä¸ºä»€ä¹ˆ GQA æœ‰æ•ˆ?

Query éœ€è¦å¤šæ ·æ€§æ¥æ•æ‰ä¸åŒçš„è¯­ä¹‰ç‰¹å¾ï¼Œä½† Key/Value çš„å…±äº«ä¸ä¼šæ˜¾è‘—å½±å“è¡¨è¾¾èƒ½åŠ›ã€‚GQA åœ¨ä¸¤è€…ä¹‹é—´æ‰¾åˆ°äº†æœ€ä½³å¹³è¡¡ç‚¹ã€‚

**ä»£ç ä½ç½®**: [src/llm_foundry/models/components.py:140](../../../src/llm_foundry/models/components.py#L140)

---

## 5. Scaled Dot-Product Attention

ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œä½¿ç”¨ PyTorch ä¼˜åŒ–å®ç°ã€‚

### PyTorch å®ç°

```python
# ä½¿ç”¨ PyTorch 2.0+ çš„é«˜æ•ˆå®ç°
output = F.scaled_dot_product_attention(
    query, key, value,
    attn_mask=None,
    dropout_p=dropout if training else 0.0,
    is_causal=True  # å› æœæ©ç 
)
```

### æ‰‹åŠ¨å®ç°ï¼ˆæ•™å­¦ç”¨ï¼‰

```python
# 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
scores = query @ key.transpose(-2, -1)  # (B, H, S, S)
scores = scores / sqrt(head_dim)         # ç¼©æ”¾

# 2. å› æœæ©ç ï¼ˆä¸‹ä¸‰è§’ï¼‰
mask = torch.triu(torch.ones(S, S) * float('-inf'), diagonal=1)
scores = scores + mask  # å±è”½æœªæ¥ä½ç½®

# 3. Softmax
probs = F.softmax(scores, dim=-1)
probs = dropout(probs)

# 4. åŠ æƒæ±‚å’Œ
output = probs @ value  # (B, H, S, D_h)
```

### å› æœæ©ç ç¤ºä¾‹

```
è¾“å…¥åºåˆ—: ["The", "cat", "sat"]

æ³¨æ„åŠ›çŸ©é˜µï¼ˆå…è®¸çœ‹åˆ°çš„ä½ç½®ï¼‰:
       The  cat  sat
The  [  1    0    0  ]
cat  [  1    1    0  ]
sat  [  1    1    1  ]

â†’ "cat" åªèƒ½çœ‹åˆ° "The" å’Œ "cat"
â†’ "sat" å¯ä»¥çœ‹åˆ°æ‰€æœ‰å‰é¢çš„è¯
```

### ä¸ºä»€ä¹ˆéœ€è¦ç¼©æ”¾?

```python
# ä¸ç¼©æ”¾çš„é—®é¢˜:
scores = Q @ K^T  # å€¼å¯èƒ½å¾ˆå¤§

# å½“ head_dim = 64 æ—¶:
# scores çš„æ–¹å·® â‰ˆ 64
# softmax ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±

# ç¼©æ”¾å:
scores = Q @ K^T / sqrt(head_dim)
# scores çš„æ–¹å·® â‰ˆ 1
# softmax æ¢¯åº¦æ›´ç¨³å®š
```

---

## 6. MLP with SwiGLU

å‰é¦ˆç½‘ç»œï¼Œä½¿ç”¨ SwiGLU æ¿€æ´»å‡½æ•°ã€‚

### å®ç°

```python
class MLP(nn.Module):
    def __init__(self, cfg):
        hidden_dim = 4 * cfg.dim
        hidden_dim = int(2 * hidden_dim / 3)  # SwiGLU æƒ¯ä¾‹

        self.w1 = nn.Linear(dim, hidden_dim)  # Gate
        self.w2 = nn.Linear(hidden_dim, dim)  # Down
        self.w3 = nn.Linear(dim, hidden_dim)  # Up

    def forward(self, x):
        # SwiGLU: (Swish(xW1) âŠ™ xW3) W2
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```

### æ¿€æ´»å‡½æ•°å¯¹æ¯”

| æ¿€æ´»å‡½æ•° | å…¬å¼ | æ€§èƒ½ |
|---------|------|------|
| ReLU | max(0, x) | åŸºçº¿ |
| GELU | x Ã— Î¦(x) | +1-2% |
| Swish | x Ã— Ïƒ(x) | +1-2% |
| **SwiGLU** | **Swish(xWâ‚) âŠ™ xWâ‚ƒ** | **+2-3%** |

### ä¸ºä»€ä¹ˆæ˜¯ 2/3?

```
æ ‡å‡† FFN: dim â†’ 4Ã—dim â†’ dim
å‚æ•°é‡: 8Ã—dimÂ²

SwiGLU éœ€è¦ä¸¤ä¸ªé—¨: dim â†’ hidden_dim (Ã—2 é—¨) â†’ dim
ä¸ºäº†ä¿æŒå‚æ•°é‡ç›¸è¿‘:
hidden_dim = (8Ã—dimÂ²) / (2Ã—2Ã—dim) = 2Ã—dim

ä½†å®é™…ä½¿ç”¨ 2/3 ç³»æ•°:
hidden_dim = int(2 * (4Ã—dim) / 3) â‰ˆ 2.67Ã—dim
```

### é—¨æ§æœºåˆ¶çš„ä¼˜åŠ¿

```python
# æ ‡å‡† FFN:
output = W2(activation(W1(x)))

# SwiGLU (é—¨æ§):
output = W2(activation(W1(x)) * W3(x))
         â†‘                      â†‘
         å†…å®¹å˜æ¢              é—¨æ§ä¿¡å·

# é—¨æ§å…è®¸ç½‘ç»œåŠ¨æ€é€‰æ‹©è¦ä¼ é€’çš„ä¿¡æ¯
```

**ä»£ç ä½ç½®**: [src/llm_foundry/models/components.py:248](../../../src/llm_foundry/models/components.py#L248)

---

## 7. Transformer Block

å®Œæ•´çš„ Transformer å±‚ï¼Œä½¿ç”¨ Pre-Normalizationã€‚

### å®ç°

```python
class Block(nn.Module):
    def forward(self, x, freqs_cis):
        # Pre-normalization
        h = x + self.attention(self.attention_norm(x), freqs_cis)
        out = h + self.feed_forward(self.ffn_norm(h))
        return out
```

### Pre-Norm vs Post-Norm

```
Post-Norm (åŸå§‹ Transformer):
x â†’ Attention â†’ Add â†’ Norm â†’ FFN â†’ Add â†’ Norm

Pre-Norm (ç°ä»£ LLM):
x â†’ Norm â†’ Attention â†’ Add â†’ Norm â†’ FFN â†’ Add
```

### ä¸ºä»€ä¹ˆä½¿ç”¨ Pre-Norm?

- âœ… è®­ç»ƒæ›´ç¨³å®š
- âœ… ä¸éœ€è¦å­¦ä¹ ç‡é¢„çƒ­ï¼ˆæˆ–å‡å°‘é¢„çƒ­æ­¥æ•°ï¼‰
- âœ… å¯ä»¥è®­ç»ƒæ›´æ·±çš„æ¨¡å‹
- âœ… æ¢¯åº¦æµåŠ¨æ›´å¥½

### å®Œæ•´æµç¨‹å›¾

```
è¾“å…¥ x (batch, seq_len, dim)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Attention Normalization  â”‚
â”‚  h1 = RMSNorm(x)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Causal Self-Attention    â”‚
â”‚  h2 = Attention(h1, RoPE) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Residual Connection      â”‚
â”‚  h3 = x + h2              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FFN Normalization        â”‚
â”‚  h4 = RMSNorm(h3)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feed-Forward (SwiGLU)    â”‚
â”‚  h5 = MLP(h4)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Residual Connection      â”‚
â”‚  output = h3 + h5         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡º output (batch, seq_len, dim)
```

**ä»£ç ä½ç½®**: [src/llm_foundry/models/components.py:280](../../../src/llm_foundry/models/components.py#L280)

---

## 8. ä»£ç å¯¼èˆª

### ä¸»å·¥ç¨‹å®ç°

- **å®Œæ•´æ¨¡å‹**: [src/llm_foundry/models/transformer.py](../../../src/llm_foundry/models/transformer.py)
- **ç»„ä»¶å®ç°**: [src/llm_foundry/models/components.py](../../../src/llm_foundry/models/components.py)
  - RMSNorm: ç¬¬ 18 è¡Œ
  - RoPE: ç¬¬ 66 è¡Œ
  - Attention: ç¬¬ 140 è¡Œ
  - MLP: ç¬¬ 248 è¡Œ
  - Block: ç¬¬ 280 è¡Œ

### æ•™å­¦å®ç°

- **æ•™å­¦ç‰ˆ Model**: [tutorials/model.py](../../../tutorials/model.py)
  - è¯¦ç»†æ³¨é‡Š
  - å•æ–‡ä»¶å®Œæ•´å®ç°
  - ä¸ä¸»å·¥ç¨‹åŠŸèƒ½å¯¹ç­‰

### æµ‹è¯•ç”¨ä¾‹

- **å•å…ƒæµ‹è¯•**: [tests/test_models.py](../../../tests/test_models.py)
  - ç»„ä»¶æµ‹è¯•
  - å½¢çŠ¶éªŒè¯
  - æ•°å€¼æ­£ç¡®æ€§

---

## å‚æ•°é‡è®¡ç®—ç¤ºä¾‹

### Small é…ç½®

```python
ModelConfig(
    dim=256,
    n_layers=4,
    n_heads=8,
    n_kv_heads=4,
    vocab_size=8192,
    max_seq_len=256
)
```

**å‚æ•°åˆ†è§£:**

| ç»„ä»¶ | å‚æ•°é‡ | å…¬å¼ |
|------|--------|------|
| Token Embedding | 2.1M | vocab_size Ã— dim |
| **æ¯ä¸ª Block** | | |
| - RMSNorm (Ã—2) | 512 | 2 Ã— dim |
| - Attention Q | 65K | dim Ã— (n_heads Ã— head_dim) |
| - Attention K | 33K | dim Ã— (n_kv_heads Ã— head_dim) |
| - Attention V | 33K | dim Ã— (n_kv_heads Ã— head_dim) |
| - Attention Out | 65K | (n_heads Ã— head_dim) Ã— dim |
| - MLP W1 | 87K | dim Ã— hidden_dim |
| - MLP W2 | 87K | hidden_dim Ã— dim |
| - MLP W3 | 87K | dim Ã— hidden_dim |
| **Block å°è®¡** | **458K** | |
| **4 ä¸ª Block** | **1.83M** | 458K Ã— 4 |
| Output Layer | 2.1M | dim Ã— vocab_size |
| **æ€»è®¡** | **~2.08M** | |

---

## ç›¸å…³æ–‡æ¡£

- [æ¶æ„æ¦‚è§ˆ](README.md) - æ•´ä½“æ¶æ„è®¾è®¡
- [è®­ç»ƒç³»ç»Ÿ](training-system.md) - LLM è®­ç»ƒå®Œæ•´çŸ¥è¯†
- [è®¾è®¡å†³ç­–](design-decisions.md) - ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›æŠ€æœ¯
- [å­¦ä¹ è·¯å¾„](../../../LEARNING_PATH.md) - æŒ‰æ­¥éª¤å­¦ä¹ 

---

## å»¶ä¼¸é˜…è¯»

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) - RoPE è®ºæ–‡
- [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467) - RMSNorm è®ºæ–‡
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) - GQA, SwiGLU åº”ç”¨
- [GQA: Training Generalized Multi-Query Transformer Models](https://arxiv.org/abs/2305.13245) - GQA è®ºæ–‡

---

**æ·±å…¥ç†è§£æ¯ä¸ªç»„ä»¶ï¼ŒæŒæ¡ç°ä»£ Transformer çš„æ ¸å¿ƒæŠ€æœ¯ï¼** ğŸš€
