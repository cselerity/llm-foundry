"""å®‰è£…éªŒè¯è„šæœ¬

å¿«é€ŸéªŒè¯ LLM Foundry çš„å®‰è£…å’Œç¯å¢ƒé…ç½®ã€‚

è¿è¡Œ:
    python scripts/verify_installation.py
"""

import sys
import importlib.util


def check_python_version():
    """æ£€æŸ¥ Python ç‰ˆæœ¬"""
    print("=" * 60)
    print("1. æ£€æŸ¥ Python ç‰ˆæœ¬")
    print("=" * 60)
    
    version = sys.version_info
    print(f"Python ç‰ˆæœ¬: {version.major}.{version.minor}.{version.micro}")
    
    if version.major < 3 or (version.major == 3 and version.minor < 8):
        print("âŒ Python ç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦ >= 3.8")
        return False
    else:
        print("âœ… Python ç‰ˆæœ¬ç¬¦åˆè¦æ±‚\n")
        return True


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åŒ…"""
    print("=" * 60)
    print("2. æ£€æŸ¥ä¾èµ–åŒ…")
    print("=" * 60)
    
    dependencies = {
        'torch': 'PyTorch',
        'numpy': 'NumPy',
        'sentencepiece': 'SentencePiece',
    }
    
    all_ok = True
    for module, name in dependencies.items():
        try:
            mod = importlib.import_module(module)
            version = getattr(mod, '__version__', 'unknown')
            print(f"âœ… {name:15s} {version}")
        except ImportError:
            print(f"âŒ {name:15s} æœªå®‰è£…")
            all_ok = False
    
    print()
    return all_ok


def check_gpu():
    """æ£€æŸ¥ GPU å¯ç”¨æ€§"""
    print("=" * 60)
    print("3. æ£€æŸ¥ GPU")
    print("=" * 60)
    
    try:
        import torch
        
        # æ£€æŸ¥ CUDA
        if torch.cuda.is_available():
            print(f"âœ… CUDA å¯ç”¨")
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
            props = torch.cuda.get_device_properties(0)
            print(f"   æ˜¾å­˜: {props.total_memory / 1e9:.1f} GB")
            print(f"   è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        else:
            print("âš ï¸  CUDA ä¸å¯ç”¨")
        
        # æ£€æŸ¥ MPS (Apple Silicon)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print(f"âœ… MPS (Apple Silicon) å¯ç”¨")
        else:
            print("âš ï¸  MPS ä¸å¯ç”¨")
        
        # æ¨èè®¾å¤‡
        if torch.cuda.is_available():
            device = 'cuda'
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = 'mps'
        else:
            device = 'cpu'
        
        print(f"\næ¨èä½¿ç”¨è®¾å¤‡: {device}")
        print()
        return True
        
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥ GPU")
        print()
        return False


def check_llm_foundry():
    """æ£€æŸ¥ LLM Foundry å®‰è£…"""
    print("=" * 60)
    print("4. æ£€æŸ¥ LLM Foundry")
    print("=" * 60)
    
    try:
        import llm_foundry
        print(f"âœ… LLM Foundry å·²å®‰è£…")
        print(f"   ç‰ˆæœ¬: {llm_foundry.__version__}")
        
        # æ£€æŸ¥ä¸»è¦æ¨¡å—
        modules = [
            'ModelConfig',
            'TrainConfig',
            'MiniLLM',
            'Tokenizer',
            'DataLoader',
            'get_device',
        ]
        
        print("\næ£€æŸ¥æ¨¡å—å¯¼å…¥:")
        all_ok = True
        for module in modules:
            try:
                getattr(llm_foundry, module)
                print(f"   âœ… {module}")
            except AttributeError:
                print(f"   âŒ {module} ä¸å¯ç”¨")
                all_ok = False
        
        print()
        return all_ok
        
    except ImportError:
        print("âŒ LLM Foundry æœªå®‰è£…")
        print("\nè¯·è¿è¡Œ: pip install -e .")
        print()
        return False


def run_quick_test():
    """è¿è¡Œå¿«é€Ÿæµ‹è¯•"""
    print("=" * 60)
    print("5. å¿«é€ŸåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        import torch
        from llm_foundry import ModelConfig, MiniLLM, get_device
        
        # æµ‹è¯•è®¾å¤‡æ£€æµ‹
        device = get_device()
        print(f"âœ… è®¾å¤‡æ£€æµ‹: {device}")
        
        # æµ‹è¯•æ¨¡å‹åˆ›å»º
        cfg = ModelConfig(dim=128, n_layers=2, n_heads=4, n_kv_heads=2)
        model = MiniLLM(cfg)
        print(f"âœ… æ¨¡å‹åˆ›å»º: {model.get_num_params()/1e6:.2f}M å‚æ•°")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        tokens = torch.randint(0, cfg.vocab_size, (2, 16))
        logits, loss = model(tokens, tokens)
        print(f"âœ… å‰å‘ä¼ æ’­: logits shape {logits.shape}")
        
        print()
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("LLM Foundry å®‰è£…éªŒè¯")
    print("=" * 60)
    print()
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æ£€æŸ¥
    results.append(("Python ç‰ˆæœ¬", check_python_version()))
    results.append(("ä¾èµ–åŒ…", check_dependencies()))
    results.append(("GPU", check_gpu()))
    results.append(("LLM Foundry", check_llm_foundry()))
    results.append(("åŠŸèƒ½æµ‹è¯•", run_quick_test()))
    
    # æ€»ç»“
    print("=" * 60)
    print("éªŒè¯æ€»ç»“")
    print("=" * 60)
    
    for name, ok in results:
        status = "âœ… é€šè¿‡" if ok else "âŒ å¤±è´¥"
        print(f"{name:15s} {status}")
    
    all_passed = all(ok for _, ok in results)
    
    print()
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼")
        print("\nä¸‹ä¸€æ­¥:")
        print("  1. cd tutorials")
        print("  2. python train.py")
        print("  3. python generate.py")
    else:
        print("âš ï¸  éƒ¨åˆ†æ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯ä¿®å¤é—®é¢˜ã€‚")
        print("\nå¸¸è§é—®é¢˜:")
        print("  - ä¾èµ–æœªå®‰è£…: pip install -e .")
        print("  - LLM Foundry æœªå®‰è£…: pip install -e .")
        print("  - GPU ä¸å¯ç”¨: æ£€æŸ¥ CUDA/é©±åŠ¨å®‰è£…")
    
    print()
    return 0 if all_passed else 1


if __name__ == '__main__':
    sys.exit(main())
