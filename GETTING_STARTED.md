# LLM Foundry å¿«é€Ÿå¼€å§‹

> **5-10 åˆ†é’Ÿä¸Šæ‰‹ï¼Œè®­ç»ƒæ‚¨çš„ç¬¬ä¸€ä¸ªè¯­è¨€æ¨¡å‹**

æ¬¢è¿ä½¿ç”¨ LLM Foundryï¼æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨å¿«é€Ÿå¼€å§‹è®­ç»ƒå’Œä½¿ç”¨æ‚¨çš„ç¬¬ä¸€ä¸ªè¯­è¨€æ¨¡å‹ã€‚

---

## ğŸ“‹ å‰ç½®è¦æ±‚

- **Python** 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **RAM** è‡³å°‘ 4GB
- **GPU** (å¯é€‰) NVIDIA GPU ç”¨äºåŠ é€Ÿè®­ç»ƒ

### æ£€æŸ¥ç¯å¢ƒ

```bash
python --version  # åº”è¯¥ >= 3.8
pip --version     # ç¡®ä¿ pip å·²å®‰è£…
```

---

## ğŸš€ å®‰è£…

### æ–¹æ³• 1: ä»æºç å®‰è£… (æ¨è)

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/your-org/llm-foundry.git
cd llm-foundry

# 2. å®‰è£…ä¾èµ–
pip install -e .

# 3. (å¯é€‰) å®‰è£…å¼€å‘å·¥å…·
pip install -r requirements-dev.txt
```

### æ–¹æ³• 2: ä½¿ç”¨ pip å®‰è£…

```bash
pip install llm-foundry
```

### éªŒè¯å®‰è£…

```python
import llm_foundry
print(f"LLM Foundry ç‰ˆæœ¬: {llm_foundry.__version__}")

# æµ‹è¯•å¯¼å…¥
from llm_foundry import ModelConfig, MiniLLM
print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ!")
```

---

## ğŸ¯ ç¬¬ä¸€ä¸ªæ¨¡å‹ (5 åˆ†é’Ÿ)

### æ­¥éª¤ 1: è®­ç»ƒæ¨¡å‹

```bash
cd tutorials
python train.py
```

**æ­£åœ¨å‘ç”Ÿä»€ä¹ˆ?**

1. ğŸ“¥ **ä¸‹è½½æ•°æ®**: è‡ªåŠ¨ä¸‹è½½çº¢æ¥¼æ¢¦æ•°æ®é›† (~100KB)
2. ğŸ”¤ **è®­ç»ƒåˆ†è¯å™¨**: ä½¿ç”¨ SentencePiece BPE (è¯è¡¨å¤§å°: 8192)
3. ğŸ§  **è®­ç»ƒæ¨¡å‹**: è®­ç»ƒ Mini LLM æ¨¡å‹ (é»˜è®¤ 100 æ­¥ï¼Œ~2M å‚æ•°)
4. ğŸ’¾ **ä¿å­˜æ£€æŸ¥ç‚¹**: ä¿å­˜åˆ° `minillm.pt`

**é¢„æœŸè¾“å‡º:**

```
ä½¿ç”¨è®¾å¤‡: cuda
æ­£åœ¨ä¸‹è½½æ•°æ®...
æ­£åœ¨è®­ç»ƒ Tokenizer (vocab_size=8192)...
Tokenizer è®­ç»ƒå®Œæˆã€‚
æ•°æ®åŠ è½½å®Œæˆã€‚æ€» token æ•°: 145234
æ¨¡å‹å‚æ•°é‡: 2.08M
å¼€å§‹è®­ç»ƒ...
step 0: train loss 9.1234, val loss 9.2345
step 50: train loss 5.6789, val loss 5.7890
è®­ç»ƒå®Œæˆï¼Œè€—æ—¶ 32.45s
æ¨¡å‹å·²ä¿å­˜è‡³ minillm.pt
```

---

### æ­¥éª¤ 2: ç”Ÿæˆæ–‡æœ¬

```bash
python generate.py
```

**é¢„æœŸè¾“å‡º:**

```
ä½¿ç”¨è®¾å¤‡: cuda
å·²åŠ è½½ Checkpoint 'minillm.pt'

æç¤ºè¯: æ»¡çº¸è’å”è¨€ï¼Œ
æ­£åœ¨ç”Ÿæˆ...

--- ç”Ÿæˆçš„æ–‡æœ¬ ---
æ»¡çº¸è’å”è¨€ï¼Œä¸€æŠŠè¾›é…¸æ³ªã€‚éƒ½äº‘ä½œè€…ç—´ï¼Œè°è§£å…¶ä¸­å‘³ï¼Ÿ...
```

---

### æ­¥éª¤ 3: ç†è§£è¾“å‡º

**è®­ç»ƒè¿‡ç¨‹:**
- `train loss`: è®­ç»ƒé›†æŸå¤±ï¼Œåº”è¯¥é€æ­¥ä¸‹é™
- `val loss`: éªŒè¯é›†æŸå¤±ï¼Œè¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›
- æŸå¤±ä» ~9 é™åˆ° ~5-6 æ˜¯æ­£å¸¸çš„

**ç”Ÿæˆè´¨é‡:**
- åˆå§‹æ¨¡å‹ (100 æ­¥): å¯èƒ½ç”Ÿæˆè¾ƒçŸ­æˆ–é‡å¤çš„æ–‡æœ¬
- éœ€è¦æ›´å¤šè®­ç»ƒæ­¥æ•° (1000-5000 æ­¥) è·å¾—æ›´å¥½è´¨é‡

---

## ğŸ“ ä¸¤ç§ä½¿ç”¨æ¨¡å¼

LLM Foundry æä¾›ä¸¤ç§ä½¿ç”¨æ¨¡å¼ï¼Œé€‚åº”ä¸åŒçš„éœ€æ±‚:

### æ¨¡å¼ 1: æ•™å­¦æ¨¡å¼ (tutorials/)

**é€‚åˆ**: å­¦ä¹ ã€æ•™å­¦ã€å¿«é€Ÿå®éªŒ

```bash
cd tutorials
python train.py      # è®­ç»ƒæ¨¡å‹
python generate.py   # ç”Ÿæˆæ–‡æœ¬
```

**ç‰¹ç‚¹:**
- âœ… å•æ–‡ä»¶å®Œæ•´å®ç°
- âœ… è¯¦ç»†æ³¨é‡Šè¯´æ˜
- âœ… ç‹¬ç«‹è¿è¡Œ
- âœ… æ•™å­¦ä¼˜å…ˆ

---

### æ¨¡å¼ 2: åŒ…æ¨¡å¼ (src/)

**é€‚åˆ**: ç ”ç©¶ã€ç”Ÿäº§ã€å®šåˆ¶åŒ–å¼€å‘

```python
from llm_foundry import ModelConfig, MiniLLM, DataLoader
from llm_foundry.utils import get_device

# 1. é…ç½®
cfg = ModelConfig(
    dim=512,
    n_layers=8,
    n_heads=8,
    vocab_size=8192,
    max_seq_len=512
)

# 2. åˆ›å»ºæ¨¡å‹
device = get_device()
model = MiniLLM(cfg).to(device)
print(f"æ¨¡å‹å‚æ•°é‡: {model.get_num_params()/1e6:.2f}M")

# 3. åŠ è½½æ•°æ®
loader = DataLoader(
    file_path='data/your_text.txt',
    batch_size=32,
    block_size=cfg.max_seq_len,
    device=device
)

# 4. è®­ç»ƒ (ç®€åŒ–ç¤ºä¾‹)
import torch
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

model.train()
for step in range(1000):
    x, y = loader.get_batch('train')
    logits, loss = model(x, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % 50 == 0:
        print(f"Step {step}: loss {loss.item():.4f}")

# 5. ä¿å­˜
torch.save(model.state_dict(), 'model.pt')
```

**ç‰¹ç‚¹:**
- âœ… æ¨¡å—åŒ– API
- âœ… ç”Ÿäº§å°±ç»ª
- âœ… æ˜“äºé›†æˆ
- âœ… æ€§èƒ½ä¼˜å…ˆ

---

## ğŸ“Š è‡ªå®šä¹‰é…ç½®

### æ–¹æ³• 1: ç¼–è¾‘é…ç½®æ–‡ä»¶ (æ•™å­¦æ¨¡å¼)

ç¼–è¾‘ `tutorials/config.py`:

```python
@dataclass
class ModelConfig:
    dim: int = 512          # å¢å¤§æ¨¡å‹ç»´åº¦
    n_layers: int = 8       # å¢åŠ å±‚æ•°
    n_heads: int = 8
    n_kv_heads: int = 4
    vocab_size: int = 8192
    max_seq_len: int = 512  # å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦

@dataclass
class TrainConfig:
    batch_size: int = 16
    learning_rate: float = 3e-4
    max_iters: int = 5000   # è®­ç»ƒæ›´å¤šæ­¥æ•°
    eval_interval: int = 100
```

**é‡æ–°è®­ç»ƒ:**

```bash
cd tutorials
python train.py
```

---

### æ–¹æ³• 2: ä½¿ç”¨é¢„è®¾é…ç½®

```python
# å°å‹æ¨¡å‹ (é€‚åˆå­¦ä¹ , CPU)
from tutorials.config import get_small_config, get_small_train_config
model_cfg = get_small_config()      # ~2M å‚æ•°
train_cfg = get_small_train_config()

# ä¸­å‹æ¨¡å‹ (é€‚åˆå®éªŒ)
from tutorials.config import get_medium_config
model_cfg = get_medium_config()     # ~10M å‚æ•°

# RTX 5060 ä¼˜åŒ–é…ç½®
from tutorials.config import get_rtx5060_config
model_cfg = get_rtx5060_config()    # ~70M å‚æ•°
```

---

## ğŸ”§ æ ¹æ®ç”¨ä¾‹é€‰æ‹©ä¸‹ä¸€æ­¥

### ğŸ’¡ ç³»ç»Ÿå­¦ä¹ 

æƒ³è¦æ·±å…¥ç†è§£ LLM åŸç†ï¼Ÿ

â†’ **[å­¦ä¹ è·¯å¾„ (LEARNING_PATH.md)](LEARNING_PATH.md)**
   - 5 é˜¶æ®µç»“æ„åŒ–å­¦ä¹ 
   - 10-15 å°æ—¶å®Œæ•´è¯¾ç¨‹
   - ä»åŸºç¡€åˆ°é«˜çº§

â†’ **[æ¶æ„è¯¦è§£](docs/)**
   - [æ ¸å¿ƒç»„ä»¶](docs/architecture-components.md) - RMSNorm, RoPE, GQA
   - [è®­ç»ƒç³»ç»Ÿ](docs/architecture-training.md) - å®Œæ•´è®­ç»ƒçŸ¥è¯†
   - [è®¾è®¡å†³ç­–](docs/architecture-design.md) - æŠ€æœ¯é€‰å‹

---

### ğŸ–¥ï¸ ç‰¹å®šç¡¬ä»¶ä¼˜åŒ–

éœ€è¦åœ¨ç‰¹å®šç¡¬ä»¶ä¸Šä¼˜åŒ–ï¼Ÿ

â†’ **[ç¡¬ä»¶æŒ‡å—](docs/)**
   - [RTX 5060 æŒ‡å—](docs/hardware-rtx5060.md) - 8GB GPU ä¼˜åŒ–
   - [Apple Silicon æŒ‡å—](docs/hardware-apple.md) - M4 Pro ä¼˜åŒ–
   - [é…ç½®é€ŸæŸ¥è¡¨](docs/hardware-config.md) - å¿«é€Ÿå‚è€ƒ

**ç¡¬ä»¶é€‰æ‹©å‚è€ƒ:**

| ç¡¬ä»¶ | æ¨¡å‹è§„æ¨¡ | è®­ç»ƒæ—¶é—´* | æŒ‡å— |
|------|---------|----------|------|
| CPU | 2M | 10-30min | ä½¿ç”¨ small é…ç½® |
| RTX 5060 (8GB) | 70M | 30-40min | [RTX 5060 æŒ‡å—](docs/hardware-rtx5060.md) |
| Apple M4 Pro | 68M | 40-60min | [Apple Silicon æŒ‡å—](docs/hardware-apple.md) |
| RTX 4090 (24GB) | 200M+ | 10-20min | è‡ªå®šä¹‰é…ç½® |

*åŸºäº 10k training steps

---

### ğŸ“ ä½¿ç”¨è‡ªå·±çš„æ•°æ®

éœ€è¦åœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè®­ç»ƒï¼Ÿ

â†’ **[è‡ªå®šä¹‰æ•°æ®æŒ‡å—](docs/guides-data.md)** (å¾…åˆ›å»º)

**å¿«é€Ÿæ­¥éª¤:**

```python
# 1. å‡†å¤‡çº¯æ–‡æœ¬æ–‡ä»¶
# your_data.txt

# 2. ä½¿ç”¨æ•™å­¦æ¨¡å¼
cd tutorials
# ç¼–è¾‘ train.pyï¼Œä¿®æ”¹ data_file è·¯å¾„
python train.py

# 3. æˆ–ä½¿ç”¨åŒ…æ¨¡å¼
from llm_foundry import DataLoader

loader = DataLoader(
    file_path='your_data.txt',
    batch_size=32,
    block_size=256
)
```

---

### ğŸš€ ç”Ÿäº§éƒ¨ç½²

å‡†å¤‡éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Ÿ

â†’ **[ç”Ÿäº§éƒ¨ç½²](docs/)**
   - [åˆ†å¸ƒå¼è®­ç»ƒ](docs/production-distributed.md) - å¤š GPU è®­ç»ƒ
   - [æ··åˆç²¾åº¦](docs/production-mixed.md) - FP16/BF16 åŠ é€Ÿ
   - [æ¨¡å‹æœåŠ¡](docs/production-serving.md) - API éƒ¨ç½²
   - [æ¨ç†ä¼˜åŒ–](docs/production-optimize.md) - é‡åŒ–å’ŒåŠ é€Ÿ

---

## â“ æ•…éšœæ’é™¤

### é—®é¢˜ 1: è®­ç»ƒå¤ªæ…¢

**ç—‡çŠ¶:** è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢ï¼Œæ¯æ­¥éœ€è¦å¾ˆé•¿æ—¶é—´

**è§£å†³æ–¹æ¡ˆ:**
1. **æ£€æŸ¥ GPU**:
   ```python
   import torch
   print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
   print(f"å½“å‰è®¾å¤‡: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
   ```
2. **å‡å°æ¨¡å‹**: é™ä½ `dim` æˆ– `n_layers`
3. **å‡å°æ‰¹æ¬¡**: é™ä½ `batch_size`
4. **ä½¿ç”¨æ··åˆç²¾åº¦**: å‚è€ƒ [æ··åˆç²¾åº¦è®­ç»ƒ](docs/production-mixed.md)

---

### é—®é¢˜ 2: CUDA Out of Memory (OOM)

**ç—‡çŠ¶:** è®­ç»ƒæ—¶å‡ºç°æ˜¾å­˜ä¸è¶³é”™è¯¯

**è§£å†³æ–¹æ¡ˆ:**
```python
# æ–¹æ³• 1: å‡å° batch_size
train_cfg.batch_size = 16  # æˆ–æ›´å°

# æ–¹æ³• 2: å‡å° max_seq_len
model_cfg.max_seq_len = 128  # æˆ–æ›´å°

# æ–¹æ³• 3: å‡å°æ¨¡å‹å¤§å°
model_cfg.dim = 256
model_cfg.n_layers = 4

# æ–¹æ³• 4: ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
# åœ¨ train.py ä¸­å®ç°
```

---

### é—®é¢˜ 3: ç”Ÿæˆçš„æ–‡æœ¬è´¨é‡ä¸å¥½

**ç—‡çŠ¶:** ç”Ÿæˆæ–‡æœ¬ä¸è¿è´¯æˆ–é‡å¤

**è§£å†³æ–¹æ¡ˆ:**
1. **å¢åŠ è®­ç»ƒæ­¥æ•°**:
   ```python
   train_cfg.max_iters = 5000  # è€Œä¸æ˜¯ 100
   ```
2. **ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹**:
   ```python
   model_cfg = get_medium_config()  # 10M å‚æ•°
   ```
3. **ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®**: ç¡®ä¿æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ–‡æœ¬
4. **è°ƒæ•´é‡‡æ ·å‚æ•°**:
   ```python
   # åœ¨ generate.py ä¸­
   temperature = 0.8   # é™ä½éšæœºæ€§
   top_k = 50          # é™åˆ¶å€™é€‰è¯
   top_p = 0.9         # æ ¸é‡‡æ ·
   ```

---

### é—®é¢˜ 4: æ‰¾ä¸åˆ°æ¨¡å—

**ç—‡çŠ¶:** `ModuleNotFoundError: No module named 'llm_foundry'`

**è§£å†³æ–¹æ¡ˆ:**
```bash
# ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•
cd llm-foundry

# é‡æ–°å®‰è£…
pip install -e .

# éªŒè¯å®‰è£…
python -c "import llm_foundry; print('OK')"
```

---

## ğŸ’¬ è·å–å¸®åŠ©

é‡åˆ°å…¶ä»–é—®é¢˜ï¼Ÿ

- ğŸ“š **æŸ¥çœ‹å®Œæ•´æ–‡æ¡£**: [docs/README.md](docs/README.md)
- ğŸ› **æäº¤ Issue**: [GitHub Issues](https://github.com/your-org/llm-foundry/issues)
- ğŸ’¬ **è®¨è®º**: [GitHub Discussions](https://github.com/your-org/llm-foundry/discussions)
- ğŸ“– **å­¦ä¹ è·¯å¾„**: [LEARNING_PATH.md](LEARNING_PATH.md)

---

## ğŸ‰ æ­å–œï¼

æ‚¨å·²ç»æˆåŠŸå®Œæˆå¿«é€Ÿå…¥é—¨ï¼ç°åœ¨æ‚¨å¯ä»¥:
- âœ… è®­ç»ƒè‡ªå·±çš„è¯­è¨€æ¨¡å‹
- âœ… ç”Ÿæˆæ–‡æœ¬
- âœ… ç†è§£åŸºæœ¬å·¥ä½œæµç¨‹
- âœ… é€‰æ‹©é€‚åˆæ‚¨çš„å­¦ä¹ è·¯å¾„

### æ¨èä¸‹ä¸€æ­¥

**å¦‚æœæ‚¨æƒ³...**

- **æ·±å…¥å­¦ä¹ ** â†’ [LEARNING_PATH.md](LEARNING_PATH.md) (ç¬¬ 1 é˜¶æ®µ)
- **ç†è§£æ¶æ„** â†’ [æ¶æ„ç»„ä»¶](docs/architecture-components.md)
- **ä¼˜åŒ–ç¡¬ä»¶** â†’ [ç¡¬ä»¶é…ç½®](docs/)
- **ç”Ÿäº§éƒ¨ç½²** â†’ [ç”Ÿäº§éƒ¨ç½²](docs/)

---

**ç»§ç»­æ¢ç´¢ LLM Foundry çš„æ›´å¤šåŠŸèƒ½å§ï¼** ğŸš€
