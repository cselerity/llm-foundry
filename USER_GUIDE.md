# LLM Foundry ç”¨æˆ·æŒ‡å—

> **ä»å¿«é€Ÿä¸Šæ‰‹åˆ°æ·±å…¥æŒæ¡çš„å®Œæ•´æŒ‡å—**

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨ä»é›¶å¼€å§‹ä½¿ç”¨ LLM Foundryï¼Œä»å¿«é€Ÿä½“éªŒç¬¬ä¸€ä¸ªæ¨¡å‹åˆ°æ·±å…¥ç†è§£ Transformer æ¶æ„ã€‚

---

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿä¸Šæ‰‹](#å¿«é€Ÿä¸Šæ‰‹-5-10-åˆ†é’Ÿ)
- [ç³»ç»Ÿå­¦ä¹ ](#ç³»ç»Ÿå­¦ä¹ -10-15-å°æ—¶)
- [æ·±å…¥ç†è§£](#æ·±å…¥ç†è§£)
- [å®è·µåº”ç”¨](#å®è·µåº”ç”¨)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## å¿«é€Ÿä¸Šæ‰‹ (5-10 åˆ†é’Ÿ)

### å‰ç½®è¦æ±‚

- **Python** 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **RAM** è‡³å°‘ 4GB
- **GPU** (å¯é€‰) NVIDIA GPU ç”¨äºåŠ é€Ÿè®­ç»ƒ

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-org/llm-foundry.git
cd llm-foundry

# å®‰è£…ä¾èµ–
pip install -e .

# éªŒè¯å®‰è£…
python -c "import llm_foundry; print('OK')"
```

### è®­ç»ƒç¬¬ä¸€ä¸ªæ¨¡å‹

```bash
cd tutorials
python train.py      # è®­ç»ƒæ¨¡å‹ (~30 ç§’)
python generate.py   # ç”Ÿæˆæ–‡æœ¬
```

**é¢„æœŸè¾“å‡º**:
```
ä½¿ç”¨è®¾å¤‡: cuda
æ¨¡å‹å‚æ•°é‡: 2.08M
step 0: train loss 9.1234, val loss 9.2345
step 50: train loss 5.6789, val loss 5.7890
è®­ç»ƒå®Œæˆï¼Œè€—æ—¶ 32.45s
```

### ä¸¤ç§ä½¿ç”¨æ¨¡å¼

**æ•™å­¦æ¨¡å¼ (tutorials/)** - é€‚åˆå­¦ä¹ ã€æ•™å­¦ã€å¿«é€Ÿå®éªŒ
```bash
cd tutorials
python train.py
python generate.py
```

**åŒ…æ¨¡å¼ (src/)** - é€‚åˆç ”ç©¶ã€ç”Ÿäº§ã€å®šåˆ¶åŒ–å¼€å‘
```python
from llm_foundry import ModelConfig, MiniLLM, DataLoader

cfg = ModelConfig(dim=512, n_layers=8)
model = MiniLLM(cfg)
# ...
```

---

## ç³»ç»Ÿå­¦ä¹  (10-15 å°æ—¶)

### ç¬¬ä¸€é˜¶æ®µ: ç†è§£æ ¸å¿ƒä»£ç  (2-3 å°æ—¶)

#### 1. é…ç½®ç³»ç»Ÿ
ğŸ“– é˜…è¯»: [tutorials/config.py](tutorials/config.py)
- `ModelConfig`: æ¨¡å‹æ¶æ„é…ç½®
- `TrainConfig`: è®­ç»ƒè¶…å‚æ•°é…ç½®

#### 2. æ¨¡å‹æ¶æ„ - æ ¸å¿ƒç»„ä»¶
ğŸ“– é˜…è¯»: [src/llm_foundry/models/components.py](src/llm_foundry/models/components.py)

**RMSNorm** (ç¬¬18-64è¡Œ)
- å½’ä¸€åŒ–æŠ€æœ¯ï¼Œæ¯” LayerNorm æ›´é«˜æ•ˆ
- åªéœ€è®¡ç®— RMSï¼Œä¸éœ€è¦å‡å€¼

**RoPE ä½ç½®ç¼–ç ** (ç¬¬66-148è¡Œ)
- é€šè¿‡æ—‹è½¬ç¼–ç ä½ç½®ä¿¡æ¯
- ç›¸å¯¹ä½ç½®åœ¨ç‚¹ç§¯ä¸­è‡ªç„¶ä½“ç°

**æ³¨æ„åŠ›æœºåˆ¶** (ç¬¬150-258è¡Œ)
- Self-Attention å’Œ GQA (åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›)
- GQA å‡å°‘ KV Cache å¤§å°ï¼Œé™ä½å‚æ•°é‡

**å‰é¦ˆç½‘ç»œ** (ç¬¬260-278è¡Œ)
- SwiGLU æ¿€æ´»å‡½æ•°
- é—¨æ§æœºåˆ¶æå‡æ€§èƒ½

**Transformer å—** (ç¬¬280-312è¡Œ)
- Pre-normalization æ¶æ„
- æ®‹å·®è¿æ¥

#### 3. å®Œæ•´æ¨¡å‹
ğŸ“– é˜…è¯»: [src/llm_foundry/models/transformer.py](src/llm_foundry/models/transformer.py)
- ç†è§£å¦‚ä½•ç»„è£…å„ä¸ªç»„ä»¶
- æŸ¥çœ‹ `forward` æ–¹æ³•çš„å®Œæ•´æµç¨‹

#### 4. æ•°æ®å¤„ç†å’Œåˆ†è¯å™¨
ğŸ“– é˜…è¯»:
- [tutorials/data.py](tutorials/data.py) - æ•°æ®åŠ è½½æµç¨‹
- [tutorials/tokenizer.py](tutorials/tokenizer.py) - BPE åˆ†è¯åŸç†

---

### ç¬¬äºŒé˜¶æ®µ: æ·±å…¥ç†è§£ (3-4 å°æ—¶)

#### 5. æ¶æ„æ·±åº¦è§£æ
ğŸ“– é˜…è¯»: [docs/architecture-components.md](docs/architecture-components.md)

**æ ¸å¿ƒç»„ä»¶è¯¦è§£**:
- Token Embedding
- RMSNorm vs LayerNorm
- RoPE å·¥ä½œåŸç†
- GQA vs MHA vs MQA
- SwiGLU é—¨æ§æœºåˆ¶
- Pre-Norm vs Post-Norm

#### 6. è®­ç»ƒæµç¨‹è¯¦è§£
ğŸ“– é˜…è¯»:
- [tutorials/train.py](tutorials/train.py)
- [src/llm_foundry/training/trainer.py](src/llm_foundry/training/trainer.py)
- [docs/architecture-training.md](docs/architecture-training.md)

**è®­ç»ƒå…¨æµç¨‹**:
1. æ•°æ®å‡†å¤‡ (Data Preparation)
2. é¢„è®­ç»ƒ (Pre-training)
3. ç›‘ç£å¾®è°ƒ (SFT)
4. å¥–åŠ±å»ºæ¨¡ (Reward Modeling)
5. å¼ºåŒ–å­¦ä¹  (RLHF)
6. è¯„ä¼°ä¸éƒ¨ç½² (Evaluation & Deployment)

**å…³é”®æŠ€æœ¯**:
- AdamW ä¼˜åŒ–å™¨
- Cosine å­¦ä¹ ç‡è°ƒåº¦ + Warmup
- æ¢¯åº¦è£å‰ª
- æ··åˆç²¾åº¦è®­ç»ƒ

#### 7. æ¨ç†å’Œç”Ÿæˆ
ğŸ“– é˜…è¯»:
- [tutorials/generate.py](tutorials/generate.py)
- [src/llm_foundry/inference/generator.py](src/llm_foundry/inference/generator.py)

**ç”ŸæˆæŠ€æœ¯**:
- è‡ªå›å½’ç”Ÿæˆæµç¨‹
- Temperature æ§åˆ¶éšæœºæ€§
- Top-k å’Œ Top-p é‡‡æ ·
- KV Cache ä¼˜åŒ–

---

### ç¬¬ä¸‰é˜¶æ®µ: å®è·µåº”ç”¨ (4-6 å°æ—¶)

#### 8. è‡ªå®šä¹‰æ•°æ®é›†
ğŸ“– é˜…è¯»: [tutorials/dataloader.py](tutorials/dataloader.py)
ğŸ¯ å®è·µ: [examples/02_custom_data.py](examples/02_custom_data.py)

**ä»»åŠ¡**:
- å‡†å¤‡è‡ªå·±çš„æ–‡æœ¬æ•°æ®
- è®­ç»ƒè‡ªå®šä¹‰è¯è¡¨
- åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹

#### 9. è°ƒæ•´æ¨¡å‹é…ç½®
ğŸ¯ å°è¯•ä¸åŒçš„æ¨¡å‹é…ç½®

```python
# å°å‹æ¨¡å‹ (å¿«é€Ÿå®éªŒ)
small_cfg = ModelConfig(dim=256, n_layers=4, n_heads=8, n_kv_heads=4)

# ä¸­å‹æ¨¡å‹ (æ›´å¥½æ•ˆæœ)
medium_cfg = ModelConfig(dim=512, n_layers=8, n_heads=8, n_kv_heads=4)

# RTX 5060 ä¼˜åŒ–é…ç½®
rtx5060_cfg = ModelConfig(dim=704, n_layers=10, n_heads=10, n_kv_heads=5)
```

#### 10. è¶…å‚æ•°è°ƒä¼˜
**å®éªŒ**:
- è°ƒæ•´å­¦ä¹ ç‡ (1e-4 åˆ° 1e-3)
- è°ƒæ•´ batch size
- è°ƒæ•´ warmup æ­¥æ•°
- ä½¿ç”¨ learning rate scheduler

#### 11. é«˜çº§ç”ŸæˆæŠ€å·§
ğŸ¯ å®è·µ: [examples/03_generation_sampling.py](examples/03_generation_sampling.py)

**æ¢ç´¢**:
- Temperature å¯¹å¤šæ ·æ€§çš„å½±å“ (0.1-1.0)
- Top-k å’Œ Top-p çš„å¹³è¡¡
- ç»„åˆä½¿ç”¨å¤šç§ç­–ç•¥

---

### ç¬¬å››é˜¶æ®µ: ç”Ÿäº§å®è·µ (å¯é€‰, 6-8 å°æ—¶)

#### 12. ä½¿ç”¨åŒ…æ¨¡å¼å¼€å‘
```python
from llm_foundry import (
    ModelConfig, TrainConfig,
    MiniLLM, Tokenizer, DataLoader
)
from llm_foundry.training import Trainer
from llm_foundry.inference import Generator

# æ„å»ºå®Œæ•´åº”ç”¨
cfg = ModelConfig()
model = MiniLLM(cfg)
trainer = Trainer(model, train_cfg)
trainer.train()
```

#### 13. å‘½ä»¤è¡Œå·¥å…·
```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒ
python scripts/train.py --config configs/medium.yaml

# ç”Ÿæˆæ–‡æœ¬
python scripts/generate.py \
    --checkpoint model.pt \
    --prompt "Once upon a time" \
    --temperature 0.8
```

#### 14. ç”Ÿäº§éƒ¨ç½²
ğŸ“– é˜…è¯»: [docs/architecture-training.md](docs/architecture-training.md)

**ç”Ÿäº§çº§æŠ€æœ¯**:
- åˆ†å¸ƒå¼è®­ç»ƒ (DDP/FSDP)
- æ··åˆç²¾åº¦è®­ç»ƒ (FP16/BF16)
- æ¨¡å‹æœåŠ¡ (FastAPI)
- æ¨ç†ä¼˜åŒ– (é‡åŒ–ã€Flash Attention)

---

## æ·±å…¥ç†è§£

### æ ¸å¿ƒæŠ€æœ¯

#### RoPE (Rotary Position Embedding)
é€šè¿‡æ—‹è½¬å˜æ¢æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œç›¸å¯¹ä½ç½®åœ¨ç‚¹ç§¯ä¸­è‡ªç„¶ä½“ç°ã€‚

**ä¼˜åŠ¿**:
- é•¿åºåˆ—å¤–æ¨èƒ½åŠ›å¼º
- ä¸å¢åŠ å‚æ•°
- è®¡ç®—é«˜æ•ˆ

#### GQA (Grouped Query Attention)
åœ¨ MHA å’Œ MQA ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå‡å°‘ KV Cache å¤§å°ã€‚

**é…ç½®ç¤ºä¾‹**:
```python
# MHA (Multi-Head Attention)
n_heads = 8, n_kv_heads = 8

# GQA (Grouped Query Attention)
n_heads = 8, n_kv_heads = 4  # æ¯ 2 ä¸ª Q å…±äº« 1 ä¸ª KV

# MQA (Multi-Query Attention)
n_heads = 8, n_kv_heads = 1  # æ‰€æœ‰ Q å…±äº« 1 ä¸ª KV
```

#### SwiGLU
é«˜æ€§èƒ½æ¿€æ´»å‡½æ•°ï¼Œä½¿ç”¨é—¨æ§æœºåˆ¶ã€‚

**å…¬å¼**: `Swish(xWâ‚) âŠ™ xWâ‚ƒ Wâ‚‚`

#### RMSNorm
å‡æ–¹æ ¹å½’ä¸€åŒ–ï¼Œæ¯” LayerNorm æ›´é«˜æ•ˆã€‚

**å…¬å¼**: `y = x / RMS(x) * Î³`

---

## å®è·µåº”ç”¨

### è‡ªå®šä¹‰é…ç½®

#### æ–¹æ³• 1: ç¼–è¾‘é…ç½®æ–‡ä»¶
ç¼–è¾‘ `tutorials/config.py`:
```python
@dataclass
class ModelConfig:
    dim: int = 512
    n_layers: int = 8
    n_heads: int = 8
    n_kv_heads: int = 4
    vocab_size: int = 8192
    max_seq_len: int = 512

@dataclass
class TrainConfig:
    batch_size: int = 16
    learning_rate: float = 3e-4
    max_iters: int = 5000
    eval_interval: int = 100
```

#### æ–¹æ³• 2: ä½¿ç”¨é¢„è®¾é…ç½®
```python
from tutorials.config import (
    get_small_config,      # ~2M å‚æ•°
    get_medium_config,     # ~10M å‚æ•°
    get_rtx5060_config    # ~70M å‚æ•°
)
```

### ç¡¬ä»¶é€‰æ‹©å‚è€ƒ

| ç¡¬ä»¶ | æ¨¡å‹è§„æ¨¡ | è®­ç»ƒæ—¶é—´* | æŒ‡å— |
|------|---------|----------|------|
| CPU | 2M | 10-30min | ä½¿ç”¨ small é…ç½® |
| RTX 5060 (8GB) | 70M | 30-40min | [RTX 5060 æŒ‡å—](docs/hardware-rtx5060.md) |
| Apple M4 Pro | 68M | 40-60min | è‡ªå®šä¹‰é…ç½® |
| RTX 4090 (24GB) | 200M+ | 10-20min | è‡ªå®šä¹‰é…ç½® |

*åŸºäº 10k training steps

---

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: è®­ç»ƒå¤ªæ…¢

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥ GPU: `torch.cuda.is_available()`
2. å‡å°æ¨¡å‹: é™ä½ `dim` æˆ– `n_layers`
3. å‡å°æ‰¹æ¬¡: é™ä½ `batch_size`
4. ä½¿ç”¨æ··åˆç²¾åº¦: å‚è€ƒ [è®­ç»ƒç³»ç»Ÿ](docs/architecture-training.md)

### é—®é¢˜ 2: CUDA Out of Memory

**è§£å†³æ–¹æ¡ˆ**:
```python
# å‡å° batch_size
train_cfg.batch_size = 16

# å‡å° max_seq_len
model_cfg.max_seq_len = 128

# å‡å°æ¨¡å‹å¤§å°
model_cfg.dim = 256
model_cfg.n_layers = 4
```

### é—®é¢˜ 3: ç”Ÿæˆè´¨é‡ä¸å¥½

**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ è®­ç»ƒæ­¥æ•°: `train_cfg.max_iters = 5000`
2. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹: `model_cfg = get_medium_config()`
3. è°ƒæ•´é‡‡æ ·å‚æ•°:
   ```python
   temperature = 0.8   # é™ä½éšæœºæ€§
   top_k = 50          # é™åˆ¶å€™é€‰è¯
   top_p = 0.9         # æ ¸é‡‡æ ·
   ```

### é—®é¢˜ 4: æ‰¾ä¸åˆ°æ¨¡å—

**è§£å†³æ–¹æ¡ˆ**:
```bash
cd llm-foundry
pip install -e .
python -c "import llm_foundry; print('OK')"
```

---

## å­¦ä¹ æ£€æŸ¥æ¸…å•

### åŸºç¡€æ¦‚å¿µ
- [ ] ç†è§£ Transformer çš„åŸºæœ¬æ¶æ„
- [ ] èƒ½è§£é‡Š Self-Attention çš„å·¥ä½œåŸç†
- [ ] ç†è§£å› æœè¯­è¨€å»ºæ¨¡çš„è®­ç»ƒç›®æ ‡
- [ ] çŸ¥é“å¦‚ä½•è®¡ç®—æ¨¡å‹å‚æ•°é‡

### æ ¸å¿ƒæŠ€æœ¯
- [ ] èƒ½è§£é‡Š RoPE å¦‚ä½•ç¼–ç ä½ç½®ä¿¡æ¯
- [ ] ç†è§£ GQA çš„å‚æ•°å…±äº«æœºåˆ¶
- [ ] çŸ¥é“ SwiGLU çš„é—¨æ§æœºåˆ¶
- [ ] ç†è§£ Pre-normalization çš„ä¼˜åŠ¿

### å®è·µèƒ½åŠ›
- [ ] èƒ½ç‹¬ç«‹è®­ç»ƒä¸€ä¸ªå°å‹æ¨¡å‹
- [ ] èƒ½è°ƒæ•´æ¨¡å‹é…ç½®å’Œè®­ç»ƒå‚æ•°
- [ ] èƒ½ä½¿ç”¨ä¸åŒé‡‡æ ·ç­–ç•¥ç”Ÿæˆæ–‡æœ¬
- [ ] èƒ½åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹

### é«˜çº§æŠ€èƒ½
- [ ] èƒ½é˜…è¯»å’Œç†è§£æ ¸å¿ƒä»£ç 
- [ ] èƒ½ç¼–å†™æµ‹è¯•ç”¨ä¾‹
- [ ] èƒ½ä½¿ç”¨æ¨¡å—åŒ– API å¼€å‘åº”ç”¨
- [ ] ç†è§£ç”Ÿäº§éƒ¨ç½²çš„è€ƒè™‘å› ç´ 

---

## å¤–éƒ¨èµ„æº

### ç»å…¸è®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [RoFormer](https://arxiv.org/abs/2104.09864) - RoPE è®ºæ–‡
- [LLaMA](https://arxiv.org/abs/2302.13971) - ç°ä»£ LLM æ¶æ„å‚è€ƒ
- [GPT-3](https://arxiv.org/abs/2005.14165) - å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹

### å­¦ä¹ èµ„æº
- [Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT) - æç®€ GPT å®ç°
- [Stanford CS224N](http://web.stanford.edu/class/cs224n/) - NLP è¯¾ç¨‹
- [Hugging Face Course](https://huggingface.co/course) - å…è´¹ NLP è¯¾ç¨‹

---

## è·å–å¸®åŠ©

- ğŸ“š **æŸ¥çœ‹æ–‡æ¡£**: [docs/README.md](docs/README.md)
- ğŸ› **æäº¤ Issue**: [GitHub Issues](https://github.com/your-org/llm-foundry/issues)
- ğŸ’¬ **è®¨è®º**: [GitHub Discussions](https://github.com/your-org/llm-foundry/discussions)
- ğŸ¤ **è´¡çŒ®ä»£ç **: [è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)

---

**ç¥æ‚¨å­¦ä¹ æ„‰å¿«ï¼** ğŸš€
