# LLM Foundry ç”¨æˆ·æŒ‡å—

> **ä»å¿«é€Ÿä¸Šæ‰‹åˆ°æ·±å…¥æŒæ¡çš„å®Œæ•´æŒ‡å—**

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨ä»é›¶å¼€å§‹ä½¿ç”¨ LLM Foundryï¼Œä»å¿«é€Ÿä½“éªŒç¬¬ä¸€ä¸ªæ¨¡å‹åˆ°æ·±å…¥ç†è§£ Transformer æ¶æ„ã€‚

---

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿä¸Šæ‰‹](#å¿«é€Ÿä¸Šæ‰‹-5-10-åˆ†é’Ÿ)
- [ç³»ç»Ÿå­¦ä¹ ](#ç³»ç»Ÿå­¦ä¹ -10-15-å°æ—¶)
- [æ·±å…¥ç†è§£](#æ·±å…¥ç†è§£)
- [å®è·µåº”ç”¨](#å®è·µåº”ç”¨)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## å¿«é€Ÿä¸Šæ‰‹ (5-10 åˆ†é’Ÿ)

### å‰ç½®è¦æ±‚

- **Python** 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **RAM** è‡³å°‘ 4GB
- **GPU** (å¯é€‰) NVIDIA GPU ç”¨äºåŠ é€Ÿè®­ç»ƒ

### ç¡¬ä»¶æ£€æµ‹

åœ¨å®‰è£…å‰ï¼Œå…ˆæ£€æµ‹ä½ çš„ç¯å¢ƒï¼š

```bash
# æ£€æµ‹ PyTorch å’ŒåŠ é€Ÿå¼•æ“
python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA:', torch.cuda.is_available()); mps = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False; print('MPS:', mps); device = 'cuda' if torch.cuda.is_available() else 'mps' if mps else 'cpu'; print('Device:', device)"
```

**é¢„æœŸè¾“å‡ºï¼ˆNVIDIA GPU ç¯å¢ƒ - Windows/Linuxï¼‰**:
```
PyTorch: 2.10.0+cu118
CUDA: True
MPS: False
Device: cuda
```

**é¢„æœŸè¾“å‡ºï¼ˆApple Silicon ç¯å¢ƒ - macOSï¼‰**:
```
PyTorch: 2.10.0
CUDA: False
MPS: True
Device: mps
```

**é¢„æœŸè¾“å‡ºï¼ˆCPU ç¯å¢ƒï¼‰**:
```
PyTorch: 2.10.0+cpu
CUDA: False
MPS: False
Device: cpu
```

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-org/llm-foundry.git
cd llm-foundry

# åŸºç¡€å®‰è£…ï¼ˆCPUï¼‰
pip install -e .

# æˆ–å®‰è£…å¼€å‘ä¾èµ–
pip install -e .[dev]

# éªŒè¯å®‰è£…
python -c "import llm_foundry; print('OK')"
```

### GPU ç”¨æˆ·ï¼šå®‰è£…åŠ é€Ÿç‰ˆæœ¬ PyTorch

å¦‚æœä½ æœ‰ NVIDIA GPU æˆ– Apple Siliconï¼Œéœ€è¦å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„ PyTorchï¼š

#### NVIDIA GPUï¼ˆWindows/Linuxï¼‰

```bash
# 1. å¸è½½ CPU ç‰ˆæœ¬
pip uninstall torch -y

# 2. å®‰è£… CUDA ç‰ˆæœ¬ï¼ˆé€‰æ‹©é€‚åˆä½ çš„ç‰ˆæœ¬ï¼‰
# CUDA 11.8ï¼ˆæ¨èï¼Œå…¼å®¹æ€§æœ€å¥½ï¼‰
pip install torch --index-url https://download.pytorch.org/whl/cu118

# æˆ– CUDA 12.1ï¼ˆè¾ƒæ–°ç‰ˆæœ¬ï¼‰
pip install torch --index-url https://download.pytorch.org/whl/cu121

# å¯¹äº RTX 5060 ç­‰éœ€è¦ sm_120 è®¡ç®—èƒ½åŠ›çš„ 50 ç³»åˆ—æ˜¾å¡
# éœ€è¦å®‰è£… PyTorch Nightly é¢„è§ˆç‰ˆï¼ˆæ”¯æŒ CUDA 12.8ï¼‰
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

**éªŒè¯å®‰è£…**:
```bash
python -c "import torch; print('CUDA:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0))"
```

**é¢„æœŸè¾“å‡º**:
```
CUDA: True
GPU: NVIDIA GeForce RTX 5060
```

#### Apple Siliconï¼ˆmacOSï¼‰

macOS ä½¿ç”¨ **MPS**ï¼ˆMetal Performance Shadersï¼‰åŠ é€Ÿï¼Œæ— éœ€é¢å¤–å®‰è£…ï¼š

```bash
# åŸºç¡€å®‰è£…å³å¯
pip install -e .

# éªŒè¯ MPS
python -c "import torch; print('MPS:', torch.backends.mps.is_available())"
```

**é¢„æœŸè¾“å‡º**:
```
MPS: True
```

### è®­ç»ƒç¬¬ä¸€ä¸ªæ¨¡å‹

```bash
cd tutorials
python train.py      # è®­ç»ƒæ¨¡å‹ (~30 ç§’ GPUï¼Œ~10 åˆ†é’Ÿ CPU)
python generate.py   # ç”Ÿæˆæ–‡æœ¬
```

**é¢„æœŸè¾“å‡ºï¼ˆGPUï¼‰**:
```
ä½¿ç”¨è®¾å¤‡: cuda
æ¨¡å‹å‚æ•°é‡: 2.08M
step 0: train loss 9.1234, val loss 9.2345
step 50: train loss 5.6789, val loss 5.7890
è®­ç»ƒå®Œæˆï¼Œè€—æ—¶ 32.45s
```

**é¢„æœŸè¾“å‡ºï¼ˆCPUï¼‰**:
```
ä½¿ç”¨è®¾å¤‡: cpu
æ¨¡å‹å‚æ•°é‡: 2.08M
step 0: train loss 9.1234, val loss 9.2345
step 50: train loss 5.6789, val loss 5.7890
è®­ç»ƒå®Œæˆï¼Œè€—æ—¶ 645.12s (çº¦ 10 åˆ†é’Ÿ)
```

### ä¸¤ç§ä½¿ç”¨æ¨¡å¼

**æ•™å­¦æ¨¡å¼ (tutorials/)** - é€‚åˆå­¦ä¹ ã€æ•™å­¦ã€å¿«é€Ÿå®éªŒ
```bash
cd tutorials
python train.py
python generate.py
```

**åŒ…æ¨¡å¼ (src/)** - é€‚åˆç ”ç©¶ã€ç”Ÿäº§ã€å®šåˆ¶åŒ–å¼€å‘
```python
from llm_foundry import ModelConfig, MiniLLM, DataLoader

cfg = ModelConfig(dim=512, n_layers=8)
model = MiniLLM(cfg)
# ...
```

---

## ç³»ç»Ÿå­¦ä¹  (10-15 å°æ—¶)

### ç¬¬ä¸€é˜¶æ®µ: ç†è§£æ ¸å¿ƒä»£ç  (2-3 å°æ—¶)

#### 1. é…ç½®ç³»ç»Ÿ
ğŸ“– é˜…è¯»: [tutorials/config.py](tutorials/config.py)
- `ModelConfig`: æ¨¡å‹æ¶æ„é…ç½®
- `TrainConfig`: è®­ç»ƒè¶…å‚æ•°é…ç½®

#### 2. æ¨¡å‹æ¶æ„ - æ ¸å¿ƒç»„ä»¶
ğŸ“– é˜…è¯»: [src/llm_foundry/models/components.py](src/llm_foundry/models/components.py)

**RMSNorm** (ç¬¬18-64è¡Œ)
- å½’ä¸€åŒ–æŠ€æœ¯ï¼Œæ¯” LayerNorm æ›´é«˜æ•ˆ
- åªéœ€è®¡ç®— RMSï¼Œä¸éœ€è¦å‡å€¼

**RoPE ä½ç½®ç¼–ç ** (ç¬¬66-148è¡Œ)
- é€šè¿‡æ—‹è½¬ç¼–ç ä½ç½®ä¿¡æ¯
- ç›¸å¯¹ä½ç½®åœ¨ç‚¹ç§¯ä¸­è‡ªç„¶ä½“ç°

**æ³¨æ„åŠ›æœºåˆ¶** (ç¬¬150-258è¡Œ)
- Self-Attention å’Œ GQA (åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›)
- GQA å‡å°‘ KV Cache å¤§å°ï¼Œé™ä½å‚æ•°é‡

**å‰é¦ˆç½‘ç»œ** (ç¬¬260-278è¡Œ)
- SwiGLU æ¿€æ´»å‡½æ•°
- é—¨æ§æœºåˆ¶æå‡æ€§èƒ½

**Transformer å—** (ç¬¬280-312è¡Œ)
- Pre-normalization æ¶æ„
- æ®‹å·®è¿æ¥

#### 3. å®Œæ•´æ¨¡å‹
ğŸ“– é˜…è¯»: [src/llm_foundry/models/transformer.py](src/llm_foundry/models/transformer.py)
- ç†è§£å¦‚ä½•ç»„è£…å„ä¸ªç»„ä»¶
- æŸ¥çœ‹ `forward` æ–¹æ³•çš„å®Œæ•´æµç¨‹

#### 4. æ•°æ®å¤„ç†å’Œåˆ†è¯å™¨
ğŸ“– é˜…è¯»:
- [tutorials/data.py](tutorials/data.py) - æ•°æ®åŠ è½½æµç¨‹
- [tutorials/tokenizer.py](tutorials/tokenizer.py) - BPE åˆ†è¯åŸç†

---

### ç¬¬äºŒé˜¶æ®µ: æ·±å…¥ç†è§£ (3-4 å°æ—¶)

#### 5. æ¶æ„æ·±åº¦è§£æ
ğŸ“– é˜…è¯»: [docs/architecture-components.md](docs/architecture-components.md)

**æ ¸å¿ƒç»„ä»¶è¯¦è§£**:
- Token Embedding
- RMSNorm vs LayerNorm
- RoPE å·¥ä½œåŸç†
- GQA vs MHA vs MQA
- SwiGLU é—¨æ§æœºåˆ¶
- Pre-Norm vs Post-Norm

#### 6. è®­ç»ƒæµç¨‹è¯¦è§£
ğŸ“– é˜…è¯»:
- [tutorials/train.py](tutorials/train.py)
- [src/llm_foundry/training/trainer.py](src/llm_foundry/training/trainer.py)
- [docs/architecture-training.md](docs/architecture-training.md)

**è®­ç»ƒå…¨æµç¨‹**:
1. æ•°æ®å‡†å¤‡ (Data Preparation)
2. é¢„è®­ç»ƒ (Pre-training)
3. ç›‘ç£å¾®è°ƒ (SFT)
4. å¥–åŠ±å»ºæ¨¡ (Reward Modeling)
5. å¼ºåŒ–å­¦ä¹  (RLHF)
6. è¯„ä¼°ä¸éƒ¨ç½² (Evaluation & Deployment)

**å…³é”®æŠ€æœ¯**:
- AdamW ä¼˜åŒ–å™¨
- Cosine å­¦ä¹ ç‡è°ƒåº¦ + Warmup
- æ¢¯åº¦è£å‰ª
- æ··åˆç²¾åº¦è®­ç»ƒ

#### 7. æ¨ç†å’Œç”Ÿæˆ
ğŸ“– é˜…è¯»:
- [tutorials/generate.py](tutorials/generate.py)
- [src/llm_foundry/inference/generator.py](src/llm_foundry/inference/generator.py)

**ç”ŸæˆæŠ€æœ¯**:
- è‡ªå›å½’ç”Ÿæˆæµç¨‹
- Temperature æ§åˆ¶éšæœºæ€§
- Top-k å’Œ Top-p é‡‡æ ·
- KV Cache ä¼˜åŒ–

---

### ç¬¬ä¸‰é˜¶æ®µ: å®è·µåº”ç”¨ (4-6 å°æ—¶)

#### 8. è‡ªå®šä¹‰æ•°æ®é›†
ğŸ“– é˜…è¯»: [tutorials/dataloader.py](tutorials/dataloader.py)
ğŸ¯ å®è·µ: [examples/02_custom_data.py](examples/02_custom_data.py)

**ä»»åŠ¡**:
- å‡†å¤‡è‡ªå·±çš„æ–‡æœ¬æ•°æ®
- è®­ç»ƒè‡ªå®šä¹‰è¯è¡¨
- åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹

#### 9. è°ƒæ•´æ¨¡å‹é…ç½®
ğŸ¯ å°è¯•ä¸åŒçš„æ¨¡å‹é…ç½®

```python
# å°å‹æ¨¡å‹ (å¿«é€Ÿå®éªŒ)
small_cfg = ModelConfig(dim=256, n_layers=4, n_heads=8, n_kv_heads=4)

# ä¸­å‹æ¨¡å‹ (æ›´å¥½æ•ˆæœ)
medium_cfg = ModelConfig(dim=512, n_layers=8, n_heads=8, n_kv_heads=4)

# RTX 5060 ä¼˜åŒ–é…ç½®
rtx5060_cfg = ModelConfig(dim=704, n_layers=10, n_heads=10, n_kv_heads=5)
```

#### 10. è¶…å‚æ•°è°ƒä¼˜
**å®éªŒ**:
- è°ƒæ•´å­¦ä¹ ç‡ (1e-4 åˆ° 1e-3)
- è°ƒæ•´ batch size
- è°ƒæ•´ warmup æ­¥æ•°
- ä½¿ç”¨ learning rate scheduler

#### 11. é«˜çº§ç”ŸæˆæŠ€å·§
ğŸ¯ å®è·µ: [examples/03_generation_sampling.py](examples/03_generation_sampling.py)

**æ¢ç´¢**:
- Temperature å¯¹å¤šæ ·æ€§çš„å½±å“ (0.1-1.0)
- Top-k å’Œ Top-p çš„å¹³è¡¡
- ç»„åˆä½¿ç”¨å¤šç§ç­–ç•¥

---

### ç¬¬å››é˜¶æ®µ: ç”Ÿäº§å®è·µ (å¯é€‰, 6-8 å°æ—¶)

#### 12. ä½¿ç”¨åŒ…æ¨¡å¼å¼€å‘
```python
from llm_foundry import (
    ModelConfig, TrainConfig,
    MiniLLM, Tokenizer, DataLoader
)
from llm_foundry.training import Trainer
from llm_foundry.inference import Generator

# æ„å»ºå®Œæ•´åº”ç”¨
cfg = ModelConfig()
model = MiniLLM(cfg)
trainer = Trainer(model, train_cfg)
trainer.train()
```

#### 13. å‘½ä»¤è¡Œå·¥å…·
```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒ
python scripts/train.py --config configs/medium.yaml

# ç”Ÿæˆæ–‡æœ¬
python scripts/generate.py \
    --checkpoint model.pt \
    --prompt "Once upon a time" \
    --temperature 0.8
```

#### 14. ç”Ÿäº§éƒ¨ç½²
ğŸ“– é˜…è¯»: [docs/architecture-training.md](docs/architecture-training.md)

**ç”Ÿäº§çº§æŠ€æœ¯**:
- åˆ†å¸ƒå¼è®­ç»ƒ (DDP/FSDP)
- æ··åˆç²¾åº¦è®­ç»ƒ (FP16/BF16)
- æ¨¡å‹æœåŠ¡ (FastAPI)
- æ¨ç†ä¼˜åŒ– (é‡åŒ–ã€Flash Attention)

---

## æ·±å…¥ç†è§£

### æ ¸å¿ƒæŠ€æœ¯

#### RoPE (Rotary Position Embedding)
é€šè¿‡æ—‹è½¬å˜æ¢æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œç›¸å¯¹ä½ç½®åœ¨ç‚¹ç§¯ä¸­è‡ªç„¶ä½“ç°ã€‚

**ä¼˜åŠ¿**:
- é•¿åºåˆ—å¤–æ¨èƒ½åŠ›å¼º
- ä¸å¢åŠ å‚æ•°
- è®¡ç®—é«˜æ•ˆ

#### GQA (Grouped Query Attention)
åœ¨ MHA å’Œ MQA ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå‡å°‘ KV Cache å¤§å°ã€‚

**é…ç½®ç¤ºä¾‹**:
```python
# MHA (Multi-Head Attention)
n_heads = 8, n_kv_heads = 8

# GQA (Grouped Query Attention)
n_heads = 8, n_kv_heads = 4  # æ¯ 2 ä¸ª Q å…±äº« 1 ä¸ª KV

# MQA (Multi-Query Attention)
n_heads = 8, n_kv_heads = 1  # æ‰€æœ‰ Q å…±äº« 1 ä¸ª KV
```

#### SwiGLU
é«˜æ€§èƒ½æ¿€æ´»å‡½æ•°ï¼Œä½¿ç”¨é—¨æ§æœºåˆ¶ã€‚

**å…¬å¼**: `Swish(xWâ‚) âŠ™ xWâ‚ƒ Wâ‚‚`

#### RMSNorm
å‡æ–¹æ ¹å½’ä¸€åŒ–ï¼Œæ¯” LayerNorm æ›´é«˜æ•ˆã€‚

**å…¬å¼**: `y = x / RMS(x) * Î³`

---

## å®è·µåº”ç”¨

### è‡ªå®šä¹‰é…ç½®

#### æ–¹æ³• 1: ç¼–è¾‘é…ç½®æ–‡ä»¶
ç¼–è¾‘ `tutorials/config.py`:
```python
@dataclass
class ModelConfig:
    dim: int = 512
    n_layers: int = 8
    n_heads: int = 8
    n_kv_heads: int = 4
    vocab_size: int = 8192
    max_seq_len: int = 512

@dataclass
class TrainConfig:
    batch_size: int = 16
    learning_rate: float = 3e-4
    max_iters: int = 5000
    eval_interval: int = 100
```

#### æ–¹æ³• 2: ä½¿ç”¨é¢„è®¾é…ç½®
```python
from tutorials.config import (
    get_small_config,      # ~2M å‚æ•°
    get_medium_config,     # ~10M å‚æ•°
    get_rtx5060_config    # ~70M å‚æ•°
)
```

### ç¡¬ä»¶é€‰æ‹©å‚è€ƒ

| ç¡¬ä»¶ | æ¨¡å‹è§„æ¨¡ | è®­ç»ƒæ—¶é—´* | æŒ‡å— |
|------|---------|----------|------|
| CPU | 2M | 10-30min | ä½¿ç”¨ small é…ç½® |
| RTX 5060 (8GB) | 70M | 30-40min | [RTX 5060 æŒ‡å—](docs/hardware-rtx5060.md) |
| Apple M4 Pro | 68M | 40-60min | è‡ªå®šä¹‰é…ç½® |
| RTX 4090 (24GB) | 200M+ | 10-20min | è‡ªå®šä¹‰é…ç½® |

*åŸºäº 10k training steps

---

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: è®­ç»ƒå¤ªæ…¢ï¼ˆCPU æ¨¡å¼ï¼‰

**ç—‡çŠ¶**: è¾“å‡ºæ˜¾ç¤º `ä½¿ç”¨è®¾å¤‡: cpu`ï¼Œè®­ç»ƒéå¸¸æ…¢

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨ï¼š
   ```bash
   python -c "import torch; print('CUDA:', torch.cuda.is_available())"
   ```
2. å¦‚æœè¾“å‡º `CUDA: False`ï¼Œè¯´æ˜ä½¿ç”¨çš„æ˜¯ CPU ç‰ˆæœ¬ PyTorch
3. æŒ‰ç…§ [GPU ç”¨æˆ·å®‰è£…æŒ‡å—](#gpu-ç”¨æˆ·å®‰è£…-cuda-ç‰ˆæœ¬-pytorch) é‡æ–°å®‰è£… CUDA ç‰ˆæœ¬

### é—®é¢˜ 2: CUDA ä¸å¯ç”¨

**ç—‡çŠ¶**: `torch.cuda.is_available()` è¿”å› `False`

**è§£å†³æ–¹æ¡ˆ**:
1. ç¡®è®¤æœ‰ NVIDIA GPUï¼š`nvidia-smi`
2. å¸è½½ CPU ç‰ˆæœ¬ PyTorchï¼š
   ```bash
   pip uninstall torch -y
   ```
3. å®‰è£… CUDA ç‰ˆæœ¬ï¼š
   ```bash
   # CUDA 11.8ï¼ˆæ¨èï¼‰
   pip install torch --index-url https://download.pytorch.org/whl/cu118
   ```
4. æˆ–ä½¿ç”¨ condaï¼š
   ```bash
   conda install pytorch cuda -c pytorch
   ```

### é—®é¢˜ 3: CUDA å†…æ ¸ä¸å¯ç”¨ï¼ˆno kernel image errorï¼‰

**ç—‡çŠ¶**: è¿è¡Œæ—¶å‡ºç° `RuntimeError: CUDA error: no kernel image is available for execution on the device`
- é€šå¸¸å‘ç”Ÿåœ¨ RTX 5060 ç­‰ NVIDIA 50 ç³»åˆ—æ˜¾å¡ä¸Š
- PyTorch ç¨³å®šç‰ˆä¸æ”¯æŒ sm_120 è®¡ç®—èƒ½åŠ›

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥ GPU è®¡ç®—èƒ½åŠ›ï¼š
   ```bash
   python -c "import torch; print('Compute capability:', torch.cuda.get_device_capability(0) if torch.cuda.is_available() else 'N/A')"
   ```
2. å¦‚æœè®¡ç®—èƒ½åŠ›æ˜¯ `(12, 0)` (sm_120)ï¼Œéœ€è¦å®‰è£… PyTorch Nightly é¢„è§ˆç‰ˆï¼š
   ```bash
   pip uninstall torch torchvision torchaudio -y
   pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
   ```
3. éªŒè¯å®‰è£…ï¼š
   ```bash
   python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA:', torch.version.cuda if torch.cuda.is_available() else 'N/A')"
   ```

### é—®é¢˜ 4: CUDA Out of Memory

**ç—‡çŠ¶**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```python
# å‡å° batch_size
train_cfg.batch_size = 16

# å‡å° max_seq_len
model_cfg.max_seq_len = 128

# å‡å°æ¨¡å‹å¤§å°
model_cfg.dim = 256
model_cfg.n_layers = 4
```

### é—®é¢˜ 5: ç”Ÿæˆè´¨é‡ä¸å¥½

**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ è®­ç»ƒæ­¥æ•°: `train_cfg.max_iters = 5000`
2. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹: `model_cfg = get_medium_config()`
3. è°ƒæ•´é‡‡æ ·å‚æ•°:
   ```python
   temperature = 0.8   # é™ä½éšæœºæ€§
   top_k = 50          # é™åˆ¶å€™é€‰è¯
   top_p = 0.9         # æ ¸é‡‡æ ·
   ```

### é—®é¢˜ 6: æ‰¾ä¸åˆ°æ¨¡å—

**è§£å†³æ–¹æ¡ˆ**:
```bash
cd llm-foundry
pip install -e .
python -c "import llm_foundry; print('OK')"
```

---

## å­¦ä¹ æ£€æŸ¥æ¸…å•

### åŸºç¡€æ¦‚å¿µ
- [ ] ç†è§£ Transformer çš„åŸºæœ¬æ¶æ„
- [ ] èƒ½è§£é‡Š Self-Attention çš„å·¥ä½œåŸç†
- [ ] ç†è§£å› æœè¯­è¨€å»ºæ¨¡çš„è®­ç»ƒç›®æ ‡
- [ ] çŸ¥é“å¦‚ä½•è®¡ç®—æ¨¡å‹å‚æ•°é‡

### æ ¸å¿ƒæŠ€æœ¯
- [ ] èƒ½è§£é‡Š RoPE å¦‚ä½•ç¼–ç ä½ç½®ä¿¡æ¯
- [ ] ç†è§£ GQA çš„å‚æ•°å…±äº«æœºåˆ¶
- [ ] çŸ¥é“ SwiGLU çš„é—¨æ§æœºåˆ¶
- [ ] ç†è§£ Pre-normalization çš„ä¼˜åŠ¿

### å®è·µèƒ½åŠ›
- [ ] èƒ½ç‹¬ç«‹è®­ç»ƒä¸€ä¸ªå°å‹æ¨¡å‹
- [ ] èƒ½è°ƒæ•´æ¨¡å‹é…ç½®å’Œè®­ç»ƒå‚æ•°
- [ ] èƒ½ä½¿ç”¨ä¸åŒé‡‡æ ·ç­–ç•¥ç”Ÿæˆæ–‡æœ¬
- [ ] èƒ½åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹

### é«˜çº§æŠ€èƒ½
- [ ] èƒ½é˜…è¯»å’Œç†è§£æ ¸å¿ƒä»£ç 
- [ ] èƒ½ç¼–å†™æµ‹è¯•ç”¨ä¾‹
- [ ] èƒ½ä½¿ç”¨æ¨¡å—åŒ– API å¼€å‘åº”ç”¨
- [ ] ç†è§£ç”Ÿäº§éƒ¨ç½²çš„è€ƒè™‘å› ç´ 

---

## å¤–éƒ¨èµ„æº

### ç»å…¸è®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [RoFormer](https://arxiv.org/abs/2104.09864) - RoPE è®ºæ–‡
- [LLaMA](https://arxiv.org/abs/2302.13971) - ç°ä»£ LLM æ¶æ„å‚è€ƒ
- [GPT-3](https://arxiv.org/abs/2005.14165) - å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹

### å­¦ä¹ èµ„æº
- [Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT) - æç®€ GPT å®ç°
- [Stanford CS224N](http://web.stanford.edu/class/cs224n/) - NLP è¯¾ç¨‹
- [Hugging Face Course](https://huggingface.co/course) - å…è´¹ NLP è¯¾ç¨‹

---

## è·å–å¸®åŠ©

- ğŸ“š **æŸ¥çœ‹æ–‡æ¡£**: [docs/README.md](docs/README.md)
- ğŸ› **æäº¤ Issue**: [GitHub Issues](https://github.com/your-org/llm-foundry/issues)
- ğŸ’¬ **è®¨è®º**: [GitHub Discussions](https://github.com/your-org/llm-foundry/discussions)
- ğŸ¤ **è´¡çŒ®ä»£ç **: [è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)

---

**ç¥æ‚¨å­¦ä¹ æ„‰å¿«ï¼** ğŸš€
