# è´¡çŒ®æŒ‡å—

> **æ¬¢è¿è´¡çŒ®ä»£ç ã€æ–‡æ¡£å’Œæƒ³æ³•ï¼**

æ„Ÿè°¢æ‚¨å¯¹ LLM Foundry é¡¹ç›®çš„å…³æ³¨ã€‚æœ¬æ–‡æ¡£å°†å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•å‚ä¸è´¡çŒ®ã€‚

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [å¼€å‘å·¥ä½œæµ](#å¼€å‘å·¥ä½œæµ)
3. [ä»£ç è§„èŒƒ](#ä»£ç è§„èŒƒ)
4. [æµ‹è¯•æŒ‡å—](#æµ‹è¯•æŒ‡å—)
5. [æ·»åŠ æ–°åŠŸèƒ½](#æ·»åŠ æ–°åŠŸèƒ½)
6. [æ–‡æ¡£æ ‡å‡†](#æ–‡æ¡£æ ‡å‡†)
7. [æäº¤ Pull Request](#æäº¤-pull-request)

---

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè®¾ç½®

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/your-org/llm-foundry.git
cd llm-foundry

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. å®‰è£…ä¾èµ–
pip install -e .  # å¼€å‘æ¨¡å¼å®‰è£…
pip install -r requirements-dev.txt  # å¼€å‘å·¥å…·

# 4. éªŒè¯å®‰è£…
python -c "import llm_foundry; print(llm_foundry.__version__)"
```

### åˆ†æ”¯å‘½åçº¦å®š

- `main`: ä¸»åˆ†æ”¯ï¼Œç¨³å®šç‰ˆæœ¬
- `feature/<name>`: æ–°åŠŸèƒ½åˆ†æ”¯ (å¦‚ `feature/flash-attention`)
- `fix/<name>`: Bug ä¿®å¤åˆ†æ”¯ (å¦‚ `fix/rope-overflow`)
- `docs/<name>`: æ–‡æ¡£æ›´æ–°åˆ†æ”¯ (å¦‚ `docs/quickstart`)
- `refactor/<name>`: é‡æ„åˆ†æ”¯ (å¦‚ `refactor/data-loader`)

---

## å¼€å‘å·¥ä½œæµ

### æäº¤ä¿¡æ¯æŒ‡å—

æ ¼å¼: `<type>(<scope>): <subject>`

**ç±»å‹**:
- `feat`: æ–°åŠŸèƒ½
- `fix`: Bug ä¿®å¤
- `docs`: æ–‡æ¡£æ›´æ–°
- `style`: ä»£ç æ ¼å¼è°ƒæ•´
- `refactor`: é‡æ„
- `test`: æµ‹è¯•ç›¸å…³
- `chore`: æ„å»º/å·¥å…·é…ç½®

**ç¤ºä¾‹**:
```
feat(models): add Flash Attention support
fix(data): handle empty tokenizer files
docs(zh): update quickstart guide
refactor(training): extract loss computation
```

### Pull Request æµç¨‹

1. **åˆ›å»ºåˆ†æ”¯**: ä» `main` åˆ›å»ºç‰¹æ€§åˆ†æ”¯
2. **å¼€å‘**: å®ç°åŠŸèƒ½ï¼Œæ·»åŠ æµ‹è¯•ï¼Œæ›´æ–°æ–‡æ¡£
3. **æäº¤**: ä½¿ç”¨æ¸…æ™°çš„æäº¤ä¿¡æ¯
4. **æµ‹è¯•**: è¿è¡Œ `pytest tests/` ç¡®ä¿æµ‹è¯•é€šè¿‡
5. **PR**: åˆ›å»º Pull Requestï¼Œæè¿°å˜æ›´å†…å®¹
6. **å®¡æŸ¥**: ç­‰å¾…ä»£ç å®¡æŸ¥ï¼Œæ ¹æ®åé¦ˆä¿®æ”¹
7. **åˆå¹¶**: å®¡æŸ¥é€šè¿‡ååˆå¹¶åˆ° `main`

### ä»£ç å®¡æŸ¥æ¸…å•

- [ ] ä»£ç éµå¾ª PEP 8 é£æ ¼
- [ ] æœ‰æ¸…æ™°çš„æ–‡æ¡£å­—ç¬¦ä¸²
- [ ] æ·»åŠ äº†å¿…è¦çš„æµ‹è¯•
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] æ›´æ–°äº†ç›¸å…³æ–‡æ¡£
- [ ] æ²¡æœ‰å¼•å…¥æ€§èƒ½é—®é¢˜
- [ ] æ²¡æœ‰ç ´åç°æœ‰åŠŸèƒ½

---

## ä»£ç è§„èŒƒ

### å¯¼å…¥çº¦å®š

**æ¨èçš„å¯¼å…¥æ–¹å¼**:

```python
# 1. é¡¶å±‚å¯¼å…¥(ç®€å•ä½¿ç”¨)
from llm_foundry import ModelConfig, MiniLLM, Tokenizer

# 2. æ¨¡å—å¯¼å…¥(æ˜ç¡®æ¥æº)
from llm_foundry.models import MiniLLM
from llm_foundry.config import ModelConfig, TrainConfig

# 3. å†…éƒ¨å¯¼å…¥(åŒ…å†…ä½¿ç”¨)
from ..config import ModelConfig
from .components import RMSNorm
```

**é¿å…çš„å¯¼å…¥æ–¹å¼**:

```python
# âŒ é¿å… import *
from llm_foundry import *

# âŒ é¿å…æ·±å±‚å¯¼å…¥(ç ´åå°è£…)
from llm_foundry.models.components import RMSNorm  # åº”è¯¥ä» models å¯¼å…¥
```

### å‘½åçº¦å®š

éµå¾ª PEP 8 æ ‡å‡†:

- **ç±»å**: `PascalCase` (å¦‚ `MiniLLM`, `RMSNorm`)
- **å‡½æ•°/æ–¹æ³•**: `snake_case` (å¦‚ `get_batch`, `apply_rotary_emb`)
- **å¸¸é‡**: `UPPER_SNAKE_CASE` (å¦‚ `MAX_SEQ_LEN`)
- **ç§æœ‰æ–¹æ³•**: `_leading_underscore` (å¦‚ `_init_weights`)
- **é…ç½®ç±»**: `Config` åç¼€ (å¦‚ `ModelConfig`, `TrainConfig`)

### æ–‡æ¡£å­—ç¬¦ä¸²æ ¼å¼

ä½¿ç”¨ Google é£æ ¼çš„æ–‡æ¡£å­—ç¬¦ä¸²:

```python
def function_name(arg1, arg2):
    """ç®€çŸ­æè¿°(ä¸€è¡Œ)

    è¯¦ç»†æè¿°(å¯é€‰,å¤šè¡Œ)ã€‚
    è§£é‡Šå‡½æ•°çš„è¡Œä¸ºã€ç®—æ³•æˆ–è®¾è®¡å†³ç­–ã€‚

    Args:
        arg1: å‚æ•° 1 çš„æè¿°
        arg2: å‚æ•° 2 çš„æè¿°

    Returns:
        è¿”å›å€¼çš„æè¿°

    Raises:
        ExceptionType: å¼‚å¸¸æƒ…å†µçš„æè¿°

    Example:
        >>> result = function_name(1, 2)
        >>> print(result)
        3
    """
    pass
```

### ä»£ç æ³¨é‡Šæœ€ä½³å®è·µ

**å¥½çš„æ³¨é‡Š**:
```python
# ä½¿ç”¨ RoPE è€Œä¸æ˜¯ç»å¯¹ä½ç½®ç¼–ç ,å› ä¸ºå®ƒå¯¹é•¿åºåˆ—å¤–æ¨æ•ˆæœæ›´å¥½
xq, xk = apply_rotary_emb(xq, xk, freqs_cis)

# å¯¹äº GQA,é‡å¤ KV heads ä»¥åŒ¹é… query heads çš„æ•°é‡
# ä¾‹å¦‚: 8 query heads, 4 KV heads -> æ¯ä¸ª KV head é‡å¤ 2 æ¬¡
if self.n_kv_heads != self.n_heads:
    xk = torch.repeat_interleave(xk, self.n_heads // self.n_kv_heads, dim=2)
```

**ä¸å¥½çš„æ³¨é‡Š**:
```python
# åº”ç”¨ RoPE
xq, xk = apply_rotary_emb(xq, xk, freqs_cis)  # âŒ é‡å¤ä»£ç å«ä¹‰

# i = i + 1
i = i + 1  # âŒ æ— æ„ä¹‰æ³¨é‡Š
```

---

## æµ‹è¯•æŒ‡å—

### æµ‹è¯•ç»“æ„

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_models.py        # æ¨¡å‹ç»„ä»¶æµ‹è¯•
â”œâ”€â”€ test_tokenizer.py     # åˆ†è¯å™¨æµ‹è¯•
â”œâ”€â”€ test_data.py          # æ•°æ®åŠ è½½æµ‹è¯•
â”œâ”€â”€ test_training.py      # è®­ç»ƒæµç¨‹æµ‹è¯•
â””â”€â”€ fixtures/             # æµ‹è¯•æ•°æ®
    â””â”€â”€ sample.txt
```

### å•å…ƒæµ‹è¯•è¦æ±‚

**æ¯ä¸ªæ–°åŠŸèƒ½éƒ½åº”è¯¥æœ‰æµ‹è¯•**:

```python
import pytest
import torch
from llm_foundry import ModelConfig
from llm_foundry.models import RMSNorm

def test_rmsnorm_shape():
    """æµ‹è¯• RMSNorm è¾“å‡ºå½¢çŠ¶"""
    cfg = ModelConfig()
    norm = RMSNorm(cfg.dim)
    x = torch.randn(2, 16, cfg.dim)
    output = norm(x)
    assert output.shape == x.shape

def test_rmsnorm_normalization():
    """æµ‹è¯• RMSNorm å½’ä¸€åŒ–æ•ˆæœ"""
    cfg = ModelConfig()
    norm = RMSNorm(cfg.dim)
    x = torch.randn(2, 16, cfg.dim)
    output = norm(x)
    # éªŒè¯å½’ä¸€åŒ–å±æ€§
    rms = torch.sqrt(torch.mean(output ** 2, dim=-1))
    # RMS åº”è¯¥æ¥è¿‘ 1
    assert torch.allclose(rms, torch.ones_like(rms), atol=0.1)
```

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/

# è¿è¡Œç‰¹å®šæ–‡ä»¶
pytest tests/test_models.py

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_models.py::test_rmsnorm_shape

# æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
pytest tests/ -v

# æ˜¾ç¤ºæ‰“å°è¾“å‡º
pytest tests/ -s

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest tests/ --cov=llm_foundry --cov-report=html
```

### æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡

- æ ¸å¿ƒæ¨¡å— (`models`, `data`): **> 80%**
- å·¥å…·æ¨¡å— (`utils`): **> 70%**
- æ•´ä½“é¡¹ç›®: **> 75%**

---

## æ·»åŠ æ–°åŠŸèƒ½

### æ·»åŠ æ–°æ¨¡å‹ç»„ä»¶

**åœºæ™¯**: æ·»åŠ æ–°çš„æ³¨æ„åŠ›æœºåˆ¶(å¦‚ Flash Attention)

**æ­¥éª¤**:

1. **åœ¨ `models/components.py` ä¸­å®šä¹‰**:

```python
class FlashAttention(nn.Module):
    """Flash Attention å®ç°

    å¿«é€Ÿä¸”å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°ã€‚
    """
    def __init__(self, cfg: ModelConfig):
        super().__init__()
        # å®ç°ç»†èŠ‚...

    def forward(self, x, freqs_cis):
        # å‰å‘ä¼ æ’­é€»è¾‘...
        pass
```

2. **åœ¨ `models/__init__.py` ä¸­å¯¼å‡º**:

```python
from .components import FlashAttention

__all__ = [
    # ... ç°æœ‰å¯¼å‡º
    'FlashAttention',
]
```

3. **åœ¨ `models/transformer.py` ä¸­ä½¿ç”¨**:

```python
# å¯é€‰åœ°åœ¨ Block ä¸­ä½¿ç”¨æ–°ç»„ä»¶
class Block(nn.Module):
    def __init__(self, cfg: ModelConfig, use_flash=False):
        super().__init__()
        if use_flash:
            self.attention = FlashAttention(cfg)
        else:
            self.attention = CausalSelfAttention(cfg)
```

4. **æ·»åŠ æµ‹è¯•** (`tests/test_models.py`):

```python
def test_flash_attention():
    cfg = ModelConfig()
    attn = FlashAttention(cfg)
    x = torch.randn(2, 16, cfg.dim)
    freqs_cis = precompute_freqs_cis(cfg.dim // cfg.n_heads, 16)
    output = attn(x, freqs_cis)
    assert output.shape == x.shape
```

5. **æ›´æ–°æ–‡æ¡£** (`docs/architecture-components.md`):

æ·»åŠ  Flash Attention çš„è¯´æ˜å’Œä½¿ç”¨ç¤ºä¾‹ã€‚

### æ·»åŠ æ–°è®­ç»ƒåŠŸèƒ½

**åœºæ™¯**: æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨

**æ­¥éª¤**:

1. **åœ¨ `training/` ä¸­åˆ›å»ºæ–°æ–‡ä»¶** (`schedulers.py`):

```python
def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    # å®ç°...
    pass
```

2. **åœ¨ `training/__init__.py` ä¸­å¯¼å‡º**
3. **åœ¨ `Trainer` ç±»ä¸­é›†æˆ**
4. **æ·»åŠ é…ç½®é€‰é¡¹** (`TrainConfig`)
5. **æ›´æ–°æ–‡æ¡£å’Œç¤ºä¾‹**

### æ·»åŠ æ–°æ•°æ®é›†

**åœºæ™¯**: æ·»åŠ è‹±æ–‡æ–‡æœ¬æ•°æ®é›†æ”¯æŒ

**æ­¥éª¤**:

1. **åœ¨ `data/datasets.py` ä¸­å®šä¹‰**:

```python
class TinyStoriesDataset:
    """TinyStories æ•°æ®é›†åŠ è½½å™¨"""
    def __init__(self, cache_dir='./data'):
        # å®ç°...
        pass
```

2. **åœ¨ `examples/datasets/` ä¸­æ·»åŠ ä¸‹è½½å™¨**:

```python
# download_english.py
def download_tinystories():
    """ä¸‹è½½ TinyStories æ•°æ®é›†"""
    # å®ç°...
```

3. **æ·»åŠ ä½¿ç”¨ç¤ºä¾‹** (`examples/02_custom_data.py`)
4. **æ›´æ–°æ–‡æ¡£** (`docs/guides/data.md`)

### æ·»åŠ æ–°é‡‡æ ·ç­–ç•¥

**åœºæ™¯**: æ·»åŠ  Top-p åŠ¨æ€è°ƒæ•´

**æ­¥éª¤**:

1. **åœ¨ `inference/generator.py` ä¸­å®ç°**
2. **æ·»åŠ å‚æ•°åˆ° `generate` å‡½æ•°**
3. **åœ¨ `examples/03_generation_sampling.py` ä¸­æ·»åŠ ç¤ºä¾‹**
4. **æ›´æ–°æ–‡æ¡£** (`docs/architecture-training.md`)

---

## æ–‡æ¡£æ ‡å‡†

### æ–‡æ¡£å­—ç¬¦ä¸²

**å¿…é¡»**: æ‰€æœ‰å…¬å…± API (ç±»ã€å‡½æ•°ã€æ–¹æ³•)

**å¯é€‰**: ç§æœ‰å‡½æ•°(å¦‚æœé€»è¾‘å¤æ‚)

**æ ¼å¼**: Google é£æ ¼(è§ä»£ç è§„èŒƒéƒ¨åˆ†)

### ä½•æ—¶æ›´æ–°æ–‡æ¡£

**å¿…é¡»æ›´æ–°**:
- æ·»åŠ æ–°çš„å…¬å…± API
- ä¿®æ”¹ç°æœ‰ API çš„è¡Œä¸º
- æ·»åŠ æ–°åŠŸèƒ½æˆ–é…ç½®é€‰é¡¹
- é‡å¤§æ¶æ„å˜æ›´

**æ–‡æ¡£æ–‡ä»¶æ˜ å°„**:
- æ¨¡å‹ç»„ä»¶å˜æ›´ â†’ `docs/architecture-components.md`
- è®­ç»ƒåŠŸèƒ½å˜æ›´ â†’ `docs/architecture-training.md`
- æ•°æ®å¤„ç†å˜æ›´ â†’ `docs/guides/data.md`
- æ¨ç†åŠŸèƒ½å˜æ›´ â†’ `docs/architecture-training.md`
- é…ç½®å˜æ›´ â†’ `docs/reference/config.md`
- API å˜æ›´ â†’ `docs/reference/api.md`

### README æ›´æ–°

å½“ä»¥ä¸‹æƒ…å†µå‘ç”Ÿæ—¶æ›´æ–° `README.md`:
- å®‰è£…æ–¹å¼å˜æ›´
- ä¸»è¦åŠŸèƒ½æ·»åŠ 
- å¿«é€Ÿå…¥é—¨æ­¥éª¤å˜åŒ–
- é¡¹ç›®ç›®æ ‡æˆ–å®šä½è°ƒæ•´

### ç¤ºä¾‹åˆ›å»ºæŒ‡å—

åœ¨ `examples/` ä¸­åˆ›å»ºæ–°ç¤ºä¾‹æ—¶:

1. **å‘½å**: ä½¿ç”¨æ•°å­—å‰ç¼€æ’åº (å¦‚ `05_new_feature.py`)
2. **ç»“æ„**:
   - ç®€çŸ­çš„æ–‡æ¡£å­—ç¬¦ä¸²è¯´æ˜ç›®çš„
   - æ¸…æ™°çš„æ­¥éª¤æ³¨é‡Š
   - å®Œæ•´çš„å¯è¿è¡Œä»£ç 
   - è¾“å‡ºç¤ºä¾‹(åœ¨æ³¨é‡Šä¸­)

3. **æ¨¡æ¿**:

```python
"""05_new_feature.py - æ–°åŠŸèƒ½ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ XXX åŠŸèƒ½æ¥å®ç° YYYã€‚

è¿è¡Œ:
    python examples/05_new_feature.py
"""

import torch
from llm_foundry import ModelConfig, MiniLLM

def main():
    # 1. è®¾ç½®é…ç½®
    cfg = ModelConfig()
    print(f"ä½¿ç”¨é…ç½®: {cfg}")

    # 2. åˆ›å»ºæ¨¡å‹
    model = MiniLLM(cfg)

    # 3. æ¼”ç¤ºåŠŸèƒ½
    # ...

    print("ç¤ºä¾‹å®Œæˆ!")

if __name__ == '__main__':
    main()
```

---

## æäº¤ Pull Request

### PR æ¨¡æ¿

```markdown
## æè¿°
ç®€è¦æè¿°æ­¤ PR çš„ç›®çš„å’Œå˜æ›´å†…å®¹ã€‚

## å˜æ›´ç±»å‹
- [ ] Bug ä¿®å¤
- [ ] æ–°åŠŸèƒ½
- [ ] ç ´åæ€§å˜æ›´
- [ ] æ–‡æ¡£æ›´æ–°

## ç›¸å…³ Issue
Closes #(issue number)

## å˜æ›´å†…å®¹
- åˆ—å‡ºä¸»è¦çš„å˜æ›´ç‚¹
- æ·»åŠ /ä¿®æ”¹äº†å“ªäº›æ–‡ä»¶
- æ–°å¢äº†å“ªäº›åŠŸèƒ½

## æµ‹è¯•
- [ ] æ·»åŠ äº†å•å…ƒæµ‹è¯•
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] æ‰‹åŠ¨æµ‹è¯•é€šè¿‡

## æ–‡æ¡£
- [ ] æ›´æ–°äº†ç›¸å…³æ–‡æ¡£
- [ ] æ·»åŠ äº†ä»£ç æ³¨é‡Š
- [ ] æ›´æ–°äº† README (å¦‚éœ€è¦)

## æ£€æŸ¥æ¸…å•
- [ ] ä»£ç éµå¾ªé¡¹ç›®è§„èŒƒ
- [ ] æäº¤ä¿¡æ¯æ¸…æ™°
- [ ] æ²¡æœ‰å¼•å…¥æ–°çš„è­¦å‘Š
- [ ] å‘åå…¼å®¹ (é™¤éæ˜¯ç ´åæ€§å˜æ›´)
```

### å®¡æŸ¥æµç¨‹

1. **è‡ªåŠ¨æ£€æŸ¥**: CI ä¼šè‡ªåŠ¨è¿è¡Œæµ‹è¯•å’Œä»£ç æ£€æŸ¥
2. **äººå·¥å®¡æŸ¥**: ç»´æŠ¤è€…ä¼šå®¡æŸ¥ä»£ç 
3. **åé¦ˆä¿®æ”¹**: æ ¹æ®å®¡æŸ¥æ„è§è¿›è¡Œä¿®æ”¹
4. **æœ€ç»ˆåˆå¹¶**: å®¡æŸ¥é€šè¿‡ååˆå¹¶

---

## è·å–å¸®åŠ©

- ğŸ“– **æŸ¥çœ‹æ–‡æ¡£**: [docs/README.md](docs/README.md)
- ğŸ› **æäº¤ Issue**: [GitHub Issues](https://github.com/your-org/llm-foundry/issues)
- ğŸ’¬ **è®¨è®º**: [GitHub Discussions](https://github.com/your-org/llm-foundry/discussions)
- ğŸ“§ **è”ç³»**: åœ¨ Discussions ä¸­æé—®

---

## é¡¹ç›®ç»“æ„

```
llm-foundry/
â”œâ”€â”€ src/llm_foundry/          # ä¸»åŒ… - ç”Ÿäº§çº§ä»£ç 
â”‚   â”œâ”€â”€ config/               # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ models/               # æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ tokenizers/           # åˆ†è¯å™¨
â”‚   â”œâ”€â”€ data/                 # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ training/             # è®­ç»ƒå·¥å…·
â”‚   â”œâ”€â”€ inference/            # æ¨ç†å·¥å…·
â”‚   â””â”€â”€ utils/                # å®ç”¨å·¥å…·
â”‚
â”œâ”€â”€ tutorials/                # æ•™å­¦å±•ç¤º - æ ¸å¿ƒåŠŸèƒ½çš„å®Œæ•´å±•ç¤º
â”‚   â”œâ”€â”€ train.py              # æ•™å­¦è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ generate.py           # æ•™å­¦ç”Ÿæˆè„šæœ¬
â”‚   â””â”€â”€ README.md             # ç®€å•æ¨¡å¼è¯´æ˜
â”‚
â”œâ”€â”€ scripts/                  # å‘½ä»¤è¡Œå·¥å…·
â”‚   â”œâ”€â”€ train.py              # è®­ç»ƒå…¥å£
â”‚   â”œâ”€â”€ generate.py           # ç”Ÿæˆå…¥å£
â”‚   â”œâ”€â”€ evaluate.py           # è¯„ä¼°å·¥å…·
â”‚   â””â”€â”€ prepare_data.py       # æ•°æ®å‡†å¤‡
â”‚
â”œâ”€â”€ examples/                 # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ 01_basic_training.py
â”‚   â”œâ”€â”€ 02_custom_data.py
â”‚   â”œâ”€â”€ 03_generation_sampling.py
â”‚   â”œâ”€â”€ 04_fine_tuning.py
â”‚   â””â”€â”€ datasets/             # æ•°æ®é›†ä¸‹è½½å™¨
â”‚
â”œâ”€â”€ tests/                    # å•å…ƒæµ‹è¯•
â”œâ”€â”€ configs/                  # é…ç½®æ–‡ä»¶
â”œâ”€â”€ docs/                     # æ–‡æ¡£
â”‚   â”œâ”€â”€ architecture/         # æ¶æ„è¯¦è§£
â”‚   â”œâ”€â”€ guides/               # å®ç”¨æŒ‡å—
â”‚   â”œâ”€â”€ hardware/             # ç¡¬ä»¶æŒ‡å—
â”‚   â””â”€â”€ reference/            # å‚è€ƒæ–‡æ¡£
â”‚
â”œâ”€â”€ README.md                 # é¡¹ç›®ç®€ä»‹
â”œâ”€â”€ LICENSE                   # MIT è®¸å¯è¯
â”œâ”€â”€ setup.py                  # åŒ…å®‰è£…
â”œâ”€â”€ requirements.txt          # ä¾èµ–
â”œâ”€â”€ requirements-dev.txt      # å¼€å‘ä¾èµ–
â””â”€â”€ CONTRIBUTING.md           # æœ¬æ–‡æ¡£
```

---

## å¸¸è§ä»»åŠ¡å¿«é€Ÿå‚è€ƒ

### æ·»åŠ æ–°æ¨¡å‹ç»„ä»¶

```bash
# 1. ç¼–è¾‘ç»„ä»¶æ–‡ä»¶
vim src/llm_foundry/models/components.py

# 2. æ›´æ–° __init__.py
vim src/llm_foundry/models/__init__.py

# 3. æ·»åŠ æµ‹è¯•
vim tests/test_models.py

# 4. è¿è¡Œæµ‹è¯•
pytest tests/test_models.py -v

# 5. æ›´æ–°æ–‡æ¡£
vim docs/architecture-components.md
```

### å®ç°æ–°æ•°æ®é›†

```bash
# 1. åˆ›å»ºæ•°æ®é›†ç±»
vim src/llm_foundry/data/datasets.py

# 2. åˆ›å»ºä¸‹è½½å™¨
vim examples/datasets/download_new.py

# 3. æ·»åŠ æµ‹è¯•
vim tests/test_data.py

# 4. åˆ›å»ºç¤ºä¾‹
vim examples/05_new_dataset.py

# 5. æ›´æ–°æ–‡æ¡£
vim docs/guides/data.md
```

### æ·»åŠ è®­ç»ƒåŠŸèƒ½

```bash
# 1. åœ¨è®­ç»ƒæ¨¡å—ä¸­å®ç°
vim src/llm_foundry/training/trainer.py

# 2. æ›´æ–°é…ç½®(å¦‚æœéœ€è¦)
vim src/llm_foundry/config/model_config.py

# 3. æ·»åŠ æµ‹è¯•
vim tests/test_training.py

# 4. æ›´æ–°æ–‡æ¡£
vim docs/architecture-training.md
```

### åˆ›å»ºæ–°ç¤ºä¾‹

```bash
# 1. åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
vim examples/06_new_example.py

# 2. æµ‹è¯•è¿è¡Œ
python examples/06_new_example.py

# 3. æ›´æ–° examples/README.md
vim examples/README.md
```

### ç¼–å†™æ–‡æ¡£

```bash
# 1. åˆ›å»º/ç¼–è¾‘æ–‡æ¡£
vim docs/guides/new_topic.md

# 2. æ›´æ–°æ–‡æ¡£ç´¢å¼•
vim docs/README.md

# 3. æ£€æŸ¥ Markdown æ ¼å¼
markdownlint new_topic.md

# 4. é¢„è§ˆ(å¯é€‰)
# ä½¿ç”¨ Markdown é¢„è§ˆå·¥å…·
```

### è¿è¡Œæµ‹è¯•

```bash
# å¿«é€Ÿæµ‹è¯•(æ ¸å¿ƒåŠŸèƒ½)
pytest tests/test_models.py tests/test_data.py

# å®Œæ•´æµ‹è¯•
pytest tests/ -v --cov=llm_foundry

# æµ‹è¯•ç‰¹å®šåŠŸèƒ½
pytest tests/ -k "test_rmsnorm"

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest tests/ --cov=llm_foundry --cov-report=html
open htmlcov/index.html
```

---

**æ„Ÿè°¢æ‚¨çš„è´¡çŒ®ï¼** ğŸ‰
