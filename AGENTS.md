# AGENTS.md - Agent åä½œæŒ‡å—

> **LLM Foundry**: å®ç”¨çš„å¼€æº LLM åŸºç¡€ â€”â€” ä»åŸºç¡€åˆ°ç”Ÿäº§

æœ¬æ–‡æ¡£ä¸º AI Agent æä¾›å…¨é¢çš„é¡¹ç›®å¯¼èˆªå’Œåä½œæŒ‡å—,å¸®åŠ©ç†è§£é¡¹ç›®ç»“æ„ã€ä»£ç ç»„ç»‡å’Œå¼€å‘å·¥ä½œæµã€‚

---

## 1. é¡¹ç›®æ¦‚è§ˆ

### 1.1 ä½¿å‘½ä¸æ„¿æ™¯

**ä½¿å‘½**: æä¾›ä¸€ä¸ªå®ç”¨çš„ã€å¼€æºçš„ LLM åŸºç¡€å®ç°,è¦†ç›–ä»åŸºç¡€æ¦‚å¿µåˆ°ç”Ÿäº§éƒ¨ç½²çš„å®Œæ•´æ—…ç¨‹ã€‚

**æ„¿æ™¯**:
- ğŸ“ **æ•™è‚²æ€§**: æ¸…æ™°çš„ä»£ç ç»“æ„å’Œè¯¦ç»†çš„æ³¨é‡Š,é€‚åˆå­¦ä¹ 
- ğŸ­ **ç”Ÿäº§æ€§**: æ¨¡å—åŒ–è®¾è®¡,æ˜“äºæ‰©å±•å’Œéƒ¨ç½²
- ğŸŒ‰ **æ¡¥æ¢æ€§**: è¿æ¥ç†è®ºå­¦ä¹ å’Œå®é™…åº”ç”¨

### 1.2 ç›®æ ‡å—ä¼—

- ML å·¥ç¨‹å¸ˆ:éœ€è¦ç†è§£å’Œå®šåˆ¶ LLM å®ç°
- ç ”ç©¶äººå‘˜:æ¢ç´¢ Transformer æ¶æ„å’Œè®­ç»ƒæŠ€æœ¯
- å­¦ç”Ÿ:å­¦ä¹ ç°ä»£ LLM çš„å·¥ä½œåŸç†

### 1.3 æ ¸å¿ƒåŸåˆ™

1. **ç®€æ´æ€§**: ä»£ç ç®€æ´æ˜“æ‡‚,é¿å…è¿‡åº¦å·¥ç¨‹åŒ–
2. **æ¨¡å—åŒ–**: æ¸…æ™°çš„æ¨¡å—è¾¹ç•Œ,ä¾¿äºæµ‹è¯•å’Œå¤ç”¨
3. **å¯æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°åŠŸèƒ½å’Œæ”¹è¿›
4. **æ–‡æ¡£åŒ–**: å®Œæ•´çš„æ–‡æ¡£å’Œæ³¨é‡Š

---

## 2. æ¶æ„æ¦‚è§ˆ

### 2.1 ç›®å½•ç»“æ„

```
llm-foundry/
â”œâ”€â”€ src/llm_foundry/          # ä¸»åŒ… - ç”Ÿäº§çº§ä»£ç 
â”‚   â”œâ”€â”€ config/               # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ models/               # æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ tokenizers/           # åˆ†è¯å™¨
â”‚   â”œâ”€â”€ data/                 # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ training/             # è®­ç»ƒå·¥å…·
â”‚   â”œâ”€â”€ inference/            # æ¨ç†å·¥å…·
â”‚   â””â”€â”€ utils/                # å®ç”¨å·¥å…·
â”‚
â”œâ”€â”€ tutorials/                # æ•™å­¦å±•ç¤º - æ ¸å¿ƒåŠŸèƒ½çš„å®Œæ•´å±•ç¤º
â”‚   â”œâ”€â”€ train.py              # æ•™å­¦è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ generate.py           # æ•™å­¦ç”Ÿæˆè„šæœ¬
â”‚   â””â”€â”€ README.md             # ç®€å•æ¨¡å¼è¯´æ˜
â”‚
â”œâ”€â”€ scripts/                  # å‘½ä»¤è¡Œå·¥å…·
â”‚   â”œâ”€â”€ train.py              # è®­ç»ƒå…¥å£
â”‚   â”œâ”€â”€ generate.py           # ç”Ÿæˆå…¥å£
â”‚   â”œâ”€â”€ evaluate.py           # è¯„ä¼°å·¥å…·
â”‚   â””â”€â”€ prepare_data.py       # æ•°æ®å‡†å¤‡
â”‚
â”œâ”€â”€ examples/                 # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ 01_basic_training.py
â”‚   â”œâ”€â”€ 02_custom_data.py
â”‚   â”œâ”€â”€ 03_generation_sampling.py
â”‚   â”œâ”€â”€ 04_fine_tuning.py
â”‚   â””â”€â”€ datasets/             # æ•°æ®é›†ä¸‹è½½å™¨
â”‚
â”œâ”€â”€ tests/                    # å•å…ƒæµ‹è¯•
â”œâ”€â”€ configs/                  # é…ç½®æ–‡ä»¶
â”œâ”€â”€ docs/                     # æ–‡æ¡£
â”‚   â”œâ”€â”€ zh/                   # ä¸­æ–‡æ–‡æ¡£
â”‚   â”‚   â”œâ”€â”€ quickstart.md
â”‚   â”‚   â”œâ”€â”€ architecture.md
â”‚   â”‚   â”œâ”€â”€ training.md
â”‚   â”‚   â”œâ”€â”€ inference.md
â”‚   â”‚   â””â”€â”€ production/       # ç”Ÿäº§éƒ¨ç½²æŒ‡å—
â”‚   â””â”€â”€ assets/               # å›¾ç‰‡èµ„æº
â”‚
â”œâ”€â”€ README.md                 # é¡¹ç›®ç®€ä»‹
â”œâ”€â”€ LICENSE                   # MIT è®¸å¯è¯
â”œâ”€â”€ setup.py                  # åŒ…å®‰è£…
â”œâ”€â”€ requirements.txt          # ä¾èµ–
â””â”€â”€ AGENTS.md                 # æœ¬æ–‡æ¡£
```

### 2.2 æ¨¡å—èŒè´£

| æ¨¡å— | èŒè´£ | å…³é”®æ–‡ä»¶ |
|------|------|---------|
| `config` | é…ç½®ç®¡ç† | `model_config.py` - æ¨¡å‹å’Œè®­ç»ƒé…ç½® |
| `models` | æ¨¡å‹å®ç° | `components.py` - åŸºç¡€ç»„ä»¶<br>`transformer.py` - å®Œæ•´æ¨¡å‹ |
| `tokenizers` | æ–‡æœ¬åˆ†è¯ | `sp_tokenizer.py` - SentencePiece BPE |
| `data` | æ•°æ®å¤„ç† | `loader.py` - æ•°æ®åŠ è½½å’Œæ‰¹æ¬¡ç”Ÿæˆ |
| `training` | è®­ç»ƒæµç¨‹ | `trainer.py` - è®­ç»ƒå™¨ç±» |
| `inference` | æ–‡æœ¬ç”Ÿæˆ | `generator.py` - ç”Ÿæˆå™¨ç±» |
| `utils` | å·¥å…·å‡½æ•° | `device.py` - è®¾å¤‡æ£€æµ‹<br>`checkpointing.py` - æ£€æŸ¥ç‚¹ç®¡ç† |

### 2.3 åŒæ¨¡å¼è®¾è®¡

**æ•™å­¦å±•ç¤º** (`tutorials/`):
- ä¸»å·¥ç¨‹æ ¸å¿ƒåŠŸèƒ½çš„å®Œæ•´å±•ç¤º
- å•æ–‡ä»¶è„šæœ¬,æ˜“äºç†è§£
- é€‚åˆå¿«é€Ÿå®éªŒå’Œæ•™å­¦
- åŠŸèƒ½ä¸å·¥ç¨‹ç‰ˆæœ¬å¯¹ç­‰

**å·¥ç¨‹å®ç°** (`src/llm_foundry/`):
- æ¨¡å—åŒ–æ¶æ„,ç”Ÿäº§å°±ç»ª
- æ”¯æŒ `pip install`
- ä¾¿äºæ‰©å±•å’Œç»´æŠ¤

---

## 3. ä»£ç ç»„ç»‡

### 3.1 åŒ…ç»“æ„

```python
# src/llm_foundry/ çš„å¯¼å…¥å±‚æ¬¡

llm_foundry/
â”œâ”€â”€ __init__.py           # é¡¶å±‚å¯¼å‡º
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py       # å¯¼å‡º: ModelConfig, TrainConfig
â”‚   â””â”€â”€ model_config.py   # é…ç½®ç±»å®šä¹‰
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py       # å¯¼å‡º: MiniLLM, RMSNorm, etc.
â”‚   â”œâ”€â”€ components.py     # åŸºç¡€ç»„ä»¶
â”‚   â””â”€â”€ transformer.py    # å®Œæ•´æ¨¡å‹
â””â”€â”€ ...
```

### 3.2 å¯¼å…¥çº¦å®š

**æ¨èçš„å¯¼å…¥æ–¹å¼**:

```python
# 1. é¡¶å±‚å¯¼å…¥(ç®€å•ä½¿ç”¨)
from llm_foundry import ModelConfig, MiniLLM, Tokenizer

# 2. æ¨¡å—å¯¼å…¥(æ˜ç¡®æ¥æº)
from llm_foundry.models import MiniLLM
from llm_foundry.config import ModelConfig, TrainConfig

# 3. å†…éƒ¨å¯¼å…¥(åŒ…å†…ä½¿ç”¨)
from ..config import ModelConfig
from .components import RMSNorm
```

**é¿å…çš„å¯¼å…¥æ–¹å¼**:

```python
# âŒ é¿å… import *
from llm_foundry import *

# âŒ é¿å…æ·±å±‚å¯¼å…¥(ç ´åå°è£…)
from llm_foundry.models.components import RMSNorm  # åº”è¯¥ä» models å¯¼å…¥
```

### 3.3 å‘½åçº¦å®š

éµå¾ª PEP 8 æ ‡å‡†:

- **ç±»å**: `PascalCase` (å¦‚ `MiniLLM`, `RMSNorm`)
- **å‡½æ•°/æ–¹æ³•**: `snake_case` (å¦‚ `get_batch`, `apply_rotary_emb`)
- **å¸¸é‡**: `UPPER_SNAKE_CASE` (å¦‚ `MAX_SEQ_LEN`)
- **ç§æœ‰æ–¹æ³•**: `_leading_underscore` (å¦‚ `_init_weights`)
- **é…ç½®ç±»**: `Config` åç¼€ (å¦‚ `ModelConfig`, `TrainConfig`)

### 3.4 æ–‡æ¡£å­—ç¬¦ä¸²æ ¼å¼

ä½¿ç”¨ Google é£æ ¼çš„æ–‡æ¡£å­—ç¬¦ä¸²:

```python
def function_name(arg1, arg2):
    """ç®€çŸ­æè¿°(ä¸€è¡Œ)

    è¯¦ç»†æè¿°(å¯é€‰,å¤šè¡Œ)ã€‚
    è§£é‡Šå‡½æ•°çš„è¡Œä¸ºã€ç®—æ³•æˆ–è®¾è®¡å†³ç­–ã€‚

    Args:
        arg1: å‚æ•° 1 çš„æè¿°
        arg2: å‚æ•° 2 çš„æè¿°

    Returns:
        è¿”å›å€¼çš„æè¿°

    Raises:
        ExceptionType: å¼‚å¸¸æƒ…å†µçš„æè¿°

    Example:
        >>> result = function_name(1, 2)
        >>> print(result)
        3
    """
    pass
```

---

## 4. å¼€å‘å·¥ä½œæµ

### 4.1 ç¯å¢ƒè®¾ç½®

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/your-org/llm-foundry.git
cd llm-foundry

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. å®‰è£…ä¾èµ–
pip install -e .  # å¼€å‘æ¨¡å¼å®‰è£…
pip install -r requirements-dev.txt  # å¼€å‘å·¥å…·

# 4. éªŒè¯å®‰è£…
python -c "import llm_foundry; print(llm_foundry.__version__)"
```

### 4.2 åˆ†æ”¯å‘½åçº¦å®š

- `main`: ä¸»åˆ†æ”¯,ç¨³å®šç‰ˆæœ¬
- `feature/<name>`: æ–°åŠŸèƒ½åˆ†æ”¯ (å¦‚ `feature/flash-attention`)
- `fix/<name>`: Bug ä¿®å¤åˆ†æ”¯ (å¦‚ `fix/rope-overflow`)
- `docs/<name>`: æ–‡æ¡£æ›´æ–°åˆ†æ”¯ (å¦‚ `docs/quickstart`)
- `refactor/<name>`: é‡æ„åˆ†æ”¯ (å¦‚ `refactor/data-loader`)

### 4.3 æäº¤ä¿¡æ¯æŒ‡å—

æ ¼å¼: `<type>(<scope>): <subject>`

**ç±»å‹**:
- `feat`: æ–°åŠŸèƒ½
- `fix`: Bug ä¿®å¤
- `docs`: æ–‡æ¡£æ›´æ–°
- `style`: ä»£ç æ ¼å¼è°ƒæ•´
- `refactor`: é‡æ„
- `test`: æµ‹è¯•ç›¸å…³
- `chore`: æ„å»º/å·¥å…·é…ç½®

**ç¤ºä¾‹**:
```
feat(models): add Flash Attention support
fix(data): handle empty tokenizer files
docs(zh): update quickstart guide
refactor(training): extract loss computation
```

### 4.4 Pull Request æµç¨‹

1. **åˆ›å»ºåˆ†æ”¯**: ä» `main` åˆ›å»ºç‰¹æ€§åˆ†æ”¯
2. **å¼€å‘**: å®ç°åŠŸèƒ½,æ·»åŠ æµ‹è¯•,æ›´æ–°æ–‡æ¡£
3. **æäº¤**: ä½¿ç”¨æ¸…æ™°çš„æäº¤ä¿¡æ¯
4. **æµ‹è¯•**: è¿è¡Œ `pytest tests/` ç¡®ä¿æµ‹è¯•é€šè¿‡
5. **PR**: åˆ›å»º Pull Request,æè¿°å˜æ›´å†…å®¹
6. **å®¡æŸ¥**: ç­‰å¾…ä»£ç å®¡æŸ¥,æ ¹æ®åé¦ˆä¿®æ”¹
7. **åˆå¹¶**: å®¡æŸ¥é€šè¿‡ååˆå¹¶åˆ° `main`

### 4.5 ä»£ç å®¡æŸ¥æ¸…å•

- [ ] ä»£ç éµå¾ª PEP 8 é£æ ¼
- [ ] æœ‰æ¸…æ™°çš„æ–‡æ¡£å­—ç¬¦ä¸²
- [ ] æ·»åŠ äº†å¿…è¦çš„æµ‹è¯•
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] æ›´æ–°äº†ç›¸å…³æ–‡æ¡£
- [ ] æ²¡æœ‰å¼•å…¥æ€§èƒ½é—®é¢˜
- [ ] æ²¡æœ‰ç ´åç°æœ‰åŠŸèƒ½

---

## 5. æ·»åŠ æ–°åŠŸèƒ½

### 5.1 æ·»åŠ æ–°æ¨¡å‹ç»„ä»¶

**åœºæ™¯**: æ·»åŠ æ–°çš„æ³¨æ„åŠ›æœºåˆ¶(å¦‚ Flash Attention)

**æ­¥éª¤**:

1. **åœ¨ `models/components.py` ä¸­å®šä¹‰**:

```python
class FlashAttention(nn.Module):
    """Flash Attention å®ç°

    å¿«é€Ÿä¸”å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°ã€‚
    """
    def __init__(self, cfg: ModelConfig):
        super().__init__()
        # å®ç°ç»†èŠ‚...

    def forward(self, x, freqs_cis):
        # å‰å‘ä¼ æ’­é€»è¾‘...
        pass
```

2. **åœ¨ `models/__init__.py` ä¸­å¯¼å‡º**:

```python
from .components import FlashAttention

__all__ = [
    # ... ç°æœ‰å¯¼å‡º
    'FlashAttention',
]
```

3. **åœ¨ `models/transformer.py` ä¸­ä½¿ç”¨**:

```python
# å¯é€‰åœ°åœ¨ Block ä¸­ä½¿ç”¨æ–°ç»„ä»¶
class Block(nn.Module):
    def __init__(self, cfg: ModelConfig, use_flash=False):
        super().__init__()
        if use_flash:
            self.attention = FlashAttention(cfg)
        else:
            self.attention = CausalSelfAttention(cfg)
```

4. **æ·»åŠ æµ‹è¯•** (`tests/test_models.py`):

```python
def test_flash_attention():
    cfg = ModelConfig()
    attn = FlashAttention(cfg)
    x = torch.randn(2, 16, cfg.dim)
    freqs_cis = precompute_freqs_cis(cfg.dim // cfg.n_heads, 16)
    output = attn(x, freqs_cis)
    assert output.shape == x.shape
```

5. **æ›´æ–°æ–‡æ¡£** (`docs/zh/architecture.md`):

æ·»åŠ  Flash Attention çš„è¯´æ˜å’Œä½¿ç”¨ç¤ºä¾‹ã€‚

### 5.2 æ·»åŠ æ–°è®­ç»ƒåŠŸèƒ½

**åœºæ™¯**: æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨

**æ­¥éª¤**:

1. **åœ¨ `training/` ä¸­åˆ›å»ºæ–°æ–‡ä»¶** (`schedulers.py`):

```python
def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    # å®ç°...
    pass
```

2. **åœ¨ `training/__init__.py` ä¸­å¯¼å‡º**
3. **åœ¨ `Trainer` ç±»ä¸­é›†æˆ**
4. **æ·»åŠ é…ç½®é€‰é¡¹** (`TrainConfig`)
5. **æ›´æ–°æ–‡æ¡£å’Œç¤ºä¾‹**

### 5.3 æ·»åŠ æ–°æ•°æ®é›†

**åœºæ™¯**: æ·»åŠ è‹±æ–‡æ–‡æœ¬æ•°æ®é›†æ”¯æŒ

**æ­¥éª¤**:

1. **åœ¨ `data/datasets.py` ä¸­å®šä¹‰**:

```python
class TinyStoriesDataset:
    """TinyStories æ•°æ®é›†åŠ è½½å™¨"""
    def __init__(self, cache_dir='./data'):
        # å®ç°...
        pass
```

2. **åœ¨ `examples/datasets/` ä¸­æ·»åŠ ä¸‹è½½å™¨**:

```python
# download_english.py
def download_tinystories():
    """ä¸‹è½½ TinyStories æ•°æ®é›†"""
    # å®ç°...
```

3. **æ·»åŠ ä½¿ç”¨ç¤ºä¾‹** (`examples/02_custom_data.py`)
4. **æ›´æ–°æ–‡æ¡£** (`docs/zh/data-preparation.md`)

### 5.4 æ·»åŠ æ–°é‡‡æ ·ç­–ç•¥

**åœºæ™¯**: æ·»åŠ  Top-p åŠ¨æ€è°ƒæ•´

**æ­¥éª¤**:

1. **åœ¨ `inference/generator.py` ä¸­å®ç°**
2. **æ·»åŠ å‚æ•°åˆ° `generate` å‡½æ•°**
3. **åœ¨ `examples/03_generation_sampling.py` ä¸­æ·»åŠ ç¤ºä¾‹**
4. **æ›´æ–°æ–‡æ¡£** (`docs/zh/inference.md`)

---

## 6. æµ‹è¯•æŒ‡å—

### 6.1 æµ‹è¯•ç»“æ„

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_models.py        # æ¨¡å‹ç»„ä»¶æµ‹è¯•
â”œâ”€â”€ test_tokenizer.py     # åˆ†è¯å™¨æµ‹è¯•
â”œâ”€â”€ test_data.py          # æ•°æ®åŠ è½½æµ‹è¯•
â”œâ”€â”€ test_training.py      # è®­ç»ƒæµç¨‹æµ‹è¯•
â””â”€â”€ fixtures/             # æµ‹è¯•æ•°æ®
    â””â”€â”€ sample.txt
```

### 6.2 å•å…ƒæµ‹è¯•è¦æ±‚

**æ¯ä¸ªæ–°åŠŸèƒ½éƒ½åº”è¯¥æœ‰æµ‹è¯•**:

```python
import pytest
import torch
from llm_foundry import ModelConfig
from llm_foundry.models import RMSNorm

def test_rmsnorm_shape():
    """æµ‹è¯• RMSNorm è¾“å‡ºå½¢çŠ¶"""
    cfg = ModelConfig()
    norm = RMSNorm(cfg.dim)
    x = torch.randn(2, 16, cfg.dim)
    output = norm(x)
    assert output.shape == x.shape

def test_rmsnorm_normalization():
    """æµ‹è¯• RMSNorm å½’ä¸€åŒ–æ•ˆæœ"""
    cfg = ModelConfig()
    norm = RMSNorm(cfg.dim)
    x = torch.randn(2, 16, cfg.dim)
    output = norm(x)
    # éªŒè¯å½’ä¸€åŒ–å±æ€§
    rms = torch.sqrt(torch.mean(output ** 2, dim=-1))
    # RMS åº”è¯¥æ¥è¿‘ 1
    assert torch.allclose(rms, torch.ones_like(rms), atol=0.1)
```

### 6.3 è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/

# è¿è¡Œç‰¹å®šæ–‡ä»¶
pytest tests/test_models.py

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_models.py::test_rmsnorm_shape

# æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
pytest tests/ -v

# æ˜¾ç¤ºæ‰“å°è¾“å‡º
pytest tests/ -s

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest tests/ --cov=llm_foundry --cov-report=html
```

### 6.4 æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡

- æ ¸å¿ƒæ¨¡å— (`models`, `data`): **> 80%**
- å·¥å…·æ¨¡å— (`utils`): **> 70%**
- æ•´ä½“é¡¹ç›®: **> 75%**

---

## 7. æ–‡æ¡£æ ‡å‡†

### 7.1 æ–‡æ¡£å­—ç¬¦ä¸²

**å¿…é¡»**: æ‰€æœ‰å…¬å…± API (ç±»ã€å‡½æ•°ã€æ–¹æ³•)

**å¯é€‰**: ç§æœ‰å‡½æ•°(å¦‚æœé€»è¾‘å¤æ‚)

**æ ¼å¼**: Google é£æ ¼(è§ 3.4 èŠ‚)

### 7.2 ä½•æ—¶æ›´æ–° docs/zh/

**å¿…é¡»æ›´æ–°**:
- æ·»åŠ æ–°çš„å…¬å…± API
- ä¿®æ”¹ç°æœ‰ API çš„è¡Œä¸º
- æ·»åŠ æ–°åŠŸèƒ½æˆ–é…ç½®é€‰é¡¹
- é‡å¤§æ¶æ„å˜æ›´

**æ–‡æ¡£æ–‡ä»¶æ˜ å°„**:
- æ¨¡å‹ç»„ä»¶å˜æ›´ â†’ `architecture.md`
- è®­ç»ƒåŠŸèƒ½å˜æ›´ â†’ `training.md`
- æ•°æ®å¤„ç†å˜æ›´ â†’ `data-preparation.md`
- æ¨ç†åŠŸèƒ½å˜æ›´ â†’ `inference.md`
- é…ç½®å˜æ›´ â†’ `configuration.md`
- API å˜æ›´ â†’ `api-reference.md`

### 7.3 README æ›´æ–°

å½“ä»¥ä¸‹æƒ…å†µå‘ç”Ÿæ—¶æ›´æ–° `README.md`:
- å®‰è£…æ–¹å¼å˜æ›´
- ä¸»è¦åŠŸèƒ½æ·»åŠ 
- å¿«é€Ÿå…¥é—¨æ­¥éª¤å˜åŒ–
- é¡¹ç›®ç›®æ ‡æˆ–å®šä½è°ƒæ•´

### 7.4 ä»£ç æ³¨é‡Šæœ€ä½³å®è·µ

**å¥½çš„æ³¨é‡Š**:
```python
# ä½¿ç”¨ RoPE è€Œä¸æ˜¯ç»å¯¹ä½ç½®ç¼–ç ,å› ä¸ºå®ƒå¯¹é•¿åºåˆ—å¤–æ¨æ•ˆæœæ›´å¥½
xq, xk = apply_rotary_emb(xq, xk, freqs_cis)

# å¯¹äº GQA,é‡å¤ KV heads ä»¥åŒ¹é… query heads çš„æ•°é‡
# ä¾‹å¦‚: 8 query heads, 4 KV heads -> æ¯ä¸ª KV head é‡å¤ 2 æ¬¡
if self.n_kv_heads != self.n_heads:
    xk = torch.repeat_interleave(xk, self.n_heads // self.n_kv_heads, dim=2)
```

**ä¸å¥½çš„æ³¨é‡Š**:
```python
# åº”ç”¨ RoPE
xq, xk = apply_rotary_emb(xq, xk, freqs_cis)  # âŒ é‡å¤ä»£ç å«ä¹‰

# i = i + 1
i = i + 1  # âŒ æ— æ„ä¹‰æ³¨é‡Š
```

### 7.5 ç¤ºä¾‹åˆ›å»ºæŒ‡å—

åœ¨ `examples/` ä¸­åˆ›å»ºæ–°ç¤ºä¾‹æ—¶:

1. **å‘½å**: ä½¿ç”¨æ•°å­—å‰ç¼€æ’åº (å¦‚ `05_new_feature.py`)
2. **ç»“æ„**:
   - ç®€çŸ­çš„æ–‡æ¡£å­—ç¬¦ä¸²è¯´æ˜ç›®çš„
   - æ¸…æ™°çš„æ­¥éª¤æ³¨é‡Š
   - å®Œæ•´çš„å¯è¿è¡Œä»£ç 
   - è¾“å‡ºç¤ºä¾‹(åœ¨æ³¨é‡Šä¸­)

3. **æ¨¡æ¿**:

```python
"""05_new_feature.py - æ–°åŠŸèƒ½ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ XXX åŠŸèƒ½æ¥å®ç° YYYã€‚

è¿è¡Œ:
    python examples/05_new_feature.py
"""

import torch
from llm_foundry import ModelConfig, MiniLLM

def main():
    # 1. è®¾ç½®é…ç½®
    cfg = ModelConfig()
    print(f"ä½¿ç”¨é…ç½®: {cfg}")

    # 2. åˆ›å»ºæ¨¡å‹
    model = MiniLLM(cfg)

    # 3. æ¼”ç¤ºåŠŸèƒ½
    # ...

    print("ç¤ºä¾‹å®Œæˆ!")

if __name__ == '__main__':
    main()
```

---

## 8. Agent åä½œæ¨¡å¼

### 8.1 å¦‚ä½•é«˜æ•ˆå¯¼èˆªä»£ç åº“

**æŸ¥æ‰¾åŠŸèƒ½çš„ä½ç½®**:

| éœ€æ±‚ | ä½ç½® | æ–¹æ³• |
|------|------|------|
| æ¨¡å‹æ¶æ„ç»†èŠ‚ | `src/llm_foundry/models/` | æŸ¥çœ‹ `components.py` å’Œ `transformer.py` |
| è®­ç»ƒé€»è¾‘ | `src/llm_foundry/training/` | æŸ¥çœ‹ `trainer.py` |
| æ•°æ®å¤„ç†æµç¨‹ | `src/llm_foundry/data/` | æŸ¥çœ‹ `loader.py` |
| é…ç½®é€‰é¡¹ | `src/llm_foundry/config/` | æŸ¥çœ‹ `model_config.py` |
| ç”Ÿæˆé€»è¾‘ | `src/llm_foundry/inference/` | æŸ¥çœ‹ `generator.py` |
| ä½¿ç”¨ç¤ºä¾‹ | `examples/` | æµè§ˆç¼–å·æ–‡ä»¶ |
| æ•™å­¦è„šæœ¬ | `tutorials/` | æŸ¥çœ‹ `train.py` å’Œ `generate.py` |

**ä½¿ç”¨ Grep æœç´¢**:

```bash
# æŸ¥æ‰¾å‡½æ•°å®šä¹‰
grep -r "def generate" src/

# æŸ¥æ‰¾ç±»å®šä¹‰
grep -r "class MiniLLM" src/

# æŸ¥æ‰¾é…ç½®ä½¿ç”¨
grep -r "ModelConfig" src/

# æŸ¥æ‰¾ TODO æ³¨é‡Š
grep -r "TODO" src/
```

### 8.2 å¸¸è§ä¿®æ”¹æ¨¡å¼

**æ¨¡å¼ 1: ä¿®æ”¹è¶…å‚æ•°**
1. æ›´æ–° `config/model_config.py` ä¸­çš„é»˜è®¤å€¼
2. æˆ–åœ¨ `configs/*.yaml` ä¸­æ·»åŠ æ–°é…ç½®æ–‡ä»¶
3. æ›´æ–°æ–‡æ¡£ `docs/zh/configuration.md`

**æ¨¡å¼ 2: ä¼˜åŒ–ç°æœ‰ç»„ä»¶**
1. ä¿®æ”¹ `models/components.py` ä¸­çš„å®ç°
2. ä¿æŒæ¥å£ä¸å˜(å‘åå…¼å®¹)
3. æ·»åŠ /æ›´æ–°æµ‹è¯•
4. æ›´æ–°æ–‡æ¡£(å¦‚æœè¡Œä¸ºæ”¹å˜)

**æ¨¡å¼ 3: æ·»åŠ æ–°åŠŸèƒ½**
1. åœ¨é€‚å½“æ¨¡å—ä¸­å®ç°
2. æ›´æ–°æ¨¡å—çš„ `__init__.py`
3. æ·»åŠ æµ‹è¯•
4. åˆ›å»ºä½¿ç”¨ç¤ºä¾‹
5. æ›´æ–°æ–‡æ¡£

### 8.3 æ¨¡å—é—´ä¾èµ–å…³ç³»

```
ä¾èµ–æ–¹å‘: é«˜å±‚ â†’ ä½å±‚

åº”ç”¨å±‚:      scripts/, tutorials/, examples/
                â†“
é«˜å±‚ API:     training/, inference/
                â†“
æ ¸å¿ƒæ¨¡å—:     models/, data/, tokenizers/
                â†“
åŸºç¡€æ¨¡å—:     config/, utils/
```

**åŸåˆ™**:
- ä½å±‚æ¨¡å—ä¸åº”ä¾èµ–é«˜å±‚æ¨¡å—
- é¿å…å¾ªç¯ä¾èµ–
- ä½¿ç”¨æ¥å£(é…ç½®ç±»)è§£è€¦

### 8.4 å®‰å…¨é‡æ„å®è·µ

**é‡æ„æ£€æŸ¥æ¸…å•**:

1. **ç†è§£ç°æœ‰ä»£ç **:
   - é˜…è¯»ç›¸å…³æ¨¡å—çš„ä»£ç 
   - ç†è§£åŠŸèƒ½å’Œè¾¹ç•Œæ¡ä»¶
   - æŸ¥çœ‹ç°æœ‰æµ‹è¯•

2. **åˆ¶å®šè®¡åˆ’**:
   - æ˜ç¡®é‡æ„ç›®æ ‡
   - è¯†åˆ«å—å½±å“çš„æ¨¡å—
   - è®¡åˆ’å‘åå…¼å®¹ç­–ç•¥

3. **å¢é‡é‡æ„**:
   - å°æ­¥éª¤æäº¤
   - æ¯æ­¥åè¿è¡Œæµ‹è¯•
   - ä¿æŒåŠŸèƒ½ä¸å˜

4. **æ›´æ–°ç›¸å…³å†…å®¹**:
   - æ›´æ–°å¯¼å…¥è¯­å¥
   - æ›´æ–°æ–‡æ¡£
   - æ›´æ–°ç¤ºä¾‹

5. **éªŒè¯**:
   - è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
   - è¿è¡Œç¤ºä¾‹è„šæœ¬
   - æ£€æŸ¥æ–‡æ¡£å‡†ç¡®æ€§

---

## 9. ç”Ÿäº§ç¯å¢ƒè€ƒè™‘

### 9.1 æ€§èƒ½ä¼˜åŒ–æŒ‡å—

**å…³é”®ä¼˜åŒ–ç‚¹**:

1. **æ¨¡å‹æ•ˆç‡**:
   - ä½¿ç”¨ `F.scaled_dot_product_attention`(PyTorch 2.0+)
   - å¯ç”¨ `torch.compile`(å¯é€‰)
   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

2. **æ•°æ®åŠ è½½**:
   - ä½¿ç”¨ `DataLoader` çš„ `num_workers`
   - é¢„åŠ è½½æ•°æ®åˆ°å†…å­˜(å¦‚æœå¯èƒ½)
   - ä½¿ç”¨ `pin_memory=True` (CUDA)

3. **å†…å­˜ç®¡ç†**:
   - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å¤„ç†å¤§æ‰¹æ¬¡
   - åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¼ é‡
   - ä½¿ç”¨ `torch.cuda.empty_cache()` æ¸…ç†ç¼“å­˜

**æ€§èƒ½åˆ†æ**:

```python
# ä½¿ç”¨ PyTorch Profiler
from torch.profiler import profile, ProfilerActivity

with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
    # è¿è¡Œè®­ç»ƒ/æ¨ç†ä»£ç 
    pass

print(prof.key_averages().table(sort_by="cuda_time_total"))
```

### 9.2 å†…å­˜æ•ˆç‡æ¨¡å¼

**æ¢¯åº¦ç´¯ç§¯**:

```python
# æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = batch_size * accumulation_steps
accumulation_steps = 4

for i, (x, y) in enumerate(dataloader):
    logits, loss = model(x, y)
    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**æ¢¯åº¦æ£€æŸ¥ç‚¹**:

```python
from torch.utils.checkpoint import checkpoint

# åœ¨ Block çš„ forward ä¸­
def forward(self, x, freqs_cis):
    # ä½¿ç”¨æ£€æŸ¥ç‚¹èŠ‚çœå†…å­˜(ç‰ºç‰²ä¸€äº›é€Ÿåº¦)
    h = x + checkpoint(self.attention, self.attention_norm(x), freqs_cis)
    out = h + checkpoint(self.feed_forward, self.ffn_norm(h))
    return out
```

### 9.3 åˆ†å¸ƒå¼è®­ç»ƒ

**DDP ç¤ºä¾‹**:

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def train_ddp(rank, world_size):
    setup(rank, world_size)
    model = MiniLLM(cfg).to(rank)
    model = DDP(model, device_ids=[rank])
    # è®­ç»ƒä»£ç ...
```

**ä½¿ç”¨æŒ‡å—**: æŸ¥çœ‹ `docs/zh/production/distributed-training.md`

### 9.4 æ¨¡å‹æœåŠ¡

**FastAPI æœåŠ¡ç¤ºä¾‹**:

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()
model = None  # å…¨å±€æ¨¡å‹

@app.on_event("startup")
async def load_model():
    global model
    model = MiniLLM(cfg)
    model.load_state_dict(torch.load("minillm.pt"))
    model.eval()

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100

@app.post("/generate")
async def generate(request: GenerateRequest):
    # ç”Ÿæˆé€»è¾‘...
    return {"text": generated_text}
```

**ä½¿ç”¨æŒ‡å—**: æŸ¥çœ‹ `docs/zh/production/model-serving.md`

### 9.5 ç›‘æ§å’Œæ—¥å¿—

**è®­ç»ƒç›‘æ§**:

```python
# ä½¿ç”¨ tensorboard
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/experiment_1')

for epoch in range(num_epochs):
    # è®­ç»ƒ...
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/val', val_loss, epoch)
    writer.add_scalar('Learning_rate', lr, epoch)

writer.close()
```

**æ—¥å¿—è®°å½•**:

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
logger.info("Training started")
```

---

## 10. å¸¸è§ä»»åŠ¡å¿«é€Ÿå‚è€ƒ

### 10.1 æ·»åŠ æ–°æ¨¡å‹ç»„ä»¶

```bash
# 1. ç¼–è¾‘ç»„ä»¶æ–‡ä»¶
vim src/llm_foundry/models/components.py

# 2. æ›´æ–° __init__.py
vim src/llm_foundry/models/__init__.py

# 3. æ·»åŠ æµ‹è¯•
vim tests/test_models.py

# 4. è¿è¡Œæµ‹è¯•
pytest tests/test_models.py -v

# 5. æ›´æ–°æ–‡æ¡£
vim docs/zh/architecture.md
```

### 10.2 å®ç°æ–°æ•°æ®é›†

```bash
# 1. åˆ›å»ºæ•°æ®é›†ç±»
vim src/llm_foundry/data/datasets.py

# 2. åˆ›å»ºä¸‹è½½å™¨
vim examples/datasets/download_new.py

# 3. æ·»åŠ æµ‹è¯•
vim tests/test_data.py

# 4. åˆ›å»ºç¤ºä¾‹
vim examples/05_new_dataset.py

# 5. æ›´æ–°æ–‡æ¡£
vim docs/zh/data-preparation.md
```

### 10.3 æ·»åŠ è®­ç»ƒåŠŸèƒ½

```bash
# 1. åœ¨è®­ç»ƒæ¨¡å—ä¸­å®ç°
vim src/llm_foundry/training/trainer.py

# 2. æ›´æ–°é…ç½®(å¦‚æœéœ€è¦)
vim src/llm_foundry/config/model_config.py

# 3. æ·»åŠ æµ‹è¯•
vim tests/test_training.py

# 4. æ›´æ–°æ–‡æ¡£
vim docs/zh/training.md
```

### 10.4 åˆ›å»ºæ–°ç¤ºä¾‹

```bash
# 1. åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
vim examples/06_new_example.py

# 2. æµ‹è¯•è¿è¡Œ
python examples/06_new_example.py

# 3. æ›´æ–° examples/README.md
vim examples/README.md
```

### 10.5 ç¼–å†™æ–‡æ¡£

```bash
# 1. åˆ›å»º/ç¼–è¾‘æ–‡æ¡£
vim docs/zh/new_topic.md

# 2. æ›´æ–°æ–‡æ¡£ç´¢å¼•
vim docs/README.md

# 3. æ£€æŸ¥ Markdown æ ¼å¼
markdownlint docs/zh/new_topic.md

# 4. é¢„è§ˆ(å¯é€‰)
# ä½¿ç”¨ Markdown é¢„è§ˆå·¥å…·
```

### 10.6 è¿è¡Œæµ‹è¯•

```bash
# å¿«é€Ÿæµ‹è¯•(æ ¸å¿ƒåŠŸèƒ½)
pytest tests/test_models.py tests/test_data.py

# å®Œæ•´æµ‹è¯•
pytest tests/ -v --cov=llm_foundry

# æµ‹è¯•ç‰¹å®šåŠŸèƒ½
pytest tests/ -k "test_rmsnorm"

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest tests/ --cov=llm_foundry --cov-report=html
open htmlcov/index.html
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£ä¸º AI Agent æä¾›äº†å…¨é¢çš„é¡¹ç›®å¯¼èˆªæŒ‡å—ã€‚å…³é”®è¦ç‚¹:

1. **é¡¹ç›®ä½¿å‘½**: ä»åŸºç¡€åˆ°ç”Ÿäº§çš„å®ç”¨ LLM åŸºç¡€
2. **åŒæ¨¡å¼è®¾è®¡**: ç®€å•æ¨¡å¼(æ•™å­¦)+ åŒ…æ¨¡å¼(ç”Ÿäº§)
3. **æ¸…æ™°çš„æ¨¡å—åˆ’åˆ†**: config, models, data, training, inference, utils
4. **å®Œæ•´çš„å¼€å‘å·¥ä½œæµ**: ä»ç¯å¢ƒè®¾ç½®åˆ° PR åˆå¹¶
5. **ç”Ÿäº§å°±ç»ª**: æ€§èƒ½ä¼˜åŒ–ã€åˆ†å¸ƒå¼è®­ç»ƒã€æ¨¡å‹æœåŠ¡

**ä¸‹ä¸€æ­¥**:
- é˜…è¯» `docs/zh/quickstart.md` å¿«é€Ÿä¸Šæ‰‹
- æŸ¥çœ‹ `examples/` äº†è§£ä½¿ç”¨æ–¹å¼
- æµè§ˆ `docs/zh/architecture.md` æ·±å…¥ç†è§£æ¶æ„

**ä¿æŒè”ç³»**:
- é—®é¢˜åé¦ˆ: GitHub Issues
- è®¨è®ºäº¤æµ: GitHub Discussions
- è´¡çŒ®ä»£ç : Pull Requests

---

**ç‰ˆæœ¬**: 0.1.0
**æœ€åæ›´æ–°**: 2026-01-30
**è®¸å¯è¯**: MIT
